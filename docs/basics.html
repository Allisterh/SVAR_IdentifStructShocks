<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 VARs and IRFs: the basics | The Identification of Dynamic Structural Shocks</title>
<meta name="author" content="Kenza Benhima and Jean-Paul Renne">
<meta name="description" content="Often, impulse response functions (IRFs) are generated in the context of vectorial autoregressive (VAR) models. This section presents these models and show how they can be used to compute IRFs. ...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 1 VARs and IRFs: the basics | The Identification of Dynamic Structural Shocks">
<meta property="og:type" content="book">
<meta property="og:description" content="Often, impulse response functions (IRFs) are generated in the context of vectorial autoregressive (VAR) models. This section presents these models and show how they can be used to compute IRFs. ...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 VARs and IRFs: the basics | The Identification of Dynamic Structural Shocks">
<meta name="twitter:description" content="Often, impulse response functions (IRFs) are generated in the context of vectorial autoregressive (VAR) models. This section presents these models and show how they can be used to compute IRFs. ...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="my-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">The Identification of Dynamic Structural Shocks</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">The Identification of Dynamic Structural Shocks</a></li>
<li><a class="active" href="basics.html"><span class="header-section-number">1</span> VARs and IRFs: the basics</a></li>
<li><a class="" href="identifStruct.html"><span class="header-section-number">2</span> Identification problem and standard identification techniques</a></li>
<li><a class="" href="Inference.html"><span class="header-section-number">3</span> Inference</a></li>
<li><a class="" href="Signs.html"><span class="header-section-number">4</span> Sign restrictions</a></li>
<li><a class="" href="SignsZeros.html"><span class="header-section-number">5</span> Combining sign and zero restrictions</a></li>
<li><a class="" href="forecast-error-variance-maximization.html"><span class="header-section-number">6</span> Forecast error variance maximization</a></li>
<li><a class="" href="NonGaussian.html"><span class="header-section-number">7</span> Identification based on non-normality of the shocks</a></li>
<li><a class="" href="Projections.html"><span class="header-section-number">8</span> Local projection methods</a></li>
<li><a class="" href="PanelVARs.html"><span class="header-section-number">9</span> Panel VARs</a></li>
<li><a class="" href="FAVAR.html"><span class="header-section-number">10</span> Factor-Augmented VAR</a></li>
<li><a class="" href="append.html"><span class="header-section-number">11</span> Appendix</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="basics" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> VARs and IRFs: the basics<a class="anchor" aria-label="anchor" href="#basics"><i class="fas fa-link"></i></a>
</h1>
<p>Often, impulse response functions (IRFs) are generated in the context of vectorial autoregressive (VAR) models. This section presents these models and show how they can be used to compute IRFs.</p>
<div id="definition-of-vars-and-svarma-models" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Definition of VARs (and SVARMA) models<a class="anchor" aria-label="anchor" href="#definition-of-vars-and-svarma-models"><i class="fas fa-link"></i></a>
</h2>
<div class="definition">
<p><span id="def:SVAR" class="definition"><strong>Definition 1.1  ((S)VAR model) </strong></span>Let <span class="math inline">\(y_{t}\)</span> denote a <span class="math inline">\(n \times1\)</span> vector of (endogenous) random variables. Process <span class="math inline">\(y_{t}\)</span> follows a <span class="math inline">\(p^{th}\)</span>-order (S)VAR if, for all <span class="math inline">\(t\)</span>, we have
<span class="math display" id="eq:yVAR">\[\begin{eqnarray}
\begin{array}{rllll}
VAR:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \varepsilon_t,\\
SVAR:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + B \eta_t,
\end{array}\tag{1.1}
\end{eqnarray}\]</span>
with <span class="math inline">\(\varepsilon_t = B\eta_t\)</span>, where <span class="math inline">\(\{\eta_{t}\}\)</span> is a white noise sequence whose components are mutually and serially independent.</p>
</div>
<p>The first line of Eq. <a href="basics.html#eq:yVAR">(1.1)</a> corresponds to the <strong>reduced-form</strong> of the VAR model (<strong>structural form</strong> for the second line). While the <strong>structural shocks</strong> (the components of <span class="math inline">\(\eta_t\)</span>) are mutually uncorrelated, this is not the case of the <strong>innovations</strong>, that are the components of <span class="math inline">\(\varepsilon_t\)</span>. However, in boths cases, vectors <span class="math inline">\(\eta_t\)</span> and <span class="math inline">\(\varepsilon_t\)</span> are serially correlated (through time).</p>
<p>As is the case for univariate models, VARs can be extended with MA terms in <span class="math inline">\(\eta_t\)</span>, giving rise to VARMA models:</p>
<div class="definition">
<p><span id="def:SVARMA" class="definition"><strong>Definition 1.2  ((S)VARMA model) </strong></span>Let <span class="math inline">\(y_{t}\)</span> denote a <span class="math inline">\(n \times1\)</span> vector of random variables. Process <span class="math inline">\(y_{t}\)</span> follows a VARMA model of order (p,q) if, for all <span class="math inline">\(t\)</span>, we have
<span class="math display" id="eq:yVARMA">\[\begin{eqnarray}
\begin{array}{rllll}
VARMA:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \\
&amp;&amp;&amp;\varepsilon_t + \Theta_1\varepsilon_{t-1} + \dots + \Theta_q \varepsilon_{t-q},\\
SVARMA:&amp; y_t &amp;=&amp; c + \Phi_1 y_{t-1} + \dots + \Phi_p y_{t-p} + \\
&amp;&amp;&amp; B_0 \eta_t+ B_1 \eta_{t-1} + \dots +  B_q \eta_{t-q},
\end{array}\tag{1.2}
\end{eqnarray}\]</span>
with <span class="math inline">\(\varepsilon_t = B_0\eta_t\)</span>, and <span class="math inline">\(B_j = \Theta_j B_0\)</span>, for <span class="math inline">\(j \ge 0\)</span> (with <span class="math inline">\(\Theta_0=Id\)</span>), where <span class="math inline">\(\{\eta_{t}\}\)</span> is a white noise sequence whose components are are mutually and serially independent.</p>
</div>
</div>
<div id="IRFSVARMA" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> IRFs in SVARMA<a class="anchor" aria-label="anchor" href="#IRFSVARMA"><i class="fas fa-link"></i></a>
</h2>
<p>One of the main objectives of macro-econometrics is to derive IRFs, that represent the dynamic effects of structural shocks (components of <span class="math inline">\(\eta_t\)</span>) though the system of variables <span class="math inline">\(y_t\)</span>.</p>
<!-- As illustrated by Figure \@ref(fig:NgramIRF), that makes use of Google Ngram data, there is a close link between the development of "macroeconomic analysis" and the concept ofimpulse response functions. -->
<!-- ```{r NgramIRF, echo=FALSE,fig.cap="Source: Google Ngram. Fraction of books containing the blue and red keywords."} -->
<!-- library(ngramr) -->
<!-- keyw <- c("impulse response function","macroeconomic analysis") -->
<!-- res1 <- ngram(keyw[1],year_start = 1900);res2 <- ngram(keyw[2],year_start = 1900) -->
<!-- plot(res1$Year,res1$Frequency,type="l",lwd=2,xlab="",ylab="",col="blue",las=1) -->
<!-- lines(res2$Year,res2$Frequency,type="l",lwd=2,col="red") -->
<!-- legend("topleft", -->
<!--        keyw,lty=c(1),lwd=c(2), -->
<!--        col=c("blue","red")) -->
<!-- ``` -->
<p>Formally, an IRF is a difference in conditional expectations:
<span class="math display" id="eq:boxIRFs">\[\begin{equation}
\boxed{\Psi_{i,j,h} = \mathbb{E}(y_{i,t+h}|\eta_{j,t}=1) - \mathbb{E}(y_{i,t+h})}\tag{1.3}
\end{equation}\]</span>
(effect on <span class="math inline">\(y_{i,t+h}\)</span> of a one-unit shock on <span class="math inline">\(\eta_{j,t}\)</span>).</p>
<p>IRFs closely relate to the Wold decomposition of <span class="math inline">\(y_t\)</span>. Indeed, if the dynamics of process <span class="math inline">\(y_t\)</span> can be described as a VARMA model, and if <span class="math inline">\(y_t\)</span> is covariance stationary (see Def. <a href="append.html#def:covstat">11.1</a>), then <span class="math inline">\(y_t\)</span> admits the following infinite MA representation (or Wold decomposition):
<span class="math display" id="eq:InfMA">\[\begin{equation}
y_t = \mu + \sum_{h=0}^\infty \Psi_{h} \eta_{t-h}.\tag{1.4}
\end{equation}\]</span>
With these notations, we get <span class="math inline">\(\mathbb{E}(y_{i,t+h}|\eta_{j,t}=1) = \mu_i + \Psi_{i,j,h}\)</span>, where <span class="math inline">\(\Psi_{i,j,h}\)</span> is the component <span class="math inline">\((i,j)\)</span> of matrix <span class="math inline">\(\Psi_h\)</span> and <span class="math inline">\(\mu_i\)</span> is the <span class="math inline">\(i^{th}\)</span> entry of vector <span class="math inline">\(\mu\)</span>. Since we also have <span class="math inline">\(\mathbb{E}(y_{i,t+h})=\mu_i\)</span>, we obtain Eq. <a href="basics.html#eq:boxIRFs">(1.3)</a>.</p>
<p>Hence, estimating IRFs amounts to estimating the <span class="math inline">\(\Psi_{h}\)</span>’s. In general, there exist three main approaches for that:</p>
<ul>
<li>Calibrate and solve a (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model at the first order (linearization). The solution takes the form of Eq. <a href="basics.html#eq:InfMA">(1.4)</a>.</li>
<li>Directly estimate the <span class="math inline">\(\Psi_{h}\)</span> based on <strong>projection approaches</strong> (see Section <a href="Projections.html#Projections">8</a>).</li>
<li>Approximate the infinite MA representation by estimating a parsimonious type of model, e.g. <strong>VAR(MA) models</strong> (see Section <a href="basics.html#estimVAR">1.4</a>). Once a (Structural) VARMA representation is obtained, Eq. <a href="basics.html#eq:InfMA">(1.4)</a> is easily deduced using the following proposition:</li>
</ul>
<div class="proposition">
<p><span id="prp:computPsi" class="proposition"><strong>Proposition 1.1  (IRF of an ARMA(p,q) process) </strong></span>If <span class="math inline">\(y_t\)</span> follows the VARMA model described in Def. <a href="basics.html#def:SVARMA">1.2</a>, then the matrices <span class="math inline">\(\Psi_h\)</span> appearing in Eq. <a href="basics.html#eq:InfMA">(1.4)</a> can be computed recursively as follows:</p>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\Psi_{-1}=\dots=\Psi_{-p}=0\)</span>.</li>
<li>For <span class="math inline">\(h \ge 0\)</span>, (recursively) apply:
<span class="math display">\[
\Psi_h = \Phi_1 \Psi_{h-1} + \dots + \Phi_p \Psi_{h-p} + \Theta_h B_0,
\]</span>
with <span class="math inline">\(\Theta_0 = Id\)</span> and <span class="math inline">\(\Theta_h = 0\)</span> for <span class="math inline">\(h&gt;q\)</span>.</li>
</ol>
</div>
<div class="proof">
<p><span id="unlabeled-div-1" class="proof"><em>Proof</em>. </span>This is obtained by applying the operator <span class="math inline">\(\frac{\partial}{\partial \varepsilon_{t}}\)</span> on both sides of Eq. <a href="basics.html#eq:yVARMA">(1.2)</a>.</p>
</div>
<p>Typically, consider the VAR(2) case. The first steps of the algorithm mentioned in the last bullet point are as follows:
<span class="math display">\[\begin{eqnarray*}
y_t &amp;=&amp; \Phi_1 {\color{blue}y_{t-1}} + \Phi_2 y_{t-2} + B \eta_t  \\
&amp;=&amp; \Phi_1 \color{blue}{(\Phi_1 y_{t-2} + \Phi_2 y_{t-3} + B \eta_{t-1})} + \Phi_2 y_{t-2} + B \eta_t  \\
&amp;=&amp; B \eta_t + \Phi_1 B \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{y_{t-2}} + \Phi_1\Phi_2 y_{t-3}  \\
&amp;=&amp; B \eta_t + \Phi_1 B \eta_{t-1} + (\Phi_2 + \Phi_1^2) \color{red}{(\Phi_1 y_{t-3} + \Phi_2 y_{t-4} + B \eta_{t-2})} + \Phi_1\Phi_2 y_{t-3} \\
&amp;=&amp; \underbrace{B}_{=\Psi_0} \eta_t + \underbrace{\Phi_1 B}_{=\Psi_1} \eta_{t-1} + \underbrace{(\Phi_2 + \Phi_1^2)B}_{=\Psi_2} \eta_{t-2} + f(y_{t-3},y_{t-4}).
\end{eqnarray*}\]</span></p>
<p>In particular, we have <span class="math inline">\(B = \Psi_0\)</span>. Matrix <span class="math inline">\(B\)</span> indeed captures the contemporaneous impact of <span class="math inline">\(\eta_t\)</span> on <span class="math inline">\(y_t\)</span>. That is why matrix <span class="math inline">\(B\)</span> is sometimes called <strong>impulse matrix</strong>.</p>
<div class="example">
<p><span id="exm:IRFVARMA" class="example"><strong>Example 1.1  (IRFs of an SVARMA model) </strong></span>Consider the following VARMA(1,1) model:
<span class="math display" id="eq:VARMA111">\[\begin{eqnarray}
\quad y_t &amp;=&amp;
\underbrace{\left[\begin{array}{cc}
0.5 &amp; 0.3 \\
-0.4 &amp; 0.7
\end{array}\right]}_{\Phi_1}
y_{t-1} +  
\underbrace{\left[\begin{array}{cc}
1 &amp; 2 \\
-1 &amp; 1
\end{array}\right]}_{B}\eta_t - \underbrace{\left[\begin{array}{cc}
-0.4 &amp; 0 \\
1 &amp; 0.5
\end{array}\right]}_{\Theta_1} \underbrace{\left[\begin{array}{cc}
1 &amp; 2 \\
-1 &amp; 1
\end{array}\right]}_{B}\eta_{t-1}.\tag{1.5}
\end{eqnarray}\]</span></p>
<p>We can use function <code>simul.VARMA</code> of package <code>IdSS</code> to produce IRFs (using <code>indic.IRF=1</code> in the list of arguments):</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">IdSS</span><span class="op">)</span></span>
<span><span class="va">distri</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>type<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gaussian"</span>,<span class="st">"gaussian"</span><span class="op">)</span>,df<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">4</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">distri</span><span class="op">$</span><span class="va">type</span><span class="op">)</span> <span class="co"># dimension of y_t</span></span>
<span><span class="va">nb.sim</span> <span class="op">&lt;-</span> <span class="fl">30</span></span>
<span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu">simul.distri</span><span class="op">(</span><span class="va">distri</span>,<span class="va">nb.sim</span><span class="op">)</span></span>
<span><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">n</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Phi</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.5</span>,<span class="op">-</span><span class="fl">.4</span>,<span class="fl">.3</span>,<span class="fl">.7</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Phi</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">Theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="cn">NaN</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">n</span>,<span class="va">n</span>,<span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Theta</span><span class="op">[</span>,,<span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">.4</span>,<span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">.5</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Theta</span><span class="op">)</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span></span>
<span><span class="va">Mu</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">C</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="op">-</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">1</span><span class="op">)</span>,<span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">Model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  Mu <span class="op">=</span> <span class="va">Mu</span>,Phi <span class="op">=</span> <span class="va">Phi</span>,Theta <span class="op">=</span> <span class="va">Theta</span>,C <span class="op">=</span> <span class="va">C</span>,distri <span class="op">=</span> <span class="va">distri</span><span class="op">)</span></span>
<span><span class="va">Y0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">n</span><span class="op">)</span></span>
<span><span class="va">eta0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">res.sim.1</span> <span class="op">&lt;-</span> <span class="fu">simul.VARMA</span><span class="op">(</span><span class="va">Model</span>,<span class="va">nb.sim</span>,<span class="va">Y0</span>,<span class="va">eta0</span>,indic.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">eta0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">res.sim.2</span> <span class="op">&lt;-</span> <span class="fu">simul.VARMA</span><span class="op">(</span><span class="va">Model</span>,<span class="va">nb.sim</span>,<span class="va">Y0</span>,<span class="va">eta0</span>,indic.IRF<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>plt<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.15</span>,<span class="fl">.95</span>,<span class="fl">.25</span>,<span class="fl">.8</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="va">i</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">{</span><span class="va">res.sim</span> <span class="op">&lt;-</span> <span class="va">res.sim.1</span></span>
<span>  <span class="op">}</span><span class="kw">else</span><span class="op">{</span><span class="va">res.sim</span> <span class="op">&lt;-</span> <span class="va">res.sim.2</span><span class="op">}</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">res.sim</span><span class="op">$</span><span class="va">Y</span><span class="op">[</span><span class="va">j</span>,<span class="op">]</span>,las<span class="op">=</span><span class="fl">1</span>,</span>
<span>         type<span class="op">=</span><span class="st">"l"</span>,lwd<span class="op">=</span><span class="fl">3</span>,xlab<span class="op">=</span><span class="st">""</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>         main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Resp. of y"</span>,<span class="va">j</span>,</span>
<span>                    <span class="st">" to a 1-unit increase in eta"</span>,<span class="va">i</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span>,col<span class="op">=</span><span class="st">"grey"</span>,lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simVAR"></span>
<img src="IdentifStructShocks_files/figure-html/simVAR-1.png" alt="Impulse response functions (SVARMA(1,1) specified above)." width="95%"><p class="caption">
Figure 1.1: Impulse response functions (SVARMA(1,1) specified above).
</p>
</div>
</div>
<!-- \includegraphics[width=.9\linewidth]{figures/RcodesFigure_illustrIRF.pdf} -->
<!-- \begin{defn}[Autocovariance of order $j$] -->
<!-- The autocovariance of order $j$ of $y_t$ is $\mathbb{C}ov(y_t,y_{t-j})$. -->
<!-- \end{defn} -->
<!-- \begin{defn}[Covariance-stationary process] -->
<!-- Process $y_t$ is covariance-stationary if $\mathbb{E}(y_t)$ and all autocovariances of $y_t$ are finite and do not depend on $t$. -->
<!-- \end{defn} -->
</div>
<div id="covariance-stationary-varma-models" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Covariance-stationary VARMA models<a class="anchor" aria-label="anchor" href="#covariance-stationary-varma-models"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s come back to the infinite MA case (Eq. <a href="basics.html#eq:InfMA">(1.4)</a>):
<span class="math display">\[
y_t = \mu + \sum_{h=0}^\infty \Psi_{h} \eta_{t-h}.
\]</span>
For <span class="math inline">\(y_t\)</span> to be covariance-stationary (and ergodic for the mean), it has to be the case that
<span class="math display" id="eq:condiInfiniteMA">\[\begin{equation}
\sum_{i=0}^\infty \|\Psi_i\| &lt; \infty,\tag{1.6}
\end{equation}\]</span>
where <span class="math inline">\(\|A\|\)</span> denotes a norm of the matrix <span class="math inline">\(A\)</span> (e.g. <span class="math inline">\(\|A\|=\sqrt{tr(AA')}\)</span>). This notably implies that if <span class="math inline">\(y_t\)</span> is stationary (and ergodic for the mean), then <span class="math inline">\(\|\Psi_h\|\rightarrow 0\)</span> when <span class="math inline">\(h\)</span> gets large.</p>
<p>What should be satisfied by <span class="math inline">\(\Phi_k\)</span>’s and <span class="math inline">\(\Theta_k\)</span>’s for a VARMA-based process (Eq. <a href="basics.html#eq:yVARMA">(1.2)</a>) to be stationary? The conditions will be similar to that we have in the univariate case. Let us introduce the following notations:
<span class="math display" id="eq:VARMA2">\[\begin{eqnarray}
y_t &amp;=&amp; c + \underbrace{\Phi_1 y_{t-1} + \dots +\Phi_p y_{t-p}}_{\color{blue}{\mbox{AR component}}} +  \tag{1.7}\\
&amp;&amp;\underbrace{B \eta_t - \Theta_1 B \eta_{t-1} - \dots - \Theta_q B \eta_{t-q}}_{\color{red}{\mbox{MA component}}} \nonumber\\
&amp;\Leftrightarrow&amp; \underbrace{ \color{blue}{(I - \Phi_1 L - \dots - \Phi_p L^p)}}_{= \color{blue}{\Phi(L)}}y_t = c +  \underbrace{ \color{red}{(I - \Theta_1 L - \ldots - \Theta_q L^q)}}_{=\color{red}{\Theta(L)}} B \eta_{t}. \nonumber
\end{eqnarray}\]</span></p>
<p>Process <span class="math inline">\(y_t\)</span> is stationary iff the roots of <span class="math inline">\(\det(\Phi(z))=0\)</span> are strictly outside the unit circle or, equivalently, iff the eigenvalues of
<span class="math display" id="eq:matrixPHI">\[\begin{equation}
\Phi = \left[\begin{array}{cccc}
\Phi_{1} &amp; \Phi_{2} &amp; \cdots &amp; \Phi_{p}\\
I &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; \ddots &amp; 0 &amp; 0\\
0 &amp; 0 &amp; I &amp; 0\end{array}\right]\tag{1.8}
\end{equation}\]</span>
lie strictly within the unit circle. Hence, as is the case for univariate processes, the covariance-stationarity of a VARMA model depends only on the specification of its AR part.</p>
<p>Let’s derive the first two unconditional moments of a (covariance-stationary) VARMA process.</p>
<p>Eq. <a href="basics.html#eq:VARMA2">(1.7)</a> gives <span class="math inline">\(\mathbb{E}(\Phi(L)y_t)=c\)</span>, therefore <span class="math inline">\(\Phi(1)\mathbb{E}(y_t)=c\)</span>, or
<span class="math display">\[
\mathbb{E}(y_t) = (I - \Phi_1 - \dots - \Phi_p)^{-1}c.
\]</span>
The autocovariances of <span class="math inline">\(y_t\)</span> can be deduced from the infinite MA representation (Eq. <a href="basics.html#eq:InfMA">(1.4)</a>). We have:
<span class="math display">\[
\gamma_j \equiv \mathbb{C}ov(y_t,y_{t-j}) = \sum_{i=j}^\infty \Psi_i \Psi_{i-j}'.
\]</span>
(This infinite sum exists as soon as Eq. <a href="basics.html#eq:condiInfiniteMA">(1.6)</a> is satisfied.)</p>
<p>Conditional means and autocovariances can also be deduced from Eq. <a href="basics.html#eq:InfMA">(1.4)</a>. For <span class="math inline">\(0 \le h\)</span> and <span class="math inline">\(0 \le h_1 \le h_2\)</span>:
<span class="math display">\[\begin{eqnarray*}
\mathbb{E}_t(y_{t+h}) &amp;=&amp; \mu + \sum_{k=0}^\infty \Psi_{k+h} \eta_{t-k} \\
\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &amp;=&amp; \sum_{k=0}^{h_1} \Psi_{k}\Psi_{k+h_2-h_1}'.
\end{eqnarray*}\]</span></p>
<p>The previous formula implies in particular that the forecasting error <span class="math inline">\(y_{t+h} - \mathbb{E}_t(y_{t+h})\)</span> has a variance equal to:
<span class="math display">\[
\mathbb{V}ar_t(y_{t+1+h}) = \sum_{k=0}^{h} \Psi_{k}\Psi_{k}'.
\]</span>
Because the <span class="math inline">\(\eta_t\)</span> are mutually and serially independent (and therefore uncorrelated), we have:
<span class="math display">\[
\mathbb{V}ar(\Psi_k \eta_{t-k}) = \mathbb{V}ar\left(\sum_{i=1}^n \psi_{k,i} \eta_{i,t-k}\right)  = \sum_{i=1}^n \psi_{k,i}\psi_{k,i}',
\]</span>
where <span class="math inline">\(\psi_{k,i}\)</span> denotes the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\Psi_k\)</span>. This suggests the following decomposition of the variance of the forecast error (called <strong>variance decomposition</strong>):
<span class="math display">\[
\mathbb{V}ar_t(y_{t+1+h}) = \sum_{i=1}^n \underbrace{\sum_{k=0}^{h}\psi_{k,i}\psi_{k,i}'.}_{\mbox{Contribution of $\eta_{i,t}$}}
\]</span></p>
<p>Let us now turn to the estimation of VAR models. Note that if there is an MA component (i.e., if we consider a VARMA model), then OLS regressions yield biased estimates (even for asymptotically large samples). Assume for instance that <span class="math inline">\(y_t\)</span> follows a VARMA(1,1) model:
<span class="math display">\[
y_{i,t} = \phi_i y_{t-1} + \varepsilon_{i,t},
\]</span>
where <span class="math inline">\(\phi_i\)</span> is the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\Phi_1\)</span>, and where <span class="math inline">\(\varepsilon_{i,t}\)</span> is a linear combination of <span class="math inline">\(\eta_t\)</span> and <span class="math inline">\(\eta_{t-1}\)</span>. Since <span class="math inline">\(y_{t-1}\)</span> (the regressor) is correlated to <span class="math inline">\(\eta_{t-1}\)</span>, it is also correlated to <span class="math inline">\(\varepsilon_{i,t}\)</span>. The OLS regression of <span class="math inline">\(y_{i,t}\)</span> on <span class="math inline">\(y_{t-1}\)</span> yields a biased estimator of <span class="math inline">\(\phi_i\)</span> (see Figure <a href="basics.html#fig:simulARMAbiased">1.2</a>). Hence, SVARMA models cannot be consistently estimated by simple OLS regressions (contrary to VAR models, as we will see in the next section); instrumental-variable approaches can be employed to estimate SVARMA models (using past values of <span class="math inline">\(y_t\)</span> as instruments, see, e.g., <span class="citation">Gouriéroux, Monfort, and Renne (<a href="references.html#ref-Gourieroux_Monfort_Renne_2020" role="doc-biblioref">2020</a>)</span>).</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">1000</span> <span class="co"># number of replications</span></span>
<span><span class="cn">T</span> <span class="op">&lt;-</span> <span class="fl">100</span> <span class="co"># sample length</span></span>
<span><span class="va">phi</span> <span class="op">&lt;-</span> <span class="fl">.8</span> <span class="co"># autoregressive parameter</span></span>
<span><span class="va">sigma</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">theta</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>,<span class="op">-</span><span class="fl">0.4</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">all.y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">1</span>,<span class="va">N</span><span class="op">)</span></span>
<span>  <span class="va">y</span>     <span class="op">&lt;-</span> <span class="va">all.y</span></span>
<span>  <span class="va">eta_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">t</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">eta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">N</span><span class="op">)</span></span>
<span>    <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">phi</span> <span class="op">*</span> <span class="va">y</span> <span class="op">+</span> <span class="va">sigma</span> <span class="op">*</span> <span class="va">eta</span> <span class="op">+</span> <span class="va">theta</span> <span class="op">*</span> <span class="va">sigma</span> <span class="op">*</span> <span class="va">eta_1</span></span>
<span>    <span class="va">all.y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="va">all.y</span>,<span class="va">y</span><span class="op">)</span></span>
<span>    <span class="va">eta_1</span> <span class="op">&lt;-</span> <span class="va">eta</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">all.y_1</span> <span class="op">&lt;-</span> <span class="va">all.y</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="cn">T</span>,<span class="op">]</span></span>
<span>  <span class="va">all.y</span>   <span class="op">&lt;-</span> <span class="va">all.y</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="op">(</span><span class="cn">T</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span>,<span class="op">]</span></span>
<span>  <span class="va">XX_1</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">all.y_1</span> <span class="op">*</span> <span class="va">all.y_1</span>,<span class="fl">2</span>,<span class="va">sum</span><span class="op">)</span></span>
<span>  <span class="va">XY</span>   <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">all.y_1</span> <span class="op">*</span> <span class="va">all.y</span>,<span class="fl">2</span>,<span class="va">sum</span><span class="op">)</span></span>
<span>  <span class="va">phi.est.OLS</span> <span class="op">&lt;-</span> <span class="va">XX_1</span> <span class="op">*</span> <span class="va">XY</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">phi.est.OLS</span><span class="op">)</span>,xlab<span class="op">=</span><span class="st">"OLS estimate of phi"</span>,ylab<span class="op">=</span><span class="st">""</span>,</span>
<span>       main<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"theta = "</span>,<span class="va">theta</span>,sep<span class="op">=</span><span class="st">""</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>v<span class="op">=</span><span class="va">phi</span>,col<span class="op">=</span><span class="st">"red"</span>,lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span><span class="op">}</span></span></code></pre></div>
<div class="figure" style="text-align: left-aligned">
<span style="display:block;" id="fig:simulARMAbiased"></span>
<img src="IdentifStructShocks_files/figure-html/simulARMAbiased-1.png" alt="Illustration of the bias obtained when estimating the auto-regressive parameters of an ARMA process by (standard) OLS." width="95%"><p class="caption">
Figure 1.2: Illustration of the bias obtained when estimating the auto-regressive parameters of an ARMA process by (standard) OLS.
</p>
</div>
</div>
<div id="estimVAR" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> VAR estimation<a class="anchor" aria-label="anchor" href="#estimVAR"><i class="fas fa-link"></i></a>
</h2>
<p>This section discusses the estimation of VAR models. Eq. <a href="basics.html#eq:yVAR">(1.1)</a> can be written:
<span class="math display">\[
y_{t}=c+\Phi(L)y_{t-1}+\varepsilon_{t},
\]</span>
with <span class="math inline">\(\Phi(L) = \Phi_1 + \Phi_2 L + \dots + \Phi_p L^{p-1}\)</span>.</p>
<p>Consequently:
<span class="math display">\[
y_{t}\mid y_{t-1},y_{t-2},\ldots,y_{-p+1}\sim \mathcal{N}(c+\Phi_{1}y_{t-1}+\ldots\Phi_{p}y_{t-p},\Omega).
\]</span></p>
<p>Using <span class="citation">Hamilton (<a href="references.html#ref-Hamilton_1994" role="doc-biblioref">1994</a>)</span>’s notations, denote with <span class="math inline">\(\Pi\)</span> the matrix <span class="math inline">\(\left[\begin{array}{ccccc} c &amp; \Phi_{1} &amp; \Phi_{2} &amp; \ldots &amp; \Phi_{p}\end{array}\right]'\)</span> and with <span class="math inline">\(x_{t}\)</span> the vector <span class="math inline">\(\left[\begin{array}{ccccc} 1 &amp; y'_{t-1} &amp; y'_{t-2} &amp; \ldots &amp; y'_{t-p}\end{array}\right]'\)</span>, we have:
<span class="math display" id="eq:PIVAR">\[\begin{equation}
y_{t}= \Pi'x_{t} + \varepsilon_{t}. \tag{1.9}
\end{equation}\]</span>
The previous representation is convenient to discuss the estimation of the VAR model, as parameters are gathered in two matrices only: <span class="math inline">\(\Pi\)</span> and <span class="math inline">\(\Omega\)</span>.</p>
<p>Let us start with the case where the shocks are Gaussian.</p>
<div class="proposition">
<p><span id="prp:estimVARGaussian" class="proposition"><strong>Proposition 1.2  (MLE of a Gaussian VAR) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR(p) (see Definition <a href="basics.html#def:SVAR">1.1</a>), and if <span class="math inline">\(\varepsilon_t \sim \,i.i.d.\,\mathcal{N}(0,\Omega)\)</span>, then the ML estimate of <span class="math inline">\(\Pi\)</span>, denoted by <span class="math inline">\(\hat{\Pi}\)</span> (see Eq. <a href="basics.html#eq:PIVAR">(1.9)</a>), is given by
<span class="math display" id="eq:Pi">\[\begin{equation}
\hat{\Pi}=\left[\sum_{t=1}^{T}x_{t}x'_{t}\right]^{-1}\left[\sum_{t=1}^{T}y_{t}'x_{t}\right]= (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y},\tag{1.10}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is the <span class="math inline">\(T \times (1+np)\)</span> matrix whose <span class="math inline">\(t^{th}\)</span> row is <span class="math inline">\(x_t\)</span> and where <span class="math inline">\(\mathbf{y}\)</span> is the <span class="math inline">\(T \times n\)</span> matrix whose <span class="math inline">\(t^{th}\)</span> row is <span class="math inline">\(y_{t}'\)</span>.</p>
<p>That is, the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(\hat{\Pi}\)</span> (<span class="math inline">\(b_i\)</span>, say) is the OLS estimate of <span class="math inline">\(\beta_i\)</span>, where:
<span class="math display" id="eq:betayx">\[\begin{equation}
y_{i,t} = \beta_i'x_t + \varepsilon_{i,t},\tag{1.11}
\end{equation}\]</span>
(i.e., <span class="math inline">\(\beta_i' = [c_i,\phi_{i,1}',\dots,\phi_{i,p}']'\)</span>).</p>
<p>The ML estimate of <span class="math inline">\(\Omega\)</span>, denoted by <span class="math inline">\(\hat{\Omega}\)</span>, coincides with the sample covariance matrix of the <span class="math inline">\(n\)</span> series of the OLS residuals in Eq. <a href="basics.html#eq:betayx">(1.11)</a>, i.e.:
<span class="math display">\[\begin{equation}
\hat{\Omega} = \frac{1}{T} \sum_{i=1}^T \hat{\varepsilon}_t\hat{\varepsilon}_t',\quad\mbox{with } \hat{\varepsilon}_t= y_t - \hat{\Pi}'x_t.
\end{equation}\]</span></p>
<p>The asymptotic distributions of these estimators are the ones resulting from standard OLS formula.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">11.2</a>.</p>
</div>
<p>As stated by Proposition <a href="basics.html#prp:OLSVAR">1.3</a>, when the shocks are not Gaussian, then the OLS regressions still provide consistent estimates of the model parameters. However, since <span class="math inline">\(x_t\)</span> correlates to <span class="math inline">\(\varepsilon_s\)</span> for <span class="math inline">\(s&lt;t\)</span>, the OLS estimator <span class="math inline">\(\mathbf{b}_i\)</span> of <span class="math inline">\(\boldsymbol\beta_i\)</span> is biased in small sample. (That is also the case for the ML estimator.) Indeed, denoting by <span class="math inline">\(\boldsymbol\varepsilon_i\)</span> the <span class="math inline">\(T \times 1\)</span> vector of <span class="math inline">\(\varepsilon_{i,t}\)</span>’s, and using the notations of <span class="math inline">\(b_i\)</span> and <span class="math inline">\(\beta_i\)</span> introduced in Proposition <a href="basics.html#prp:estimVARGaussian">1.2</a>, we have:
<span class="math display" id="eq:olsar1">\[\begin{equation}
\mathbf{b}_i = \beta_i + (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon_i.\tag{1.12}
\end{equation}\]</span>
We have non-zero correlation between <span class="math inline">\(x_t\)</span> and <span class="math inline">\(\varepsilon_{i,s}\)</span> for <span class="math inline">\(s&lt;t\)</span> and, therefore, <span class="math inline">\(\mathbb{E}[(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\boldsymbol\varepsilon_i] \ne 0\)</span>.</p>
<p>However, when <span class="math inline">\(y_t\)</span> is covariance stationary, then <span class="math inline">\(\frac{1}{n}\mathbf{X}'\mathbf{X}\)</span> converges to a positive definite matrix <span class="math inline">\(\mathbf{Q}\)</span>, and <span class="math inline">\(\frac{1}{n}X'\boldsymbol\varepsilon_i\)</span> converges to 0. Hence <span class="math inline">\(\mathbf{b}_i \overset{p}{\rightarrow} \beta_i\)</span>. More precisely:</p>
<div class="proposition">
<p><span id="prp:OLSVAR" class="proposition"><strong>Proposition 1.3  (Asymptotic distribution of the OLS estimate of VAR coefficients (for one variable)) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR model, as defined in Definition <a href="basics.html#def:SVAR">1.1</a>, we have:
<span class="math display">\[
\sqrt{T}(\mathbf{b}_i-\beta_i) =  \underbrace{\left[\frac{1}{T}\sum_{t=p}^T x_t x_t' \right]^{-1}}_{\overset{p}{\rightarrow} \mathbf{Q}^{-1}}
\underbrace{\sqrt{T} \left[\frac{1}{T}\sum_{t=1}^T x_t\varepsilon_{i,t} \right]}_{\overset{d}{\rightarrow} \mathcal{N}(0,\sigma_i^2\mathbf{Q})},
\]</span>
where <span class="math inline">\(\sigma_i = \mathbb{V}ar(\varepsilon_{i,t})\)</span> and where <span class="math inline">\(\mathbf{Q} = \mbox{plim }\frac{1}{T}\sum_{t=p}^T x_t x_t'\)</span> is given by:
<span class="math display" id="eq:Qols">\[\begin{equation}
\mathbf{Q} = \left[
\begin{array}{ccccc}
1 &amp; \mu' &amp;\mu' &amp; \dots &amp; \mu' \\
\mu &amp; \gamma_0 + \mu\mu' &amp; \gamma_1 + \mu\mu' &amp; \dots &amp; \gamma_{p-1} + \mu\mu'\\
\mu &amp; \gamma_1 + \mu\mu' &amp; \gamma_0 + \mu\mu' &amp; \dots &amp; \gamma_{p-2} + \mu\mu'\\
\vdots &amp;\vdots &amp;\vdots &amp;\dots &amp;\vdots \\
\mu &amp; \gamma_{p-1} + \mu\mu' &amp; \gamma_{p-2} + \mu\mu' &amp; \dots &amp; \gamma_{0} + \mu\mu'
\end{array}
\right].\tag{1.13}
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>See Appendix <a href="append.html#AppendixProof">11.2</a>.</p>
</div>
<p>The following proposition extends the previous proposition and includes covariances between different <span class="math inline">\(\beta_i\)</span>’s as well as the asymptotic distribution of the ML estimates of <span class="math inline">\(\Omega\)</span>.</p>
<div class="proposition">
<p><span id="prp:OLSVAR2" class="proposition"><strong>Proposition 1.4  (Asymptotic distribution of the OLS estimates) </strong></span>If <span class="math inline">\(y_t\)</span> follows a VAR model, as defined in Definition <a href="basics.html#def:SVAR">1.1</a>, we have:
<span class="math display" id="eq:asymptPi">\[\begin{equation}
\sqrt{T}\left[
\begin{array}{c}
vec(\hat\Pi - \Pi)\\
vec(\hat\Omega - \Omega)
\end{array}
\right]
\sim \mathcal{N}\left(0,
\left[
\begin{array}{cc}
\Omega \otimes \mathbf{Q}^{-1} &amp; 0\\
0 &amp; \Sigma_{22}
\end{array}
\right]\right),\tag{1.14}
\end{equation}\]</span>
where the component of <span class="math inline">\(\Sigma_{22}\)</span> corresponding to the covariance between <span class="math inline">\(\hat\sigma_{i,j}\)</span> and <span class="math inline">\(\hat\sigma_{k,l}\)</span> (for <span class="math inline">\(i,j,l,m \in \{1,\dots,n\}^4\)</span>) is equal to <span class="math inline">\(\sigma_{i,l}\sigma_{j,m}+\sigma_{i,m}\sigma_{j,l}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>See <span class="citation">Hamilton (<a href="references.html#ref-Hamilton_1994" role="doc-biblioref">1994</a>)</span>, Appendix of Chapter 11.</p>
</div>
<p>In practice, to use the previous proposition (for instance to implement Monte-Carlo simulations, see Section <a href="Inference.html#MonteCarlo">3.1</a>), <span class="math inline">\(\Omega\)</span> is replaced with <span class="math inline">\(\hat{\Omega}\)</span>, <span class="math inline">\(\mathbf{Q}\)</span> is replaced with <span class="math inline">\(\hat{\mathbf{Q}} = \frac{1}{T}\sum_{t=p}^T x_t x_t'\)</span> and <span class="math inline">\(\Sigma\)</span> with the matrix whose components are of the form <span class="math inline">\(\hat\sigma_{i,l}\hat\sigma_{j,m}+\hat\sigma_{i,m}\hat\sigma_{j,l}\)</span>, where the <span class="math inline">\(\hat\sigma_{i,l}\)</span>’s are the components of <span class="math inline">\(\hat\Omega\)</span>.</p>
<p>The simplicity of the VAR framework and the tractability of its MLE open the way to convenient econometric testing. Let’s illustrate this with the likelihood ratio test (see Def. <a href="append.html#def:LR">11.2</a>). The maximum value achieved by the MLE is
<span class="math display">\[
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega}) = -\frac{Tn}{2}\log(2\pi)+\frac{T}{2}\log\left|\hat{\Omega}^{-1}\right| -\frac{1}{2}\sum_{t=1}^{T}\left[\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right].
\]</span>
The last term is:
<span class="math display">\[\begin{eqnarray*}
\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t} &amp;=&amp; \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\varepsilon}_{t}'\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\right] = \mbox{Tr}\left[\sum_{t=1}^{T}\hat{\Omega}^{-1}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right]\\
&amp;=&amp;\mbox{Tr}\left[\hat{\Omega}^{-1}\sum_{t=1}^{T}\hat{\varepsilon}_{t}\hat{\varepsilon}_{t}'\right] = \mbox{Tr}\left[\hat{\Omega}^{-1}\left(T\hat{\Omega}\right)\right]=Tn.
\end{eqnarray*}\]</span>
Therefore, the optimized log-likelihood is simply obtained by:
<span class="math display" id="eq:optimzedLogL">\[\begin{equation}
\log\mathcal{L}(Y_{T};\hat{\Pi},\hat{\Omega})=-(Tn/2)\log(2\pi)+(T/2)\log\left|\hat{\Omega}^{-1}\right|-Tn/2.\tag{1.15}
\end{equation}\]</span></p>
<p>Assume that we want to test the null hypothesis that a set of variables follows a VAR(<span class="math inline">\(p_{0}\)</span>) against the alternative specification of <span class="math inline">\(p_{1}\)</span> (<span class="math inline">\(&gt;p_{0}\)</span>). Let us denote by <span class="math inline">\(\hat{L}_{0}\)</span> and <span class="math inline">\(\hat{L}_{1}\)</span> the maximum log-likelihoods obtained with <span class="math inline">\(p_{0}\)</span> and <span class="math inline">\(p_{1}\)</span> lags, respectively. Under the null hypothesis (<span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span>), we have:
<span class="math display">\[\begin{eqnarray*}
2\left(\hat{L}_{1}-\hat{L}_{0}\right)&amp;=&amp;T\left(\log\left|\hat{\Omega}_{1}^{-1}\right|-\log\left|\hat{\Omega}_{0}^{-1}\right|\right)  \sim \chi^2(n^{2}(p_{1}-p_{0})).
\end{eqnarray*}\]</span></p>
<p>What precedes can be used to help determine the appropriate number of lags to use in the specification. In a VAR, using too many lags consumes numerous degrees of freedom: with <span class="math inline">\(p\)</span> lags, each of the <span class="math inline">\(n\)</span> equations in the VAR contains <span class="math inline">\(n\times p\)</span> coefficients plus the intercept term. Adding lags improve in-sample fit, but is likely to result in over-parameterization and affect the <strong>out-of-sample</strong> prediction performance.</p>
<p>To select appropriate lag length, <strong>selection criteria</strong> are often used. In the context of VAR models, using Eq. <a href="basics.html#eq:optimzedLogL">(1.15)</a> (Gaussian case), we have for instance:
<span class="math display">\[\begin{eqnarray*}
AIC &amp; = &amp; cst + \log\left|\hat{\Omega}\right|+\frac{2}{T}N\\
BIC &amp; = &amp; cst + \log\left|\hat{\Omega}\right|+\frac{\log T}{T}N,
\end{eqnarray*}\]</span>
where <span class="math inline">\(N=p \times n^{2}\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.pfaffikus.de">vars</a></span><span class="op">)</span>;<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">IdSS</span><span class="op">)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">US3var</span><span class="op">[</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"y.gdp.gap"</span>,<span class="st">"infl"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/vars/man/VARselect.html">VARselect</a></span><span class="op">(</span><span class="va">data</span>,lag.max <span class="op">=</span> <span class="fl">6</span><span class="op">)</span></span></code></pre></div>
<pre><code>## $selection
## AIC(n)  HQ(n)  SC(n) FPE(n) 
##      3      3      2      3 
## 
## $criteria
##                 1          2          3          4          5           6
## AIC(n) -0.3394120 -0.4835525 -0.5328327 -0.5210835 -0.5141079 -0.49112812
## HQ(n)  -0.3017869 -0.4208439 -0.4450407 -0.4082080 -0.3761491 -0.32808581
## SC(n)  -0.2462608 -0.3283005 -0.3154798 -0.2416298 -0.1725534 -0.08747275
## FPE(n)  0.7121914  0.6165990  0.5869659  0.5939325  0.5981364  0.61210908</code></pre>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">estimated.var</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/VAR.html">VAR</a></span><span class="op">(</span><span class="va">data</span>,p<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#print(estimated.var$varresult)</span></span>
<span><span class="va">Phi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/vars/man/A.html">Acoef</a></span><span class="op">(</span><span class="va">estimated.var</span><span class="op">)</span></span>
<span><span class="va">PHI</span> <span class="op">&lt;-</span> <span class="fu">make.PHI</span><span class="op">(</span><span class="va">Phi</span><span class="op">)</span> <span class="co"># autoregressive matrix of companion form.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="va">PHI</span><span class="op">)</span><span class="op">$</span><span class="va">values</span><span class="op">)</span><span class="op">)</span> <span class="co"># check stationarity</span></span></code></pre></div>
<pre><code>## [1] 0.9114892 0.9114892 0.6319554 0.4759403 0.4759403 0.3246995</code></pre>
</div>
<div id="BlockGranger" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Block exogeneity and Granger causality<a class="anchor" aria-label="anchor" href="#BlockGranger"><i class="fas fa-link"></i></a>
</h2>
<div id="block-exogeneity" class="section level3" number="1.5.1">
<h3>
<span class="header-section-number">1.5.1</span> Block exogeneity<a class="anchor" aria-label="anchor" href="#block-exogeneity"><i class="fas fa-link"></i></a>
</h3>
<p>Let’s decompose <span class="math inline">\(y_t\)</span> into two subvectors <span class="math inline">\(y^{(1)}_{t}\)</span> (<span class="math inline">\(n_1 \times 1\)</span>) and <span class="math inline">\(y^{(2)}_{t}\)</span> (<span class="math inline">\(n_2 \times 1\)</span>), with <span class="math inline">\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\)</span> (and therefore <span class="math inline">\(n=n_1 +n_2\)</span>), such that:
<span class="math display">\[
\left[
\begin{array}{c}
y^{(1)}_{t}\\
y^{(2)}_{t}
\end{array}
\right] = \left[
\begin{array}{cc}
\Phi^{(1,1)} &amp; \Phi^{(1,2)}\\
\Phi^{(2,1)} &amp; \Phi^{(2,2)}
\end{array}
\right]
\left[
\begin{array}{c}
y^{(1)}_{t-1}\\
y^{(2)}_{t-1}
\end{array}
\right] + \varepsilon_t.
\]</span>
Using, e.g., a likelihood ratio test (see Def. <a href="append.html#def:LR">11.2</a>), one can easily test for block exogeneity of <span class="math inline">\(y_t^{(2)}\)</span> (say). The null assumption can be expressed as <span class="math inline">\(\Phi^{(2,1)}=0\)</span>.</p>
<!-- **Companion Form and Stability of a VAR process** -->
<!-- Let us introduce vector $y_{t}^{*}$, whihc stacks the last $p$ values of $y_t$: -->
<!-- $$ -->
<!-- y_{t}^{*}=\left[\begin{array}{cccc} -->
<!-- y'_{t} & y'_{t-1} & \ldots & y'_{t-p+1}\end{array}\right]^{'}, -->
<!-- $$ -->
<!-- Eq. \@ref(eq:yVAR) can then be rewritten in its companion form: -->
<!-- \begin{equation} -->
<!-- y_{t}^{*} = -->
<!-- \underbrace{\left[\begin{array}{c} -->
<!-- c\\ -->
<!-- 0\\ -->
<!-- \vdots\\ -->
<!-- 0\end{array}\right]}_{=c^*}+ -->
<!-- \underbrace{\left[\begin{array}{cccc} -->
<!-- \Phi_{1} & \Phi_{2} & \cdots & \Phi_{p}\\ -->
<!-- I & 0 & \cdots & 0\\ -->
<!-- 0 & \ddots & 0 & 0\\ -->
<!-- 0 & 0 & I & 0\end{array}\right]}_{=\Phi} -->
<!-- y_{t-1}^{*}+ -->
<!-- \underbrace{\left[\begin{array}{c} -->
<!-- \varepsilon_{t}\\ -->
<!-- 0\\ -->
<!-- \vdots\\ -->
<!-- 0\end{array}\right]}_{\varepsilon_t^*}(\#eq:ystarVAR) -->
<!-- \end{equation} -->
<!-- Matrices $\Phi$ and $\Sigma^* = \mathbb{V}ar(\varepsilon_t^*)$ are of dimension $np \times np$. $\Sigma^*$ is filled with zeros, except the $n\times n$ upper-left block that is equal to $\Sigma = \mathbb{V}ar(\varepsilon_t)$. -->
<!-- We then have: -->
<!-- \begin{eqnarray*} -->
<!-- y_{t}^{*} & = & c^{*}+\Phi\left(c^{*}+\Phi y_{t-2}^{*}+\varepsilon_{t-1}^{*}\right)+\varepsilon_{t}^{*} \nonumber \\ -->
<!-- & = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\Phi^k y_{t-k}^{*}. -->
<!-- \end{eqnarray*} -->
<!-- If the eigenvalues of $\Phi$ are strictly within the unit circle, then $\Phi^k$ geometrically decays to the zero matrix and we get the following Wold decomposition for $y_t$: -->
<!-- \begin{eqnarray} -->
<!-- y_{t}^{*}  & = & c^{*}+\varepsilon_{t}^{*}+\Phi(c^{*}+\varepsilon_{t-1}^{*})+\ldots+\Phi^{k}(c^{*}+\varepsilon_{t-k}^{*})+\ldots \nonumber \\ -->
<!-- & = & \mu^{*} +\varepsilon_{t}^{*}+\Phi\varepsilon_{t-1}^{*}+\ldots+\Phi^{k}\varepsilon_{t-k}^{*}+\ldots,(\#eq:VARstar) -->
<!-- \end{eqnarray} -->
<!-- where $\mu^* = (I - \Phi)^{-1} c^*$. -->
<!-- (It can also be seen that $\mu^{*} = [\mu',\dots,\mu']'$, where $\mu = (I - \Phi_1 - \dots - \Phi_p)^{-1}c$). -->
<!-- The unconditional variance of $y_t$ can be derived from Eq. \@ref(eq:VARstar), exploiting the fact that the $\varepsilon_{t}^{*}$ are serially uncorrelated: -->
<!-- $$ -->
<!-- \mathbb{V}ar(y_t^*)=\Omega^*+\Phi\Omega^*\Phi'+\ldots+\Phi^{k}\Omega^*\Phi'^{k}+\ldots, -->
<!-- $$ -->
<!-- with $\mathbb{V}ar(\varepsilon_t^*)=\Omega^*$. -->
<!-- The unconditional variance of $y_t$ is the upper-left $n\times n$ block of matrix $\mathbb{V}ar(y_t^*)$. -->
<!-- Eq. \@ref(eq:VARstar) also implies that the $\Psi_k$ matrices defining the IRFs  (see Eq. \@ref(eq:InfMA)) are given by: $\Psi_k = \widetilde{\Phi^k}B$, where $\widetilde{\Phi^k}$ is the upper-left matrix block of $\Phi^k$. -->
</div>
<div id="granger-causality" class="section level3" number="1.5.2">
<h3>
<span class="header-section-number">1.5.2</span> Granger Causality<a class="anchor" aria-label="anchor" href="#granger-causality"><i class="fas fa-link"></i></a>
</h3>
<p><span class="citation">Granger (<a href="references.html#ref-Granger_1969" role="doc-biblioref">1969</a>)</span> developed a method to explore <strong>causal relationships</strong> among variables. The approach consists in determining whether the past values of <span class="math inline">\(y_{1,t}\)</span> can help explain the current <span class="math inline">\(y_{2,t}\)</span> (beyond the information already included in the past values of <span class="math inline">\(y_{2,t}\)</span>).</p>
<p>Formally, let us denote three information sets:
<span class="math display">\[\begin{eqnarray*}
\mathcal{I}_{1,t} &amp; = &amp; \left\{ y_{1,t},y_{1,t-1},\ldots\right\} \\
\mathcal{I}_{2,t} &amp; = &amp; \left\{ y_{2,t},y_{2,t-1},\ldots\right\} \\
\mathcal{I}_{t} &amp; = &amp; \left\{ y_{1,t},y_{1,t-1},\ldots y_{2,t},y_{2,t-1},\ldots\right\}.
\end{eqnarray*}\]</span>
We say that <span class="math inline">\(y_{1,t}\)</span> Granger-causes <span class="math inline">\(y_{2,t}\)</span> if
<span class="math display">\[
\mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{2,t-1}\right]\neq \mathbb{E}\left[y_{2,t}\mid \mathcal{I}_{t-1}\right].
\]</span></p>
<p>To get the intuition behind the testing procedure, consider the following
bivariate VAR(<span class="math inline">\(p\)</span>) process:
<span class="math display">\[\begin{eqnarray*}
y_{1,t} &amp; = &amp; c_1+\Sigma_{i=1}^{p}\Phi_i^{(11)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(12)}y_{2,t-i}+\varepsilon_{1,t}\\
y_{2,t} &amp; = &amp; c_2+\Sigma_{i=1}^{p}\Phi_i^{(21)}y_{1,t-i}+\Sigma_{i=1}^{p}\Phi_i^{(22)}y_{2,t-i}+\varepsilon_{2,t},
\end{eqnarray*}\]</span>
where <span class="math inline">\(\Phi_k^{(ij)}\)</span> denotes the element <span class="math inline">\((i,j)\)</span> of <span class="math inline">\(\Phi_k\)</span>. Then, <span class="math inline">\(y_{1,t}\)</span> is said not to Granger-cause <span class="math inline">\(y_{2,t}\)</span> if
<span class="math display">\[
\Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0.
\]</span>
The null and alternative hypotheses therefore are:
<span class="math display">\[
\begin{cases}
H_{0}: &amp; \Phi_1^{(21)}=\Phi_2^{(21)}=\ldots=\Phi_p^{(21)}=0\\
H_{1}: &amp; \Phi_1^{(21)}\neq0\mbox{ or }\Phi_2^{(21)}\neq0\mbox{ or}\ldots\Phi_p^{(21)}\neq0.\end{cases}
\]</span>
Loosely speaking, we reject <span class="math inline">\(H_{0}\)</span> if some of the coefficients on the lagged <span class="math inline">\(y_{1,t}\)</span>’s are statistically significant. Formally, this can be tested using the <span class="math inline">\(F\)</span>-test or asymptotic chi-square test. The <span class="math inline">\(F\)</span>-statistic is
<span class="math display">\[
F=\frac{(RSS-USS)/p}{USS/(T-2p-1)},
\]</span>
where RSS is the Restricted sum of squared residuals and USS is the Unrestricted sum of squared residuals. Under <span class="math inline">\(H_{0}\)</span>, the <span class="math inline">\(F\)</span>-statistic is distributed as <span class="math inline">\(\mathcal{F}(p,T-2p-1)\)</span> (See Table <a href="append.html#tab:Fstat">11.4</a>).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We have &lt;span class="math inline"&gt;\(pF\underset{T \rightarrow \infty}{\rightarrow}\chi^{2}(p)\)&lt;/span&gt;.&lt;/p&gt;'><sup>1</sup></a></p>
<p>According to the following lines of code, the output gap Granger-causes inflation, but the reverse is not true:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/grangertest.html">grangertest</a></span><span class="op">(</span><span class="va">US3var</span><span class="op">[</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"y.gdp.gap"</span>,<span class="st">"infl"</span><span class="op">)</span><span class="op">]</span>,order<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Granger causality test
## 
## Model 1: infl ~ Lags(infl, 1:3) + Lags(y.gdp.gap, 1:3)
## Model 2: infl ~ Lags(infl, 1:3)
##   Res.Df Df      F   Pr(&gt;F)   
## 1    214                      
## 2    217 -3 3.9761 0.008745 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/lmtest/man/grangertest.html">grangertest</a></span><span class="op">(</span><span class="va">US3var</span><span class="op">[</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"infl"</span>,<span class="st">"y.gdp.gap"</span><span class="op">)</span><span class="op">]</span>,order<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code>## Granger causality test
## 
## Model 1: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) + Lags(infl, 1:3)
## Model 2: y.gdp.gap ~ Lags(y.gdp.gap, 1:3)
##   Res.Df Df      F Pr(&gt;F)
## 1    214                 
## 2    217 -3 1.5451 0.2038</code></pre>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="index.html">The Identification of Dynamic Structural Shocks</a></div>
<div class="next"><a href="identifStruct.html"><span class="header-section-number">2</span> Identification problem and standard identification techniques</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#basics"><span class="header-section-number">1</span> VARs and IRFs: the basics</a></li>
<li><a class="nav-link" href="#definition-of-vars-and-svarma-models"><span class="header-section-number">1.1</span> Definition of VARs (and SVARMA) models</a></li>
<li><a class="nav-link" href="#IRFSVARMA"><span class="header-section-number">1.2</span> IRFs in SVARMA</a></li>
<li><a class="nav-link" href="#covariance-stationary-varma-models"><span class="header-section-number">1.3</span> Covariance-stationary VARMA models</a></li>
<li><a class="nav-link" href="#estimVAR"><span class="header-section-number">1.4</span> VAR estimation</a></li>
<li>
<a class="nav-link" href="#BlockGranger"><span class="header-section-number">1.5</span> Block exogeneity and Granger causality</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#block-exogeneity"><span class="header-section-number">1.5.1</span> Block exogeneity</a></li>
<li><a class="nav-link" href="#granger-causality"><span class="header-section-number">1.5.2</span> Granger Causality</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>The Identification of Dynamic Structural Shocks</strong>" was written by Kenza Benhima and Jean-Paul Renne. It was last built on 2023-01-30.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
