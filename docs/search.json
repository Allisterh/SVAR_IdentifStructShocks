[{"path":"index.html","id":"intro","chapter":"The Identification of Dynamic Structural Shocks","heading":"The Identification of Dynamic Structural Shocks","text":"identification estimation dynamic responses structural shocks one principal goals macroeconometrics. responses correspond effect, time, exogenous intervention propagates economy, modeled system simultaneous equations.last decades, several methodologies proposed estimate responses. objective course, developed Kenza Benhima Jean-Paul Renne, provide exhaustive view methodologies provide students tools enabling implement various contexts.Codes associated course part IdSS package (Identification Structural Shocks), available GitHub. load package GitHub, need use function install_github devtools package:Useful (R) links:Download R:\nR software: https://cran.r-project.org (basic R software)\nRStudio: https://www.rstudio.com (convenient R editor)\nDownload R:R software: https://cran.r-project.org (basic R software)RStudio: https://www.rstudio.com (convenient R editor)Tutorials:\nRstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)\nR: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)\ntutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/\nTutorials:Rstudio: https://dss.princeton.edu/training/RStudio101.pdf (Oscar Torres-Reyna)R: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf (Emmanuel Paradis)tutorial: https://jrenne.shinyapps.io/Rtuto_publiShiny/","code":"\ninstall.packages(\"devtools\") # devtools allows to use \"install_github\"\nlibrary(devtools)\ninstall_github(\"jrenne/IdSS\")\nlibrary(IdSS)"},{"path":"basics.html","id":"basics","chapter":"1 VARs and IRFs: the basics","heading":"1 VARs and IRFs: the basics","text":"Often, impulse response functions (IRFs) generated context vectorial autoregressive (VAR) models. section presents models show can used compute IRFs.","code":""},{"path":"basics.html","id":"definition-of-vars-and-svarma-models","chapter":"1 VARs and IRFs: the basics","heading":"1.1 Definition of VARs (and SVARMA) models","text":"Definition 1.1  ((S)VAR model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector (endogenous) random variables. Process \\(y_{t}\\) follows \\(p^{th}\\)-order (S)VAR , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t,\\\\\nSVAR:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + B \\eta_t,\n\\end{array}\\tag{1.1}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B\\eta_t\\), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.first line Eq. (1.1) corresponds reduced-form VAR model (structural form second line). structural shocks (components \\(\\eta_t\\)) mutually uncorrelated, case innovations, components \\(\\varepsilon_t\\). However, boths cases, vectors \\(\\eta_t\\) \\(\\varepsilon_t\\) serially correlated (time).case univariate models, VARs can extended MA terms \\(\\eta_t\\), giving rise VARMA models:Definition 1.2  ((S)VARMA model) Let \\(y_{t}\\) denote \\(n \\times1\\) vector random variables. Process \\(y_{t}\\) follows VARMA model order (p,q) , \\(t\\), \n\\[\\begin{eqnarray}\n\\begin{array}{rllll}\nVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&&\\varepsilon_t + \\Theta_1\\varepsilon_{t-1} + \\dots + \\Theta_q ,\\\\\nSVARMA:& y_t &=& c + \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\\\\n&&& B_0 \\eta_t+ B_1 \\eta_{t-1} + \\dots +  B_q \\eta_{t-q},\n\\end{array}\\tag{1.2}\n\\end{eqnarray}\\]\n\\(\\varepsilon_t = B_0\\eta_t\\) (\\(B_j = \\Theta_j B_0\\), \\(j \\ge 0\\)), \\(\\{\\eta_{t}\\}\\) white noise sequence whose components mutually serially independent.","code":""},{"path":"basics.html","id":"IRFSVARMA","chapter":"1 VARs and IRFs: the basics","heading":"1.2 IRFs in SVARMA","text":"One main objectives macro-econometrics derive IRFs, represent dynamic effects structural shocks (components \\(\\eta_t\\)) though system variables \\(y_t\\).Formally, IRF difference conditional expectations:\n\\[\\begin{equation}\n\\boxed{\\Psi_{,j,h} = \\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) - \\mathbb{E}(y_{,t+h})}\\tag{1.3}\n\\end{equation}\\]\n(effect \\(y_{,t+h}\\) one-unit shock \\(\\eta_{j,t}\\)).IRFs closely relate Wold decomposition \\(y_t\\). Indeed, dynamics process \\(y_t\\) can described VARMA model, \\(y_t\\) covariance stationary (see Def. 11.1), \\(y_t\\) admits following infinite MA representation (Wold decomposition):\n\\[\\begin{equation}\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\\tag{1.4}\n\\end{equation}\\]\nnotations, get \\(\\mathbb{E}(y_{,t+h}|\\eta_{j,t}=1) = \\mu_i + \\Psi_{,j,h}\\), \\(\\Psi_{,j,h}\\) component \\((,j)\\) matrix \\(\\Psi_h\\) \\(\\mu_i\\) \\(^{th}\\) entry vector \\(\\mu\\). Since also \\(\\mathbb{E}(y_{,t+h})=\\mu_i\\), obtain Eq. (1.3).Hence, estimating IRFs amounts estimating \\(\\Psi_{h}\\)’s. general, exist three main approaches :Calibrate solve (purely structural) Dynamic Stochastic General Equilibrium (DSGE) model first order (linearization). solution takes form Eq. (1.4).Directly estimate \\(\\Psi_{h}\\) based projection approaches (see Section 8).Approximate infinite MA representation estimating parsimonious type model, e.g. VAR(MA) models (see Section 1.4). (Structural) VARMA representation obtained, Eq. (1.4) easily deduced using following proposition:Proposition 1.1  (IRF ARMA(p,q) process) \\(y_t\\) follows VARMA model described Def. 1.2, matrices \\(\\Psi_h\\) appearing Eq. (1.4) can computed recursively follows:Set \\(\\Psi_{-1}=\\dots=\\Psi_{-p}=0\\).\\(h \\ge 0\\), (recursively) apply:\n\\[\n\\Psi_h = \\Phi_1 \\Psi_{h-1} + \\dots + \\Phi_p \\Psi_{h-p} + \\Theta_h,\n\\]\n\\(\\Theta_0 = Id\\) \\(\\Theta_h = 0\\) \\(h>q\\).Proof. obtained applying operator \\(\\frac{\\partial}{\\partial \\varepsilon_{t}}\\) sides Eq. (1.2).Typically, consider VAR(2) case. first steps algorithm mentioned last bullet point follows:\n\\[\\begin{eqnarray*}\ny_t &=& \\Phi_1 {\\color{blue}y_{t-1}} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& \\Phi_1 \\color{blue}{(\\Phi_1 y_{t-2} + \\Phi_2 y_{t-3} + B \\eta_{t-1})} + \\Phi_2 y_{t-2} + B \\eta_t  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{y_{t-2}} + \\Phi_1\\Phi_2 y_{t-3}  \\\\\n&=& B \\eta_t + \\Phi_1 B \\eta_{t-1} + (\\Phi_2 + \\Phi_1^2) \\color{red}{(\\Phi_1 y_{t-3} + \\Phi_2 y_{t-4} + B \\eta_{t-2})} + \\Phi_1\\Phi_2 y_{t-3} \\\\\n&=& \\underbrace{B}_{=\\Psi_0} \\eta_t + \\underbrace{\\Phi_1 B}_{=\\Psi_1} \\eta_{t-1} + \\underbrace{(\\Phi_2 + \\Phi_1^2)B}_{=\\Psi_2} \\eta_{t-2} + f(y_{t-3},y_{t-4}).\n\\end{eqnarray*}\\]particular, \\(B = \\Psi_0\\). Matrix \\(B\\) indeed captures contemporaneous impact \\(\\eta_t\\) \\(y_t\\). matrix \\(B\\) sometimes called impulse matrix.Example 1.1  (IRFs SVARMA model) Consider following VARMA(1,1) model:\n\\[\\begin{eqnarray}\n\\quad y_t &=&\n\\underbrace{\\left[\\begin{array}{cc}\n0.5 & 0.3 \\\\\n-0.4 & 0.7\n\\end{array}\\right]}_{\\Phi_1}\ny_{t-1} +  \n\\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_t - \\underbrace{\\left[\\begin{array}{cc}\n-0.4 & 0 \\\\\n1 & 0.5\n\\end{array}\\right]}_{\\Theta_1} \\underbrace{\\left[\\begin{array}{cc}\n1 & 2 \\\\\n-1 & 1\n\\end{array}\\right]}_{B}\\eta_{t-1}.\\tag{1.5}\n\\end{eqnarray}\\]can use function simul.VARMA package IdSS produce IRFs (using indic.IRF=1 list arguments):\nFigure 1.1: Impulse response functions (SVARMA(1,1) specified ).\n","code":"\nlibrary(IdSS)\ndistri <- list(type=c(\"gaussian\",\"gaussian\"),df=c(4,4))\nn <- length(distri$type) # dimension of y_t\nnb.sim <- 30\neps <- simul.distri(distri,nb.sim)\nPhi <- array(NaN,c(n,n,1))\nPhi[,,1] <- matrix(c(.5,-.4,.3,.7),2,2)\np <- dim(Phi)[3]\nTheta <- array(NaN,c(n,n,1))\nTheta[,,1] <- matrix(c(-.4,1,0,.5),2,2)\nq <- dim(Theta)[3]\nMu <- rep(0,n)\nC <- matrix(c(1,-1,2,1),2,2)\nModel <- list(\n  Mu = Mu,Phi = Phi,Theta = Theta,C = C,distri = distri)\nY0 <- rep(0,n)\neta0 <- c(1,0)\nres.sim.1 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\neta0 <- c(0,1)\nres.sim.2 <- simul.VARMA(Model,nb.sim,Y0,eta0,indic.IRF=1)\npar(plt=c(.15,.95,.25,.8))\npar(mfrow=c(2,2))\nfor(i in 1:2){\n  if(i == 1){res.sim <- res.sim.1\n  }else{res.sim <- res.sim.2}\n  for(j in 1:2){\n    plot(res.sim$Y[j,],las=1,\n         type=\"l\",lwd=3,xlab=\"\",ylab=\"\",\n         main=paste(\"Resp. of y\",j,\n                    \" to a 1-unit increase in eta\",i,sep=\"\"))\n    abline(h=0,col=\"grey\",lty=3)\n  }}"},{"path":"basics.html","id":"covariance-stationary-varma-models","chapter":"1 VARs and IRFs: the basics","heading":"1.3 Covariance-stationary VARMA models","text":"Let’s come back infinite MA case (Eq. (1.4)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\n\\(y_t\\) covariance-stationary (ergodic mean), case \n\\[\\begin{equation}\n\\sum_{=0}^\\infty \\|\\Psi_i\\| < \\infty,\\tag{1.6}\n\\end{equation}\\]\n\\(\\|\\|\\) denotes norm matrix \\(\\) (e.g. \\(\\|\\|=\\sqrt{tr(AA')}\\)). notably implies \\(y_t\\) stationary (ergodic mean), \\(\\|\\Psi_h\\|\\rightarrow 0\\) \\(h\\) gets large.satisfied \\(\\Phi_k\\)’s \\(\\Theta_k\\)’s VARMA-based process (Eq. (1.2)) stationary? conditions similar univariate case. Let us introduce following notations:\n\\[\\begin{eqnarray}\ny_t &=& c + \\underbrace{\\Phi_1 y_{t-1} + \\dots +\\Phi_p y_{t-p}}_{\\color{blue}{\\mbox{AR component}}} +  \\tag{1.7}\\\\\n&&\\underbrace{B \\eta_t - \\Theta_1 B \\eta_{t-1} - \\dots - \\Theta_q B \\eta_{t-q}}_{\\color{red}{\\mbox{MA component}}} \\nonumber\\\\\n&\\Leftrightarrow& \\underbrace{ \\color{blue}{(- \\Phi_1 L - \\dots - \\Phi_p L^p)}}_{= \\color{blue}{\\Phi(L)}}y_t = c +  \\underbrace{ \\color{red}{(- \\Theta_1 L - \\ldots - \\Theta_q L^q)}}_{=\\color{red}{\\Theta(L)}} B \\eta_{t}. \\nonumber\n\\end{eqnarray}\\]Process \\(y_t\\) stationary iff roots \\(\\det(\\Phi(z))=0\\) strictly outside unit circle , equivalently, iff eigenvalues \n\\[\\begin{equation}\n\\Phi = \\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]\\tag{1.8}\n\\end{equation}\\]\nlie strictly within unit circle. Hence, case univariate processes, covariance-stationarity VARMA model depends specification AR part.Let’s derive first two unconditional moments (covariance-stationary) VARMA process.Eq. (1.7) gives \\(\\mathbb{E}(\\Phi(L)y_t)=c\\), therefore \\(\\Phi(1)\\mathbb{E}(y_t)=c\\), \n\\[\n\\mathbb{E}(y_t) = (- \\Phi_1 - \\dots - \\Phi_p)^{-1}c.\n\\]\nautocovariances \\(y_t\\) can deduced infinite MA representation (Eq. (1.4)). :\n\\[\n\\gamma_j \\equiv \\mathbb{C}ov(y_t,y_{t-j}) = \\sum_{=j}^\\infty \\Psi_i \\Psi_{-j}'.\n\\]\n(infinite sum exists soon Eq. (1.6) satisfied.)Conditional means autocovariances can also deduced Eq. (1.4). \\(0 \\le h\\) \\(0 \\le h_1 \\le h_2\\):\n\\[\\begin{eqnarray*}\n\\mathbb{E}_t(y_{t+h}) &=& \\mu + \\sum_{k=0}^\\infty \\Psi_{k+h} \\eta_{t-k} \\\\\n\\mathbb{C}ov_t(y_{t+1+h_1},y_{t+1+h_2}) &=& \\sum_{k=0}^{h_1} \\Psi_{k}\\Psi_{k+h_2-h_1}'.\n\\end{eqnarray*}\\]previous formula implies particular forecasting error \\(y_{t+h} - \\mathbb{E}_t(y_{t+h})\\) variance equal :\n\\[\n\\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{k=0}^{h} \\Psi_{k}\\Psi_{k}'.\n\\]\n\\(\\eta_t\\) mutually serially independent (therefore uncorrelated), :\n\\[\n\\mathbb{V}ar(\\Psi_k \\eta_{t-k}) = \\mathbb{V}ar\\left(\\sum_{=1}^n \\psi_{k,} \\eta_{,t-k}\\right)  = \\sum_{=1}^n \\psi_{k,}\\psi_{k,}',\n\\]\n\\(\\psi_{k,}\\) denotes \\(^{th}\\) column \\(\\Psi_k\\). suggests following decomposition variance forecast error (called variance decomposition):\n\\[\n\\mathbb{V}ar_t(y_{t+1+h}) = \\sum_{=1}^n \\underbrace{\\sum_{k=0}^{h}\\psi_{k,}\\psi_{k,}'.}_{\\mbox{Contribution $\\eta_{,t}$}}\n\\]Let us now turn estimation VAR models. Note MA component (.e., consider VARMA model), OLS regressions yield biased estimates (even asymptotically large samples). Assume instance \\(y_t\\) follows VARMA(1,1) model:\n\\[\ny_{,t} = \\phi_i y_{t-1} + \\varepsilon_{,t},\n\\]\n\\(\\phi_i\\) \\(^{th}\\) row \\(\\Phi_1\\), \\(\\varepsilon_{,t}\\) linear combination \\(\\eta_t\\) \\(\\eta_{t-1}\\). Since \\(y_{t-1}\\) (regressor) correlated \\(\\eta_{t-1}\\), also correlated \\(\\varepsilon_{,t}\\). OLS regression \\(y_{,t}\\) \\(y_{t-1}\\) yields biased estimator \\(\\phi_i\\) (see Figure 1.2). Hence, SVARMA models consistently estimated simple OLS regressions (contrary VAR models, see next section); instrumental-variable approaches can employed estimate SVARMA models (using past values \\(y_t\\) instruments, see, e.g., Gouriéroux, Monfort, Renne (2020)).\nFigure 1.2: Illustration bias obtained estimating auto-regressive parameters ARMA process (standard) OLS.\n","code":"\nN <- 1000 # number of replications\nT <- 100 # sample length\nphi <- .8 # autoregressive parameter\nsigma <- 1\npar(mfrow=c(1,2))\nfor(theta in c(0,-0.4)){\n  all.y <- matrix(0,1,N)\n  y     <- all.y\n  eta_1 <- rnorm(N)\n  for(t in 1:(T+1)){\n    eta <- rnorm(N)\n    y <- phi * y + sigma * eta + theta * sigma * eta_1\n    all.y <- rbind(all.y,y)\n    eta_1 <- eta\n  }\n  all.y_1 <- all.y[1:T,]\n  all.y   <- all.y[2:(T+1),]\n  XX_1 <- 1/apply(all.y_1 * all.y_1,2,sum)\n  XY   <- apply(all.y_1 * all.y,2,sum)\n  phi.est.OLS <- XX_1 * XY\n  plot(density(phi.est.OLS),xlab=\"OLS estimate of phi\",ylab=\"\",\n       main=paste(\"theta = \",theta,sep=\"\"))\n  abline(v=phi,col=\"red\",lwd=2)}"},{"path":"basics.html","id":"estimVAR","chapter":"1 VARs and IRFs: the basics","heading":"1.4 VAR estimation","text":"section discusses estimation VAR models. Eq. (1.1) can written:\n\\[\ny_{t}=c+\\Phi(L)y_{t-1}+\\varepsilon_{t},\n\\]\n\\(\\Phi(L) = \\Phi_1 + \\Phi_2 L + \\dots + \\Phi_p L^{p-1}\\).Consequently:\n\\[\ny_{t}\\mid y_{t-1},y_{t-2},\\ldots,y_{-p+1}\\sim \\mathcal{N}(c+\\Phi_{1}y_{t-1}+\\ldots\\Phi_{p}y_{t-p},\\Omega).\n\\]Using Hamilton (1994)’s notations, denote \\(\\Pi\\) matrix \\(\\left[\\begin{array}{ccccc} c & \\Phi_{1} & \\Phi_{2} & \\ldots & \\Phi_{p}\\end{array}\\right]'\\) \\(x_{t}\\) vector \\(\\left[\\begin{array}{ccccc} 1 & y'_{t-1} & y'_{t-2} & \\ldots & y'_{t-p}\\end{array}\\right]'\\), :\n\\[\\begin{equation}\ny_{t}= \\Pi'x_{t} + \\varepsilon_{t}. \\tag{1.9}\n\\end{equation}\\]\nprevious representation convenient discuss estimation VAR model, parameters gathered two matrices : \\(\\Pi\\) \\(\\Omega\\).Let us start case shocks Gaussian.Proposition 1.2  (MLE Gaussian VAR) \\(y_t\\) follows VAR(p) (see Definition 1.1), \\(\\varepsilon_t \\sim \\,..d.\\,\\mathcal{N}(0,\\Omega)\\), ML estimate \\(\\Pi\\), denoted \\(\\hat{\\Pi}\\) (see Eq. (1.9)), given \n\\[\\begin{equation}\n\\hat{\\Pi}=\\left[\\sum_{t=1}^{T}x_{t}x'_{t}\\right]^{-1}\\left[\\sum_{t=1}^{T}y_{t}'x_{t}\\right]= (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y},\\tag{1.10}\n\\end{equation}\\]\n\\(\\mathbf{X}\\) \\(T \\times (np)\\) matrix whose \\(t^{th}\\) row \\(x_t\\) \\(\\mathbf{y}\\) \\(T \\times n\\) matrix whose \\(t^{th}\\) row \\(y_{t}'\\)., \\(^{th}\\) column \\(\\hat{\\Pi}\\) (\\(b_i\\), say) OLS estimate \\(\\beta_i\\), :\n\\[\\begin{equation}\ny_{,t} = \\beta_i'x_t + \\varepsilon_{,t},\\tag{1.11}\n\\end{equation}\\]\n(.e., \\(\\beta_i' = [c_i,\\phi_{,1}',\\dots,\\phi_{,p}']'\\)).ML estimate \\(\\Omega\\), denoted \\(\\hat{\\Omega}\\), coincides sample covariance matrix \\(n\\) series OLS residuals Eq. (1.11), .e.:\n\\[\\begin{equation}\n\\hat{\\Omega} = \\frac{1}{T} \\sum_{=1}^T \\hat{\\varepsilon}_t\\hat{\\varepsilon}_t',\\quad\\mbox{} \\hat{\\varepsilon}_t= y_t - \\hat{\\Pi}'x_t.\n\\end{equation}\\]asymptotic distributions estimators ones resulting standard OLS formula.Proof. See Appendix 11.2.stated Proposition 1.3, shocks Gaussian, OLS regressions still provide consistent estimates model parameters. However, since \\(x_t\\) correlates \\(\\varepsilon_s\\) \\(s<t\\), OLS estimator \\(\\mathbf{b}_i\\) \\(\\boldsymbol\\beta_i\\) biased small sample. (also case ML estimator.) Indeed, denoting \\(\\boldsymbol\\varepsilon_i\\) \\(T \\times 1\\) vector \\(\\varepsilon_{,t}\\)’s, using notations \\(b_i\\) \\(\\beta_i\\) introduced Proposition 1.2, :\n\\[\\begin{equation}\n\\mathbf{b}_i = \\beta_i + (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i.\\tag{1.12}\n\\end{equation}\\]\nnon-zero correlation \\(x_t\\) \\(\\varepsilon_{,s}\\) \\(s<t\\) , therefore, \\(\\mathbb{E}[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol\\varepsilon_i] \\ne 0\\).However, \\(y_t\\) covariance stationary, \\(\\frac{1}{n}\\mathbf{X}'\\mathbf{X}\\) converges positive definite matrix \\(\\mathbf{Q}\\), \\(\\frac{1}{n}X'\\boldsymbol\\varepsilon_i\\) converges 0. Hence \\(\\mathbf{b}_i \\overset{p}{\\rightarrow} \\beta_i\\). precisely:Proposition 1.3  (Asymptotic distribution OLS estimate VAR coefficients (one variable)) \\(y_t\\) follows VAR model, defined Definition 1.1, :\n\\[\n\\sqrt{T}(\\mathbf{b}_i-\\beta_i) =  \\underbrace{\\left[\\frac{1}{T}\\sum_{t=p}^T x_t x_t' \\right]^{-1}}_{\\overset{p}{\\rightarrow} \\mathbf{Q}^{-1}}\n\\underbrace{\\sqrt{T} \\left[\\frac{1}{T}\\sum_{t=1}^T x_t\\varepsilon_{,t} \\right]}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,\\sigma_i^2\\mathbf{Q})},\n\\]\n\\(\\sigma_i = \\mathbb{V}ar(\\varepsilon_{,t})\\) \\(\\mathbf{Q} = \\mbox{plim }\\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) given :\n\\[\\begin{equation}\n\\mathbf{Q} = \\left[\n\\begin{array}{ccccc}\n1 & \\mu' &\\mu' & \\dots & \\mu' \\\\\n\\mu & \\gamma_0 + \\mu\\mu' & \\gamma_1 + \\mu\\mu' & \\dots & \\gamma_{p-1} + \\mu\\mu'\\\\\n\\mu & \\gamma_1 + \\mu\\mu' & \\gamma_0 + \\mu\\mu' & \\dots & \\gamma_{p-2} + \\mu\\mu'\\\\\n\\vdots &\\vdots &\\vdots &\\dots &\\vdots \\\\\n\\mu & \\gamma_{p-1} + \\mu\\mu' & \\gamma_{p-2} + \\mu\\mu' & \\dots & \\gamma_{0} + \\mu\\mu'\n\\end{array}\n\\right].\\tag{1.13}\n\\end{equation}\\]Proof. See Appendix 11.2.following proposition extends previous proposition includes covariances different \\(\\beta_i\\)’s well asymptotic distribution ML estimates \\(\\Omega\\).Proposition 1.4  (Asymptotic distribution OLS estimates) \\(y_t\\) follows VAR model, defined Definition 1.1, :\n\\[\\begin{equation}\n\\sqrt{T}\\left[\n\\begin{array}{c}\nvec(\\hat\\Pi - \\Pi)\\\\\nvec(\\hat\\Omega - \\Omega)\n\\end{array}\n\\right]\n\\sim \\mathcal{N}\\left(0,\n\\left[\n\\begin{array}{cc}\n\\Omega \\otimes \\mathbf{Q}^{-1} & 0\\\\\n0 & \\Sigma_{22}\n\\end{array}\n\\right]\\right),\\tag{1.14}\n\\end{equation}\\]\ncomponent \\(\\Sigma_{22}\\) corresponding covariance \\(\\hat\\sigma_{,j}\\) \\(\\hat\\sigma_{k,l}\\) (\\(,j,l,m \\\\{1,\\dots,n\\}^4\\)) equal \\(\\sigma_{,l}\\sigma_{j,m}+\\sigma_{,m}\\sigma_{j,l}\\).Proof. See Hamilton (1994), Appendix Chapter 11.practice, use previous proposition (instance implement Monte-Carlo simulations, see Section 3.1), \\(\\Omega\\) replaced \\(\\hat{\\Omega}\\), \\(\\mathbf{Q}\\) replaced \\(\\hat{\\mathbf{Q}} = \\frac{1}{T}\\sum_{t=p}^T x_t x_t'\\) \\(\\Sigma\\) matrix whose components form \\(\\hat\\sigma_{,l}\\hat\\sigma_{j,m}+\\hat\\sigma_{,m}\\hat\\sigma_{j,l}\\), \\(\\hat\\sigma_{,l}\\)’s components \\(\\hat\\Omega\\).simplicity VAR framework tractability MLE open way convenient econometric testing. Let’s illustrate likelihood ratio test (see Def. 11.2). maximum value achieved MLE \n\\[\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega}) = -\\frac{Tn}{2}\\log(2\\pi)+\\frac{T}{2}\\log\\left|\\hat{\\Omega}^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\]\nlast term :\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t} &=& \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\right] = \\mbox{Tr}\\left[\\sum_{t=1}^{T}\\hat{\\Omega}^{-1}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right]\\\\\n&=&\\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}_{t}'\\right] = \\mbox{Tr}\\left[\\hat{\\Omega}^{-1}\\left(T\\hat{\\Omega}\\right)\\right]=Tn.\n\\end{eqnarray*}\\]\nTherefore, optimized log-likelihood simply obtained :\n\\[\\begin{equation}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\hat{\\Omega})=-(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\hat{\\Omega}^{-1}\\right|-Tn/2.\\tag{1.15}\n\\end{equation}\\]Assume want test null hypothesis set variables follows VAR(\\(p_{0}\\)) alternative specification \\(p_{1}\\) (\\(>p_{0}\\)). Let us denote \\(\\hat{L}_{0}\\) \\(\\hat{L}_{1}\\) maximum log-likelihoods obtained \\(p_{0}\\) \\(p_{1}\\) lags, respectively. null hypothesis (\\(H_0\\): \\(p=p_0\\)), :\n\\[\\begin{eqnarray*}\n2\\left(\\hat{L}_{1}-\\hat{L}_{0}\\right)&=&T\\left(\\log\\left|\\hat{\\Omega}_{1}^{-1}\\right|-\\log\\left|\\hat{\\Omega}_{0}^{-1}\\right|\\right)  \\sim \\chi^2(n^{2}(p_{1}-p_{0})).\n\\end{eqnarray*}\\]precedes can used help determine appropriate number lags use specification. VAR, using many lags consumes numerous degrees freedom: \\(p\\) lags, \\(n\\) equations VAR contains \\(n\\times p\\) coefficients plus intercept term. Adding lags improve -sample fit, likely result -parameterization affect --sample prediction performance.select appropriate lag length, selection criteria often used. context VAR models, using Eq. (1.15) (Gaussian case), instance:\n\\[\\begin{eqnarray*}\nAIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{2}{T}N\\\\\nBIC & = & cst + \\log\\left|\\hat{\\Omega}\\right|+\\frac{\\log T}{T}N,\n\\end{eqnarray*}\\]\n\\(N=p \\times n^{2}\\).","code":"\nlibrary(vars);library(IdSS)\ndata <- US3var[,c(\"y.gdp.gap\",\"infl\")]\nVARselect(data,lag.max = 6)## $selection\n## AIC(n)  HQ(n)  SC(n) FPE(n) \n##      3      3      2      3 \n## \n## $criteria\n##                 1          2          3          4          5           6\n## AIC(n) -0.3394120 -0.4835525 -0.5328327 -0.5210835 -0.5141079 -0.49112812\n## HQ(n)  -0.3017869 -0.4208439 -0.4450407 -0.4082080 -0.3761491 -0.32808581\n## SC(n)  -0.2462608 -0.3283005 -0.3154798 -0.2416298 -0.1725534 -0.08747275\n## FPE(n)  0.7121914  0.6165990  0.5869659  0.5939325  0.5981364  0.61210908\nestimated.var <- VAR(data,p=3)\n#print(estimated.var$varresult)\nPhi <- Acoef(estimated.var)\nPHI <- make.PHI(Phi) # autoregressive matrix of companion form.\nprint(abs(eigen(PHI)$values)) # check stationarity## [1] 0.9114892 0.9114892 0.6319554 0.4759403 0.4759403 0.3246995"},{"path":"basics.html","id":"BlockGranger","chapter":"1 VARs and IRFs: the basics","heading":"1.5 Block exogeneity and Granger causality","text":"","code":""},{"path":"basics.html","id":"block-exogeneity","chapter":"1 VARs and IRFs: the basics","heading":"1.5.1 Block exogeneity","text":"Let’s decompose \\(y_t\\) two subvectors \\(y^{(1)}_{t}\\) (\\(n_1 \\times 1\\)) \\(y^{(2)}_{t}\\) (\\(n_2 \\times 1\\)), \\(y_t' = [{y^{(1)}_{t}}',{y^{(2)}_{t}}']\\) (therefore \\(n=n_1 +n_2\\)), :\n\\[\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t}\\\\\ny^{(2)}_{t}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\n\\Phi^{(1,1)} & \\Phi^{(1,2)}\\\\\n\\Phi^{(2,1)} & \\Phi^{(2,2)}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny^{(1)}_{t-1}\\\\\ny^{(2)}_{t-1}\n\\end{array}\n\\right] + \\varepsilon_t.\n\\]\nUsing, e.g., likelihood ratio test (see Def. 11.2), one can easily test block exogeneity \\(y_t^{(2)}\\) (say). null assumption can expressed \\(\\Phi^{(2,1)}=0\\).","code":""},{"path":"basics.html","id":"granger-causality","chapter":"1 VARs and IRFs: the basics","heading":"1.5.2 Granger Causality","text":"Granger (1969) developed method explore causal relationships among variables. approach consists determining whether past values \\(y_{1,t}\\) can help explain current \\(y_{2,t}\\) (beyond information already included past values \\(y_{2,t}\\)).Formally, let us denote three information sets:\n\\[\\begin{eqnarray*}\n\\mathcal{}_{1,t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{2,t} & = & \\left\\{ y_{2,t},y_{2,t-1},\\ldots\\right\\} \\\\\n\\mathcal{}_{t} & = & \\left\\{ y_{1,t},y_{1,t-1},\\ldots y_{2,t},y_{2,t-1},\\ldots\\right\\}.\n\\end{eqnarray*}\\]\nsay \\(y_{1,t}\\) Granger-causes \\(y_{2,t}\\) \n\\[\n\\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{2,t-1}\\right]\\neq \\mathbb{E}\\left[y_{2,t}\\mid \\mathcal{}_{t-1}\\right].\n\\]get intuition behind testing procedure, consider following\nbivariate VAR(\\(p\\)) process:\n\\[\\begin{eqnarray*}\ny_{1,t} & = & c_1+\\Sigma_{=1}^{p}\\Phi_i^{(11)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(12)}y_{2,t-}+\\varepsilon_{1,t}\\\\\ny_{2,t} & = & c_2+\\Sigma_{=1}^{p}\\Phi_i^{(21)}y_{1,t-}+\\Sigma_{=1}^{p}\\Phi_i^{(22)}y_{2,t-}+\\varepsilon_{2,t},\n\\end{eqnarray*}\\]\n\\(\\Phi_k^{(ij)}\\) denotes element \\((,j)\\) \\(\\Phi_k\\). , \\(y_{1,t}\\) said Granger-cause \\(y_{2,t}\\) \n\\[\n\\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0.\n\\]\nnull alternative hypotheses therefore :\n\\[\n\\begin{cases}\nH_{0}: & \\Phi_1^{(21)}=\\Phi_2^{(21)}=\\ldots=\\Phi_p^{(21)}=0\\\\\nH_{1}: & \\Phi_1^{(21)}\\neq0\\mbox{ }\\Phi_2^{(21)}\\neq0\\mbox{ }\\ldots\\Phi_p^{(21)}\\neq0.\\end{cases}\n\\]\nLoosely speaking, reject \\(H_{0}\\) coefficients lagged \\(y_{1,t}\\)’s statistically significant. Formally, can tested using \\(F\\)-test asymptotic chi-square test. \\(F\\)-statistic \n\\[\nF=\\frac{(RSS-USS)/p}{USS/(T-2p-1)},\n\\]\nRSS Restricted sum squared residuals USS Unrestricted sum squared residuals. \\(H_{0}\\), \\(F\\)-statistic distributed \\(\\mathcal{F}(p,T-2p-1)\\) (See Table 11.4).1According following lines code, output gap Granger-causes inflation, reverse true:","code":"\ngrangertest(US3var[,c(\"y.gdp.gap\",\"infl\")],order=3)## Granger causality test\n## \n## Model 1: infl ~ Lags(infl, 1:3) + Lags(y.gdp.gap, 1:3)\n## Model 2: infl ~ Lags(infl, 1:3)\n##   Res.Df Df      F   Pr(>F)   \n## 1    214                      \n## 2    217 -3 3.9761 0.008745 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ngrangertest(US3var[,c(\"infl\",\"y.gdp.gap\")],order=3)## Granger causality test\n## \n## Model 1: y.gdp.gap ~ Lags(y.gdp.gap, 1:3) + Lags(infl, 1:3)\n## Model 2: y.gdp.gap ~ Lags(y.gdp.gap, 1:3)\n##   Res.Df Df      F Pr(>F)\n## 1    214                 \n## 2    217 -3 1.5451 0.2038"},{"path":"identifStruct.html","id":"identifStruct","chapter":"2 Identification problem and standard identification techniques","heading":"2 Identification problem and standard identification techniques","text":"","code":""},{"path":"identifStruct.html","id":"IdentifPbm","chapter":"2 Identification problem and standard identification techniques","heading":"2.1 The identification problem","text":"Section 1.4, seen estimate \\(\\mathbb{V}ar(\\varepsilon_t) =\\Omega\\) \\(\\Phi_k\\) matrices context VAR model. IRFs functions \\(B\\) \\(\\Phi_k\\)’s, \\(\\Omega\\) \\(\\Phi_k\\)’s (see Section 1.2). \\(\\Omega = BB'\\), provides restrictions components \\(B\\), sufficient fully identify \\(B\\). Indeed, seen system equations whose unknowns \\(b_{,j}\\)’s (components \\(B\\)), system \\(\\Omega = BB'\\) contains \\(n(n+1)/2\\) linearly independent equations. instance, \\(n=2\\):\n\\[\\begin{eqnarray*}\n&&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11} & b_{12} \\\\\nb_{21} & b_{22}\n\\end{array}\n\\right]\\left[\n\\begin{array}{cc}\nb_{11} & b_{21} \\\\\nb_{12} & b_{22}\n\\end{array}\n\\right]\\\\\n&\\Leftrightarrow&\\left[\n\\begin{array}{cc}\n\\omega_{11} & \\omega_{12} \\\\\n\\omega_{12} & \\omega_{22}\n\\end{array}\n\\right] = \\left[\n\\begin{array}{cc}\nb_{11}^2+b_{12}^2 & \\color{red}{b_{11}b_{21}+b_{12}b_{22}} \\\\\n\\color{red}{b_{11}b_{21}+b_{12}b_{22}} & b_{22}^2 + b_{21}^2\n\\end{array}\n\\right].\n\\end{eqnarray*}\\]3 linearly independent equations 4 unknowns. Therefore, \\(B\\) identified based second-order moments. Additional restrictions required identify \\(B\\). section covers two standard identification schemes: short-run long-run restrictions.short-run restriction (SRR) prevents structural shock affecting endogenous variable contemporaneously.easy implement: appropriate entries \\(B\\) set 0.particular (popular) case Cholesky, recursive approach.Examples include Bernanke (1986), Sims (1986), Galí (1992), Ruibio-Ramírez, Waggoner, Zha (2010).long-run restriction (LRR) prevents structural shock cumulative impact one endogenous variables.Additional computations required implement . One needs compute cumulative effect one structural shocks \\(u_{t}\\) one endogenous variable.Examples include Blanchard Quah (1989), Faust Leeper (1997), Galí (1999), Erceg, Guerrieri, Gust (2005), Christiano, Eichenbaum, Vigfusson (2007).two approaches can combined (see, e.g., Gerlach Smets (1995)).","code":""},{"path":"identifStruct.html","id":"a-stylized-example-motivating-short-run-restrictions","chapter":"2 Identification problem and standard identification techniques","heading":"2.2 A stylized example motivating short-run restrictions","text":"Let us consider simple example motivate short-run restrictions. Consider following stylized macro model:\n\\[\\begin{equation}\n\\begin{array}{clll}\ng_{t}&=& \\bar{g}-\\lambda(i_{t-1}-\\mathbb{E}_{t-1}\\pi_{t})+ \\underbrace{{\\color{blue}\\sigma_d \\eta_{d,t}}}_{\\mbox{demand shock}}& (\\mbox{curve})\\\\\n\\Delta \\pi_{t} & = & \\beta (g_{t} - \\bar{g})+ \\underbrace{{\\color{blue}\\sigma_{\\pi} \\eta_{\\pi,t}}}_{\\mbox{cost push shock}} & (\\mbox{Phillips curve})\\\\\ni_{t} & = & \\rho i_{t-1} + \\left[ \\gamma_\\pi \\mathbb{E}_{t}\\pi_{t+1}  + \\gamma_g (g_{t} - \\bar{g}) \\right]\\\\\n&& \\qquad \\qquad+\\underbrace{{\\color{blue}\\sigma_{mp} \\eta_{mp,t}}}_{\\mbox{Mon. Pol. shock}} & (\\mbox{Taylor rule}),\n\\end{array}\\tag{2.1}\n\\end{equation}\\]\n:\n\\[\\begin{equation}\n\\eta_t =\n\\left[\n\\begin{array}{c}\n\\eta_{\\pi,t}\\\\\n\\eta_{d,t}\\\\\n\\eta_{mp,t}\n\\end{array}\n\\right]\n\\sim ..d.\\,\\mathcal{N}(0,).\\tag{2.2}\n\\end{equation}\\]Vector \\(\\eta_t\\) assumed vector structural shocks, mutually serially independent. date \\(t\\):\\(g_t\\) contemporaneously affected \\(\\eta_{d,t}\\) ;\\(\\pi_t\\) contemporaneously affected \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\);\\(i_t\\) contemporaneously affected \\(\\eta_{mp,t}\\), \\(\\eta_{\\pi,t}\\) \\(\\eta_{d,t}\\).System (2.1) rewritten follows:\n\\[\\begin{equation}\n\\left[\\begin{array}{c}\nd_t\\\\\n\\pi_t\\\\\ni_t\n\\end{array}\\right]\n= \\Phi(L)\n\\left[\\begin{array}{c}\nd_{t-1}\\\\\n\\pi_{t-1}\\\\\ni_{t-1} +\n\\end{array}\\right] +\\underbrace{\\underbrace{\n\\left[\n\\begin{array}{ccc}\n0 & \\bullet & 0 \\\\\n\\bullet & \\bullet & 0 \\\\\n\\bullet & \\bullet & \\bullet\n\\end{array}\n\\right]}_{=B} \\eta_t.}_{=\\varepsilon_t}\\tag{2.3}\n\\end{equation}\\]reduced-form model. representation suggests three additional restrictions entries \\(B\\); latter matrix therefore identified soon \\(\\Omega = BB'\\) known (signs columns).","code":""},{"path":"identifStruct.html","id":"cholesky-a-specific-short-run-restriction-situation","chapter":"2 Identification problem and standard identification techniques","heading":"2.3 Cholesky: a specific short-run-restriction situation","text":"particular cases well-known matrix decomposition \\(\\Omega=\\mathbb{V}ar(\\varepsilon_t)\\) can used easily estimate specific SVAR. case -called Cholesky decomposition. Consider following context:first shock (say, \\(\\eta_{n_1,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) one endogenous variable (say, \\(y_{n_1,t}\\));second shock (say, \\(\\eta_{n_2,t}\\)) can affect instantaneously\n(.e., date \\(t\\)) two endogenous variables, \\(y_{n_1,t}\\) () \\(y_{n_2,t}\\);\\(\\dots\\)impliesthat column \\(n_1\\) \\(B\\) 1 non-zero entry (\\(n_1^{th}\\) entry),column \\(n_2\\) \\(B\\) 2 non-zero entries (\\(n_1^{th}\\) \\(n_2^{th}\\) ones), etc.Without loss generality, can set \\(n_1=n\\), \\(n_2=n-1\\), etc. context, matrix \\(B\\) lower triangular. Cholesky decomposition \\(\\Omega_{\\varepsilon}\\) provides appropriate estimate \\(B\\), since matrix decomposition yields lower triangular matrix satisfying:\n\\[\n\\Omega_\\varepsilon = BB'.\n\\]instance, Dedola Lippi (2005) estimate 5 structural VAR models US, UK, Germany, France Italy analyse monetary-policy transmission mechanisms. estimate SVAR(5) models period 1975-1997. shock-identification scheme based Cholesky decompositions, ordering endogenous variables : industrial production, consumer price index, commodity price index, short-term rate, monetary aggregate effective exchange rate (except US). ordering implies monetary policy reacts shocks affecting first three variables latter react monetary policy shocks one-period lag .Cholesky approach can employed one interested one specific structural shock. case, e.g., Christiano, Eichenbaum, Evans (1996). identification based following relationship \\(\\varepsilon_t\\) \\(\\eta_t\\):\n\\[\n\\left[\\begin{array}{c}\n\\boldsymbol\\varepsilon_{S,t}\\\\\n\\varepsilon_{r,t}\\\\\n\\boldsymbol\\varepsilon_{F,t}\n\\end{array}\\right] =\n\\left[\\begin{array}{ccc}\nB_{SS} & 0 & 0 \\\\\nB_{rS} & B_{rr} & 0 \\\\\nB_{FS} & B_{Fr} & B_{FF}\n\\end{array}\\right]\n\\left[\\begin{array}{c}\n\\boldsymbol\\eta_{S,t}\\\\\n\\eta_{r,t}\\\\\n\\boldsymbol\\eta_{F,t}\n\\end{array}\\right],\n\\]\n\\(S\\), \\(r\\) \\(F\\) respectively correspond slow-moving variables, policy variable (short-term rate) fast-moving variables. \\(\\eta_{r,t}\\) scalar, \\(\\boldsymbol\\eta_{S,t}\\) \\(\\boldsymbol\\eta_{F,t}\\) may vectors. space spanned \\(\\boldsymbol\\varepsilon_{S,t}\\) spanned \\(\\boldsymbol\\eta_{S,t}\\). result, \\(\\varepsilon_{r,t}\\) linear combination \\(\\eta_{r,t}\\) \\(\\boldsymbol\\eta_{S,t}\\) (\\(\\perp\\)), comes \\(B_{rr}\\eta_{r,t}\\)’s (population) residuals regression \\(\\varepsilon_{r,t}\\) \\(\\boldsymbol\\varepsilon_{S,t}\\). \\(\\mathbb{V}ar(\\eta_{r,t})=1\\), \\(B_{rr}\\) given square root variance \\(B_{rr}\\eta_{r,t}\\). \\(B_{F,r}\\) finally obtained regressing components \\(\\boldsymbol\\varepsilon_{F,t}\\) estimates \\(\\eta_{r,t}\\).equivalent approach consists computing Cholesky decomposition \\(BB'\\) contemporaneous impacts monetary policy shock (\\(n\\) endogenous variables) components column \\(B\\) corresponding policy variable.\nFigure 2.1: Response monetary-policy shock. Identification approach Christiano, Eichenbaum Evans (1996). Confidence intervals obtained boostrapping estimated VAR model (see inference section).\n","code":"\nlibrary(IdSS)\nlibrary(vars)\ndata(\"USmonthly\")\n# Select sample period:\nFirst.date <- \"1965-01-01\";Last.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables <- c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\nres.svar.ordering <- svar.ordering(y,p=3,\n                                   posit.of.shock = 5,\n                                   nb.periods.IRF = 20,\n                                   nb.bootstrap.replications = 100,\n                                   confidence.interval = 0.90, # expressed in pp.\n                                   indic.plot = 1 # Plots are displayed if = 1.\n)"},{"path":"identifStruct.html","id":"long-run-restrictions","chapter":"2 Identification problem and standard identification techniques","heading":"2.4 Long-run restrictions","text":"Let us now turn Long-run restrictions. restriction concerns long-run influence shock endogenous variable. Let us consider instance structural shock assumed “long-run influence” GDP. express ? long-run change GDP can expressed \\(GDP_{t+h} - GDP_t\\), \\(h\\) large. Note :\n\\[\nGDP_{t+h} - GDP_t = \\Delta GDP_{t+h} +\\Delta GDP_{t+h-1} + \\dots + \\Delta GDP_{t+1}.\n\\]\nHence, fact given structural shock (\\(\\eta_{,t}\\), say) long-run influence GDP means \n\\[\n\\lim_{h\\rightarrow\\infty}\\frac{\\partial GDP_{t+h}}{\\partial \\eta_{,t}} = \\lim_{h\\rightarrow\\infty} \\frac{\\partial}{\\partial \\eta_{,t}}\\left(\\sum_{k=1}^h \\Delta  GDP_{t+k}\\right)= 0.\n\\]long-run effect can formulated function \\(B\\) matrices \\(\\Phi_i\\) \\(y_t\\) (including \\(\\Delta GDP_t\\)) follows VAR process.Without loss generality, consider VAR(1) case. Indeed, one can always write VAR(\\(p\\)) VAR(1): , stack last \\(p\\) values vector \\(y_t\\) vector \\(y_{t}^{*}=[y_t',\\dots,y_{t-p+1}']'\\); Eq. (1.1) can rewritten companion form:\n\\[\\begin{equation}\ny_{t}^{*} =\n\\underbrace{\\left[\\begin{array}{c}\nc\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{=c^*}+\n\\underbrace{\\left[\\begin{array}{cccc}\n\\Phi_{1} & \\Phi_{2} & \\cdots & \\Phi_{p}\\\\\n& 0 & \\cdots & 0\\\\\n0 & \\ddots & 0 & 0\\\\\n0 & 0 & & 0\\end{array}\\right]}_{=\\Phi}\ny_{t-1}^{*}+\n\\underbrace{\\left[\\begin{array}{c}\n\\varepsilon_{t}\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right]}_{\\varepsilon_t^*},\\tag{2.4}\n\\end{equation}\\]\nmatrices \\(\\Phi\\) \\(\\Omega^* = \\mathbb{V}ar(\\varepsilon_t^*)\\) dimension \\(np \\times np\\); \\(\\Omega^*\\) filled zeros, except \\(n\\times n\\) upper-left block equal \\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\). (Matrix \\(\\Phi\\) introduced Eq. (1.8).)Let us focus VAR(1) case:\n\\[\\begin{eqnarray*}\ny_{t} &=& c+\\Phi y_{t-1}+\\varepsilon_{t}\\\\\n& = & c+\\varepsilon_{t}+\\Phi(c+\\varepsilon_{t-1})+\\ldots+\\Phi^{k}(c+\\varepsilon_{t-k})+\\ldots \\\\\n& = & \\mu +\\varepsilon_{t}+\\Phi\\varepsilon_{t-1}+\\ldots+\\Phi^{k}\\varepsilon_{t-k}+\\ldots \\\\\n& = & \\mu +B\\eta_{t}+\\Phi B\\eta_{t-1}+\\ldots+\\Phi^{k}B\\eta_{t-k}+\\ldots,\n\\end{eqnarray*}\\]\nWold representation \\(y_t\\).sequence shocks \\(\\{\\eta_t\\}\\) determines sequence \\(\\{y_t\\}\\). \\(\\{\\eta_t\\}\\) replaced \\(\\{\\tilde{\\eta}_t\\}\\), \\(\\tilde{\\eta}_t=\\eta_t\\) \\(t \\ne s\\) \\(\\tilde{\\eta}_s=\\eta_s + \\gamma\\)? Assume \\(\\{\\tilde{y}_t\\}\\) associated “perturbated” sequence. \\(\\tilde{y}_t = y_t\\) \\(t<s\\). \\(t \\ge s\\), Wold decomposition \\(\\{\\tilde{y}_t\\}\\) implies:\n\\[\n\\tilde{y}_t = y_t + \\Phi^{t-s} B \\gamma.\n\\]\nTherefore, cumulative impact \\(\\gamma\\) \\(\\tilde{y}_t\\) (\\(t \\ge s\\)):\n\\[\\begin{eqnarray}\n(\\tilde{y}_t - y_t) +  (\\tilde{y}_{t-1} - y_{t-1}) + \\dots +  (\\tilde{y}_s - y_s) &=& \\nonumber \\\\\n(Id + \\Phi + \\Phi^2 + \\dots + \\Phi^{t-s}) B \\gamma.&& \\tag{2.5}\n\\end{eqnarray}\\]Consider shock \\(\\eta_{1,t}\\), magnitude \\(1\\). shock corresponds \\(\\gamma = [1,0,\\dots,0]'\\). Given Eq. (2.5), long-run cumulative effect shock endogenous variables given :\n\\[\n\\underbrace{(Id+\\Phi+\\ldots+\\Phi^{k}+\\ldots)}_{=(Id - \\Phi)^{-1}}B\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n\\vdots\\\\\n0\\end{array}\\right],\n\\]\nfirst column \\(n \\times n\\) matrix \\(\\Theta \\equiv (Id - \\Phi)^{-1}B\\).context, consider following long-run restriction: “\\(j^{th}\\) structural shock cumulative impact \\(^{th}\\) endogenous variable”. equivalent \n\\[\n\\Theta_{ij}=0,\n\\]\n\\(\\Theta_{ij}\\) element \\((,j)\\) \\(\\Theta\\).Blanchard Quah (1989) implemented long-run restrictions small-scale VAR. Two variables considered: GDP unemployment. Consequently, VAR affected two types shocks. Specifically, authors want identify supply shocks (can permanent effect output) demand shocks (permanent effect output).2Blanchard Quah (1989)’s dataset quarterly, spanning period 1950:2 1987:4. VAR features 8 lags. data use:Estimate reduced-form VAR(8) model:Now, let us define loss function (loss) equal zero () \\(BB'=\\Omega\\) (b) element (1,1) \\(\\Theta = (Id - \\Phi)^{-1} B\\) equal zero:(Note: one can use type approach, based loss function, mix short- long-run restrictions.)Figure 2.2 displays resulting IRFs. Note , GDP, cumulate GDP growth IRF, response GDP level.\nFigure 2.2: IRF GDP unemployment demand supply shocks.\n","code":"\nlibrary(IdSS)\ndata(BQ)\npar(mfrow=c(1,2))\nplot(BQ$Date,BQ$Dgdp,type=\"l\",main=\"GDP quarterly growth rate\",\n     xlab=\"\",ylab=\"\",lwd=2)\nplot(BQ$Date,BQ$unemp,type=\"l\",ylim=c(-3,6),main=\"Unemployment rate (gap)\",\n     xlab=\"\",ylab=\"\",lwd=2)\nlibrary(vars)\ny <- BQ[,2:3]\nest.VAR <- VAR(y,p=8)\nOmega <- var(residuals(est.VAR))\n# Compute (Id - Phi)^{-1}:\nPhi <- Acoef(est.VAR)\nPHI <- make.PHI(Phi)\nsum.PHI.k <- solve(diag(dim(PHI)[1]) - PHI)[1:2,1:2]\nloss <- function(param){\n  B <- matrix(param,2,2)\n  X <- Omega - B %*% t(B)\n  Theta <- sum.PHI.k[1:2,1:2] %*% B\n  loss <- 10000 * ( X[1,1]^2 + X[2,1]^2 + X[2,2]^2 + Theta[1,1]^2 )\n  return(loss)\n}\nres.opt <- optim(c(1,0,0,1),loss,method=\"BFGS\",hessian=FALSE)\nprint(res.opt$par)## [1]  0.8570358 -0.2396345  0.1541395  0.1921221\nB.hat <- matrix(res.opt$par,2,2)\nprint(cbind(Omega,B.hat %*% t(B.hat)))##             Dgdp       unemp                       \n## Dgdp   0.7582704 -0.17576173  0.7582694 -0.17576173\n## unemp -0.1757617  0.09433658 -0.1757617  0.09433558\nnb.sim <- 40\npar(mfrow=c(2,2));par(plt=c(.15,.95,.15,.8))\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(1,0))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Demand shock on UNEMP\")\nY <- simul.VAR(c=matrix(0,2,1),Phi,B.hat,nb.sim,y0.star=rep(0,2*8),\n               indic.IRF = 1,u.shock = c(0,1))\nplot(cumsum(Y[,1]),type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on GDP\")\nplot(Y[,2],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",main=\"Supply shock on UNEMP\")"},{"path":"Inference.html","id":"Inference","chapter":"3 Inference","heading":"3 Inference","text":"Consider following SVAR model:\n\\[y_t = \\Phi_1 y_{t-1} + \\dots + \\Phi_p y_{t-p} + \\varepsilon_t\\]\n\\(\\varepsilon_t=B\\eta_t\\), \\(\\Omega_\\varepsilon=BB'\\).corresponding infinite MA representation (Eq. (1.4), Wold representation, :\n\\[\ny_t = \\sum_{h=0}^\\infty\\Psi_h \\eta_{t-h},\n\\]\n\\(\\Psi_0=B\\) \\(h=1,2,\\dots\\):\n\\[\n\\Psi_h = \\sum_{j=1}^h\\Psi_{h-j}\\Phi_j,\n\\]\n\\(\\Phi_j=0\\) \\(j>p\\) (see Prop. 1.1 recursive computation \\(\\Psi_j\\)’s).Inference VAR coefficients \\(\\{\\Phi_j\\}_{j=1,...,p}\\) straightforward (standard OLS inference). inference complicated regarding IRFs. Indeed, shown previous equation, (infinite) MA coefficients \\(\\{\\Psi_j\\}_{j=1,...}\\) non-linear functions \\(\\{\\Phi_j\\}_{j=1,...,p}\\) \\(\\Omega_\\varepsilon\\). issue pertain small sample bias: typically, persistent processes, auto-regressive parameters known downward biased.main inference methods following:Monte Carlo method (Hamilton (1994))Asymptotic normal approximation (Lütkepohl (1990)), Delta methodBootstrap method (Kilian (1998))","code":""},{"path":"Inference.html","id":"MonteCarlo","chapter":"3 Inference","heading":"3.1 Monte Carlo method","text":"use Monte Carlo need approximate distribution variable whose distribution unknown (: \\(\\Psi_j\\)’s) function another variable whose distribution known (, \\(\\Phi_j\\)’s).instance, suppose know distribution random variable \\(X\\), takes values \\(\\mathbb{R}\\), density function \\(p\\). Assume want compute mean \\(\\varphi(X)\\). :\n\\[\n\\mathbb{E}(\\varphi(X))=\\int_{-\\infty}^{+\\infty}\\varphi(x)p(x)dx\n\\]\nSuppose integral simple expression. compute \\(\\mathbb{E}(\\varphi(X))\\) , virtue law large numbers, can approximate follows:\n\\[\n\\mathbb{E}(\\varphi(X))\\approx\\frac{1}{N}\\sum_{=1}^N\\varphi(X^{()}),\n\\]\n\\(\\{X^{()}\\}_{=1,...,N}\\) \\(N\\) independent draws \\(X\\). generally, distribution \\(\\varphi(X)\\) can approximated empirical distribution \\(\\varphi(X^{()})\\)’s. Typically, 10’000 values \\(\\varphi(X^{()})\\) drawn, \\(5^{th}\\) percentile p.d.f. \\(\\varphi(X)\\) can approximated \\(500^{th}\\) value 10’000 draws \\(\\varphi(X^{()})\\) (arranging values ascending order).regards computation confidence intervals around IRFs, one think \\(\\{\\widehat{\\Phi}_j\\}_{j=1,...,p}\\), \\(\\widehat{\\Omega}\\) \\(X\\) \\(\\{\\widehat{\\Psi}_j\\}_{j=1,...}\\) \\(\\varphi(X)\\). (Proposition 1.4 provides us asymptotic distribution “\\(X\\).”)summarize, steps one can implement derive confidence intervals IRFs using Monte-Carlo approach: iteration \\(k\\),Draw \\(\\{\\widehat{\\Phi}_j^{(k)}\\}_{j=1,...,p}\\) \\(\\widehat{\\Omega}^{(k)}\\) asymptotic distribution (using Proposition 1.4).Compute matrix \\(B^{(k)}\\) \\(\\widehat{\\Omega}^{(k)}=B^{(k)}B^{(k)'}\\), according identification strategy.Compute associated IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).following code implements Monte Carlo method.\nFigure 3.1: IRF associated monetary policy shock; Monte Carlo method.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with different inference methods\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 3,# 0 -> no inference, 1 -> parametric bootst.,\n                  # 2 <- non-parametric bootstrap, 3 <- Monte Carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering <- res.svar.ordering$IRFs\nmedian.IRFs.ordering <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering <- res.svar.ordering$simulated.IRFs"},{"path":"Inference.html","id":"delta-method","chapter":"3 Inference","heading":"3.2 Delta method","text":"Suppose \\(\\beta\\) vector parameters \\(\\beta\\) estimator \n\\[\n\\sqrt{T}(\\hat\\beta-\\beta)\\overset{d}{\\rightarrow}\\mathcal{N}(0,\\Sigma_\\beta),\n\\]\n\\(d\\) denotes convergence distribution, \\(N(0,\\Sigma_\\beta)\\) denotes multivariate normal distribution mean vector 0 covariance matrix \\(\\Sigma_\\beta\\) \\(T\\) size sample used estimation.Let \\(g(\\beta) = (g_l(\\beta),..., g_m(\\beta))'\\) continuously differentiable function values \\(\\mathbb{R}^m\\), assume \\(\\partial g_i/\\partial \\beta' = (\\partial g_i/\\partial \\beta_j)\\) nonzero \\(\\beta\\) \\(= 1,\\dots, m\\). \n\\[\n\\sqrt{T}(g(\\hat\\beta)-g(\\beta))\\overset{d}{\\rightarrow}\\mathcal{N}\\left(0,\\frac{\\partial g}{\\partial \\beta'}\\Sigma_\\beta\\frac{\\partial g'}{\\partial \\beta}\\right).\n\\]\nUsing property, Lütkepohl (1990) provides asymptotic distributions \\(\\Psi_j\\)’s. following lines code can used get approximate confidence intervals IRFs.limit last two approaches (Monte Carlo Delta method) rely asymptotic results normality assumption. Boostrapping approaches robust small-sample non-normal situations.","code":"\nirf.function <- function(THETA){\n  c <- THETA[1]\n  phi <- THETA[2:(p+1)]\n  if(q>0){\n    theta <- c(1,THETA[(1+p+1):(1+p+q)])\n  }else{theta <- 1}\n  sigma <- THETA[1+p+q+1]\n  r <- dim(Matrix.of.Exog)[2] - 1\n  beta <- THETA[(1+p+q+1+1):(1+p+q+1+(r+1))]\n  \n  irf <- sim.arma(0,phi,beta,sigma=sd(Ramey$ED3_TC,na.rm=TRUE),T=60,\n                  y.0=rep(0,length(x$phi)),nb.sim=1,make.IRF=1,\n                  X=NaN,beta=NaN)\n  return(irf)}\nIRF.0 <- 100*irf.function(x$THETA)\neps <- .00000001\nd.IRF <- NULL\nfor(i in 1:length(x$THETA)){\n  THETA.i <- x$THETA\n  THETA.i[i] <- THETA.i[i] + eps\n  IRF.i <- 100*irf.function(THETA.i)\n  d.IRF <- cbind(d.IRF,\n                 (IRF.i - IRF.0)/eps)}\nmat.var.cov.IRF <- d.IRF %*% x$I %*% t(d.IRF)"},{"path":"Inference.html","id":"bootstrap","chapter":"3 Inference","heading":"3.3 Bootstrap","text":"IRFs’ confidence intervals intervals 90% (95%, 75%, …) IRFs lie, repeat estimation large number times similar conditions (\\(T\\) observations). obviously , one sample: \\(\\{y_t\\}_{t=1,..,T}\\). can try construct samples.Bootstrapping consists :re-sampling \\(N\\) times, .e., constructing \\(N\\) samples \\(T\\) observations, using estimated\nVAR coefficients anda sample residuals distribution \\(N(0,BB')\\) (parametric approach), ora sample residuals drawn randomly set actual estimated residuals \\(\\{\\hat\\varepsilon_t\\}_{t=1,..,T}\\). (non-parametric approach).re-estimating SVAR \\(N\\) times.algorithm non-parametric approach:Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\). (Note: parametric approach, draw \\(\\hat\\varepsilon_{t}^{(k)}\\) \\(N(0,BB')\\) distribution)Re-estimate SVAR compute IRFs \\(\\{\\widehat{\\Psi}_j\\}^{(k)}\\).Perform \\(N\\) replications report median impulse response (confidence intervals).following code implements bootstrap method.\nFigure 3.2: IRF associated monetary policy shock; bootstrap method.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with different inference methods\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 2,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering.bootstrap <- res.svar.ordering$IRFs\nmedian.IRFs.ordering.bootstrap <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering.bootstrap <- res.svar.ordering$simulated.IRFs"},{"path":"Inference.html","id":"bootstrap-after-bootstrap","chapter":"3 Inference","heading":"3.4 Bootstrap-after-bootstrap","text":"previous simple bootstrapping procedure deals non-normality small sample distribution, since use actual residuals. However, deal small sample bias, stemming, particular, small-sample bias associated OLS coefficient estimates \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\).code illustrates small sample bias.\nFigure 3.3: Estimated bootstrapped coefficients.\ndistribution bootstrapped coefficients centered around estimated coefficients.following code, perform VAR estimation bootstrap inference generating artificial data. can compare IRFs confidence intervals ``true’’ parameters used generate data.\nFigure 3.4: Simulated IRF associated monetary policy shock.\n\nFigure 3.5: IRF associated monetary policy shock; sign-restriction approach.\nmain idea bootstrap--bootstrap Kilian (1998) run two consecutive boostraps: objective first compute bias, can used correct initial estimates \\(\\Phi_i\\)’s. , corrected estimates used —second boostrap— compute set IRFs (standard boostrap).formally, algorithm follows:Estimate SVAR coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}\\) \\(\\widehat{\\Omega}\\)First bootstrap. iteration \\(k\\):Construct sample\n\\[\ny_t^{(k)}=\\widehat{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widehat{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)},\n\\]\n\\(\\hat\\varepsilon_{t}^{(k)}=\\hat\\varepsilon_{s_t^{(k)}}\\), \\(\\{s_1^{(k)},..,s_T^{(k)}\\}\\) random set \\(\\{1,..,T\\}^T\\).Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^{(k)}\\).Perform \\(N\\) replications compute median coefficients \\(\\{\\widehat{\\Phi}_j\\}_{j=1,..,p}^*\\).Approximate bias terms \\(\\widehat{\\Theta}_j=\\widehat{\\Phi}_j^*-\\widehat{\\Phi}_j\\).Construct bias-corrected terms \\(\\widetilde{\\Phi}_j=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j\\).Second bootstrap. iteration \\(k\\):Construct sample now \n\\[\ny_t^{(k)}=\\widetilde{\\Phi}_1 y_{t-1}^{(k)} + \\dots + \\widetilde{\\Phi}_p y_{t-p}^{(k)} + \\hat\\varepsilon_t^{(k)}.\n\\]Re-estimate VAR compute coefficients \\(\\{\\widehat{\\Phi}^*_j\\}_{j=1,..,p}^{(k)}\\).Construct bias-corrected estimates \\(\\widetilde{\\Phi}_j^{*(k)}=\\widehat{\\Phi}_j^{*(k)}-\\widehat{\\Theta}_j\\).Compute associated IRFs \\(\\{\\widetilde{\\Psi}_j^{*(k)}\\}_{j\\ge 1}\\).Perform \\(N\\) replications compute median confidence interval set IRFs.noted correcting bias can generate non-stationary results (\\(\\tilde \\Phi\\) eigenvalue modulus \\(>1\\)). Solution (Kilian (1998)):step 5, check largest eigenvalue \\(\\tilde\\Phi\\) modulus <1.\n, shrink bias: \\(j\\)s, set \\(\\widehat{\\Theta}_j^{(+1)}=\\delta_{+1}\\widehat{\\Theta}_j^{()}\\), \\(\\delta_{+1}=\\delta_i-0.01\\), starting \\(\\delta_1=1\\) \\(\\widehat{\\Theta}_j^{(1)} =\\widehat{\\Theta}_j\\), compute \\(\\widetilde{\\Phi}_j^{(+1)}=\\widehat{\\Phi}_j-\\widehat{\\Theta}_j^{(+1)}\\) largest eigenvalue \\(\\tilde\\Phi^{(+1)}\\) modulus <1.following code implements bootstrap--bootrap method.\nFigure 3.6: IRF associated monetary policy shock; sign-restriction approach.\nalternative, function VAR.Boot package VAR.etp (Kim (2022)) can used operate bias-correction approach Kilian (1998):","code":"\n# Distribution of coefficients stemming from non-parametric bootstrap\nn <- length(considered.variables)\nh <- 5\npar(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\nfor (i in 1:n){\n  hist(simulated.IRFs.ordering.bootstrap[h,i,],xlab=\"\",ylab=\"\",\n       main=paste(\"Effect at h = \",h,\" on \",\n                  considered.variables[i],sep=\"\"),cex.main=.9)\n  lines(array(c(IRFs.ordering.bootstrap[h,i],\n                IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=\"red\")\n  lines(array(c(median.IRFs.ordering.bootstrap[h,i],\n                median.IRFs.ordering.bootstrap[h,i],0,100),c(2,2)),col=\"blue\")\n  text(IRFs.ordering.bootstrap[h,i],25,label=\"Estimated coef.\",col=\"red\")\n}\n# Simulate a small sample\nest.VAR <- VAR(y,p=3)\nPhi     <- Acoef(est.VAR)\ncst     <- Bcoef(est.VAR)[,3*n+1]\nresids  <- residuals(est.VAR)\nOmega   <- var(resids)\nB.hat   <- t(chol(Omega))\ny0.star <- NULL\nfor(k in 3:1){\n  y0.star <- c(y0.star,y[k,])\n}\nsmall.sample <- simul.VAR(c=rep(0,dim(y)[2]),\n                          Phi,\n                          B.hat,\n                          nb.sim = 100,\n                          y0.star,\n                          indic.IRF = 0)\ncolnames(small.sample)  <- considered.variables\n# Estimate the VAR with the small sample\nres.svar.small.sample <-\n  svar.ordering.2(small.sample,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 2,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.small.sample <- res.svar.small.sample$IRFs\nmedian.IRFs.small.sample <- res.svar.small.sample$all.CI.median\nsimulated.IRFs.small.sample <- res.svar.small.sample$simulated.IRFs\n# True IRFs\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 0,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  indic.plot = 0 # Plots are displayed if = 1.\n  )\nIRFs.ordering.true <- res.svar.ordering$IRFs\n\n# Distribution of coefficients resulting from the small sample VAR\nh <- 5\npar(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\nfor (i in 1:n){\n  hist(simulated.IRFs.small.sample[h,i,],xlab=\"\",ylab=\"\",\n       main=paste(\"Effect at h = \",h,\" on \",\n                  considered.variables[i],sep=\"\"),cex.main=.9)\n  lines(array(c(IRFs.small.sample[h,i],\n                IRFs.small.sample[h,i],0,100),c(2,2)),col=\"red\")\n  lines(array(c(median.IRFs.small.sample[h,i],\n                median.IRFs.small.sample[h,i],0,100),c(2,2)),col=\"blue\")\n  lines(array(c(IRFs.ordering.true[h,i],\n                IRFs.ordering.true[h,i],0,100),c(2,2)),col=\"black\")\n  text(IRFs.small.sample[h,i],25,label=\"Estimated coef.\",col=\"red\")\n  text(IRFs.ordering.true[h,i],30,label=\"True coef.\",col=\"black\")\n}\nlibrary(IdSS);library(vars);library(Matrix)\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\ny <- as.matrix(USmonthly[considered.variables])\n# ===================================\n# CEE with different inference methods\n# ===================================\nres.svar.ordering <-\n  svar.ordering.2(y,p=3,\n                  posit.of.shock = 5,\n                  nb.periods.IRF = 20,\n                  inference = 4,# 0 -> no inference, 1 -> parametric bootstr.,\n                  # 2 <- non-parametric bootstrap, 3 <- monte carlo,\n                  # 4 <- bootstrap-after-bootstrap\n                  nb.draws = 200,\n                  confidence.interval = 0.90, # expressed in pp.\n                  indic.plot = 1 # Plots are displayed if = 1.\n  )\nIRFs.ordering <- res.svar.ordering$IRFs\nmedian.IRFs.ordering <- res.svar.ordering$all.CI.median\nsimulated.IRFs.ordering <- res.svar.ordering$simulated.IRFs\nlibrary(VAR.etp)\nlibrary(vars) #standard VAR models\ndata(dat) # part of VAR.etp package\ncorrected <- VAR.Boot(dat,p=2,nb=200,type=\"const\")\nnoncorrec <- VAR(dat,p=2)\nrbind(corrected$coef[1,],\n      (corrected$coef+corrected$Bias)[1,],\n      noncorrec$varresult$inv$coefficients)##         inv(-1)    inc(-1)  con(-1)    inv(-2)  inc(-2)   con(-2)       const\n## [1,] -0.3158957 0.09560053 1.010384 -0.1446485 0.055349 1.0204578 -0.01766718\n## [2,] -0.3196310 0.14598883 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199\n## [3,] -0.3196310 0.14598883 0.961219 -0.1605511 0.114605 0.9343938 -0.01672199"},{"path":"Signs.html","id":"Signs","chapter":"4 Sign restrictions","heading":"4 Sign restrictions","text":"identifiy structural shocks, need find matrix \\(B\\) satisfies \\(\\Omega = BB'\\) (\\(\\Omega = \\mathbb{V}ar(\\varepsilon_t)\\)) restrictions. Indeed, explained , \\(\\Omega = BB'\\) sufficient identify \\(B\\) since, take orthogonal matrix \\(Q\\) (see Def. 4.1), \\(\\mathcal{P}=BQ\\) also satisfies \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\).Definition 4.1  (Orthogonal matrix) orthogonal matrix \\(Q\\) matrix \\(QQ' = ,\\) .e., columns (rows) \\(Q\\) \northogonal unit vectors:\n\\[q_i'q_j=0\\text{ }\\neq j\\text{ }q_i'q_j=1\\text{ }= j,\\]\n\\(q_i\\) \\(^{th}\\) column \\(Q\\).","code":""},{"path":"Signs.html","id":"the-approach","chapter":"4 Sign restrictions","heading":"4.1 The approach","text":"idea behind sign-restriction approach “draw” random matrices \\(\\mathcal{P}\\) satisfy \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\), constitute set admissible matrices, keeping set simulated \\(\\mathcal{P}\\) matrices satisfy predefined sign-based restriction. example restriction “one year, contractionary monetary-policy shocks negative impact inflation”.suggested , \\(B\\) matrix satisfies \\(\\Omega = BB'\\) (instance, \\(B\\) can based Cholesky decomposition \\(\\Omega\\)), also \\(\\Omega = \\mathcal{P}\\mathcal{P}'\\) soon \\(\\mathcal{P}=BQ\\), \\(Q\\) orthogonal matrix. Therefore, draw \\(\\mathcal{P}\\) matrices, suffices draw set orthogonal matrices.fix ideas, consider dimension 2. case, orthogonal matrices rotation matrices, set orthogonal matrices can parameterized angle \\(x\\), :\n\\[\nQ_x=\\begin{pmatrix}\\cos(x)&\\cos\\left(x+\\frac{\\pi}{2}\\right)\\\\\n\\sin(x)&\\sin\\left(x+\\frac{\\pi}{2}\\right)\\end{pmatrix}=\\begin{pmatrix}\\cos(x)&-\\sin(x)\\\\\n\\sin(x)&\\cos(x)\\end{pmatrix}.\n\\]\n(angle-\\(x\\) counter-clockwise rotation.) Hence, case, drawing \\(x\\) randomly \\([0,2\\pi]\\), draw randomly set \\(2\\times2\\) rotation matrices. high-dimensional VAR, lose simple geometrical representation, though. always possible parametrize rotation matrix (high-dimentional VARs).proceed, ? Arias, Rubio-Ramírez, Waggoner (2018) provide procedure. approach based -called \\(QR\\) decomposition: square matrix \\(X\\) may decomposed \\(X=QR\\) \\(Q\\) orthogonal matrix \\(R\\) upper diagonal matrix. mind, propose two-step approach:Draw random matrix \\(X\\) drawing element independent standard normal distribution.Let \\(X = QR\\) \\(QR\\) decomposition \\(X\\) diagonal \\(R\\) normalized \npositive. random matrix \\(Q\\) orthogonal draw uniform distribution set orthogonal matrices.Equipped procedure, sign-restriction based following algorithm:Draw random orthogonal matrix \\(Q\\) (using step . ii. described ).Compute \\(B = PQ\\) \\(P\\) Cholesky decomposition reduced form residuals \\(\\Omega_{\\varepsilon}\\).Compute impulse response associated \\(B\\) \\(y_{t,t+k}=\\Phi^kB\\) cumulated response \\(\\bar y_{t,t+k}=\\sum_{j=0}^{k}\\Phi^jB\\).sign restrictions satisfied?Yes. Store impulse response set admissible response.. Discard impulse response.Perform \\(N\\) replications report median impulse response (“confidence” intervals).Note: take account uncertainty \\(B\\) \\(\\Phi\\), can draw \\(B\\) \\(\\Phi\\) Steps 2 3 using inference method (see Section 3).sign-restriction approach method advantage relatively agnostic. Moreover, fairly flexible, one can impose sign restrictions variable, horizon.","code":""},{"path":"Signs.html","id":"an-example","chapter":"4 Sign restrictions","heading":"4.2 An example","text":"prominent example Uhlig (2005). Using US monthly data 1965.2003.XII, employs sign restrictions estimate effect monetary policy shocks.According conventional wisdom, monetary contractions :3Raise federal funds rate,Lower prices,Decrease non-borrowed reserves,Reduce real output.restrictions considered Uhlig (2005) follows: expansionary monetary policy shock leads :Increases pricesIncrease nonborrowed reservesDecreases federal funds rateWhat output? Since response interest, leave un-restricted.\nFigure 4.1: IRF associated monetary policy shock; sign-restriction approach.\nstressed sign restriction approach lead unique IRF, set admissible IRFs. Accordingly, say approach set-identified, point-identified.","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nhorizon <- list()\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\nsign.restrictions[[1]][1,3] <- 1\nsign.restrictions[[1]][2,5] <- -1\nsign.restrictions[[1]][3,6] <- 1\nhorizon[[1]] <- 1:5\nres.svar.signs <- \n  svar.signs(y,p=3,\n             nb.shocks = 1, #number of identified shocks\n             nb.periods.IRF = 20,\n             bootstrap.replications = 1, # = 0 if no bootstrap\n             confidence.interval = 0.80, # expressed in pp.\n             indic.plot = 1, # Plots are displayed if = 1.\n             nb.draws = 10000, # number of draws\n             sign.restrictions,\n             horizon,\n             recursive =1 #  =0 <- draw Q directly, =1 <- draw q recursively\n  )"},{"path":"Signs.html","id":"the-penalty-function-approach-pfa","chapter":"4 Sign restrictions","heading":"4.3 The penalty-function approach (PFA)","text":"alternative approach -called penalty-function approach (PFA, Uhlig (2005), present Danne (2015)’s package). approach relies penalty function:\n\\[\n\\begin{array}{llll}f(x)&=&x&\\text{ }x\\le0\\\\\n&&100.x&\\text{ }x>0\\end{array}\n\\]\npenalizes positive responses rewards negative responses.Let \\(\\psi_k^j(q)\\) impulse response variable \\(j\\). \\(\\psi_k^j(q)\\)’s elements \\(\\psi_k(q)=\\Psi_kq\\).Let \\(\\sigma_j\\) standard deviation variable \\(j\\). Let \\(\\iota_{j,k}=1\\) restrict response variable \\(j\\) \\(k^th\\) horizon negative, \\(\\iota_{j,k}=-1\\) restrict positive, \\(\\iota_{j,k}=0\\) restriction. total penalty given \\[\n\\mathbf{P}(q)=\\sum_{j=1}^m\\sum_{k=0}^Kf\\left(\\iota_{j,k}\\frac{\\psi_k^j(q)}{\\sigma_j}\\right).\n\\]looking solution \n\\[\\begin{array}{ll}\n&\\min_q \\mathbf{P}(q)\\\\\n\\text{s.t. }&q'q=1.\\end{array}\\]problem solved numerically.","code":""},{"path":"Signs.html","id":"NarrativeSign","chapter":"4 Sign restrictions","heading":"4.4 Narrative sign restrictions","text":"related approach, introduced Antolín-Díaz Rubio-Ramírez (2018), consists imposing , specific dates (based narrative evidence), signs shocks positive (negative).4 instance, Antolín-Díaz Rubio-Ramírez (2018) argue one rule structural parameters disagree view “negative oil supply shock occurred outbreak Gulf War August 1990.”Suppose want impose restriction , dates \\(\\{t_1,\\dots,t_J\\}\\), signs \\(j^{th}\\) shock positive. , narrative sign restrictions simply imposed :\n\\[\n\\hat{\\eta}_{j,t}(B) = e_j'\\hat\\eta_{t}(B) > 0,\n\\]\n\\(\\hat\\eta_{t}(B)\\) vector structural shock associated given matrix \\(B\\) (\\(e_j\\) \\(j^{th}\\) column \\(n \\times n\\) identity matrix).","code":""},{"path":"SignsZeros.html","id":"SignsZeros","chapter":"5 Combining sign and zero restrictions","heading":"5 Combining sign and zero restrictions","text":"Sometimes need combine different types restrictions. instance:One shock satisfies zero sign restrictions.shocks can identified zero restrictions (SR LR), others sign restrictions.shocks satisfy zero restrictions (e.g. LR effect output) can distinguished sign restrictions.instances, must make independent draws set structural parameters satisfying zero restrictions. ? Arias, Rubio-Ramírez, Waggoner (2018) propose impose zero restrictions \\(B\\), check signs. Remember, \\(\\mathcal{P}=BQ\\) candidate impact IRF. structural shock \\(j\\), define \\(m\\)-column matrices \\(Z_j\\) (zero restrictions) \\(S_j\\) (sign restrictions). row \\(Z_j\\) (resp. \\(S_j\\)) defines zero (resp. sign) restriction. \\(Z_j\\) \\(m-j\\) rows (.e., \\(m-j\\) zero restriction ).Example 5.1  4-variable VAR, want impose first structural shock effect variable 1, affects positively variable 2 negatively variable 3 impact:\n\\[Z_1 = \\begin{pmatrix}1 & 0 & 0 & 0\\end{pmatrix}, \\]\n\\[S_1 = \\begin{pmatrix}0 & 1 & 0 & 0\\\\\n0 & 0 & -1 & 0\\end{pmatrix}. \\]zero sign restrictions satisfied, must \n\\[\nZ_jb_j=0 \\quad \\mbox{} \\quad S_jb_j>0,\n\\]\n\\(b_j\\) \\(j^{th}\\) column \\(B\\), .e. impact effect \\(j^{th}\\) structural shock.algorithm follows:\\(1\\le j\\le m\\), draw \\(u_j\\\\mathbb{R}^{m+1-j-z_j}\\) standard normal distribution (\\(z_j\\) number zero restrictions imposed \\(j^{th}\\) shock) set \\(w_j = u_j/||u_j||\\).Define \\(Q= \\begin{pmatrix}q_1&...&q_m\\end{pmatrix}\\) recursively \\(q_j = K_jw_j\\) matrix \\(K_j\\) whose columns form orthogonal basis null space matrix \\[M_j =\n\\begin{pmatrix} q_1&...&q_{j-1}&\\color{blue}{(Z_jP)'}\\end{pmatrix}'.\\] (Vector \\(q_j\\) orthogonal \\(\\begin{pmatrix} q_1&...&q_{j-1}\\end{pmatrix}\\) satisfy zero restriction.)Set \\(B=PQ\\).Check sign restrictions (\\(S_jb_j>0\\) \\(j\\)?).Perform \\(N\\) replications report median impulse response (confidence intervals).Function svar.signs can run algorithm. called follows:5\nFigure 5.1: IRF associated monetary policy shock; sign-restriction approach.\n\nFigure 5.2: IRF associated monetary policy shock; sign-restriction approach.\n\nFigure 5.3: IRF associated monetary policy shock; sign-restriction approach.\n\nFigure 5.4: IRF associated monetary policy shock; sign-restriction approach.\n\nFigure 5.5: IRF associated monetary policy shock; sign-restriction approach.\n\nFigure 5.6: IRF associated monetary policy shock; sign-restriction approach.\n","code":"\nlibrary(IdSS);library(vars);library(Matrix)\ndata(\"USmonthly\")\nFirst.date <- \"1965-01-01\"\nLast.date <- \"1995-06-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nconsidered.variables<-c(\"LIP\",\"UNEMP\",\"LCPI\",\"LPCOM\",\"FFR\",\"NBR\",\"TTR\",\"M1\")\nn <- length(considered.variables)\ny <- as.matrix(USmonthly[considered.variables])\nsign.restrictions <- list()\nSR.restrictions <- list()\nhorizon <- list()\n\n#Define sign restrictions and horizon for restrictions\nfor(i in 1:n){\n  sign.restrictions[[i]] <- matrix(0,n,n)\n  horizon[[i]] <- 1\n}\nsign.restrictions[[1]][1,6] <- 1\nsign.restrictions[[2]][1,7] <- 1\nsign.restrictions[[3]][1,1] <- 1\nsign.restrictions[[3]][2,5] <- 1\nsign.restrictions[[4]][1,2] <- -1\nsign.restrictions[[4]][2,5] <- 1\nsign.restrictions[[5]][1,3] <- 1\nsign.restrictions[[5]][2,5] <- 1\nsign.restrictions[[6]][1,5] <- -1\nsign.restrictions[[6]][2,3] <- 1\nsign.restrictions[[6]][3,6] <- 1\nhorizon[[6]] <- 1:5\n\n#Define zero restrictions\nSR.restrictions[[1]] <- array(0,c(1,n))\nSR.restrictions[[1]][1,5] <- 1\nSR.restrictions[[2]] <- array(0,c(1,n))\nSR.restrictions[[2]][1,5] <- 1\nfor(i in 3:n){\n  SR.restrictions[[i]] <- array(0,c(0,n))\n}\n\nres.svar.signs.zeros <- svar.signs(y,p=3,\n                                  nb.shocks = 6, #number of identified shocks\n                                  nb.periods.IRF = 20,\n                                  bootstrap.replications = 100, # = 0 or 1\n                                  confidence.interval = 0.90, # expressed in pp.\n                                  indic.plot = 1, # Plots are displayed if = 1.\n                                  nb.draws = 10000, # number of draws\n                                  sign.restrictions,\n                                  horizon,\n                                  recursive =0,\n                                  SR.restrictions\n)\nIRFs.signs <- res.svar.signs.zeros$IRFs.signs\nnb.rotations <- res.svar.signs.zeros$xx"},{"path":"forecast-error-variance-maximization.html","id":"forecast-error-variance-maximization","chapter":"6 Forecast error variance maximization","heading":"6 Forecast error variance maximization","text":"","code":""},{"path":"forecast-error-variance-maximization.html","id":"the-main-unconditional-approach","chapter":"6 Forecast error variance maximization","heading":"6.1 The main (unconditional) approach","text":"approach presented section exploits derivations Uhlig (2004). Barsky Sims (2011) exploit approach identify TFP news shock, define shock () orthogonal innovation current utilization-adjusted TFP (b) best explains variation future TFP.Consider process \\(\\{y_t\\}\\) admits infinite MA representation Eq. (1.4). Let \\(Q\\) orthogonal matrix, alternative decomposition \\(y_t\\) :\n\\[\\begin{eqnarray}\ny_t&=&\\sum_{h=0}^{+\\infty}\\Psi_h\\underbrace{\\eta_{t-h}}_{Q\\tilde \\eta_{t-h}} = \\sum_{h=0}^{+\\infty}\\underbrace{\\Psi_hQ}_{\\tilde\\Psi_h}\\tilde\n\\eta_{t-h} = \\sum_{h=0}^{+\\infty}\\tilde\\Psi_h\\tilde \\eta_{t-h},\n\\end{eqnarray}\\]\n\\(\\tilde \\eta_{t-h}=Q'\\eta_{t-h}\\) white-noise shocks associated new MA representation, \\(Q\\) orthgonal matrix. (also satisfy \\(\\mathbb{V}ar(\\tilde\\eta_t)=Id\\).)\\(h\\)-step ahead prediction error \\(y_{t+h}\\), given data , including, \\(t-1\\) given \n\\[\ne_{t+h}(h)=y_{t+h}-\\mathbb{E}_{t-1}(y_{t+h})=\\sum_{j=0}^h\\tilde \\Psi_h\\tilde \\eta_{t+h-j}.\n\\]variance-covariance matrix \\(e_{t+h}(h)\\) \n\\[\n\\Omega^{(h)}=\\sum_{j=0}^h\\tilde \\Psi_j\\tilde \\Psi_j'=\\sum_{j=0}^h \\Psi_j \\Psi_j'.\n\\]can decompose \\(\\Omega^{(h)}\\) contribution shock \\(l\\) (\\(l^{th}\\) component \\(\\tilde{\\eta}_t\\)):\n\\[\n\\Omega^{(h)}=\\sum_{l=1}^n\\Omega_l^{(h)}(Q)\n\\]\n\n\\[\n\\Omega_l^{(h)}(Q) =\\sum_{j=0}^h(\\Psi_jq_l)(\\Psi_jq_l)',\n\\]\n\\(q_l\\) \\(l^{th}\\) column \\(Q\\).decomposition can used objective finding impulse vector \\(b\\) s.t. explains much possible sum \\(h\\)-step ahead prediction error variance variable \\(\\), say, prediction horizons \\(h \\[\\underline{h} , \\overline{h}]\\).Formally, task explain much possible variance\n\\[\n\\sigma^2(\\underline{h},\\overline{h},q_1)=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}\n\\]\nsingle impulse vector \\(q_1\\).Denote \\(E_{ii}\\) matrix filled zeros, except (\\(,\\)) entry, set 1. :\n\\[\\begin{eqnarray*}\n\\sigma^2(\\underline{h},\\overline{h},q_1)&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h\\left[(\\Psi_jq_1)(\\Psi_jq_1)'\\right]_{,}=\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[E_{ii}(\\Psi_jq_1)(\\Psi_jq_1)'\\right]\\\\\n&=&\\sum_{h=\\underline{h}}^{\\overline{h}} \\sum_{j=0}^h Tr\\left[q_1'\\Psi_j'E_{ii}\\Psi_j q_1\\right]\\\\\n&=& q_1'Sq_1,\n\\end{eqnarray*}\\]\n\n\\[\\begin{eqnarray*}\n\\begin{array}{lll}S&=&\\sum_{h=\\underline{h}}^{\\overline{h}}\\sum_{j=0}^{h}\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_j'E_{ii}\\Psi_j\\\\\n&=&\\sum_{j=0}^{\\overline{h}}(\\overline{h}+1-max(\\underline{h},j))\\Psi_{j,}'\\Psi_{j,}\\\\\n\\end{array}\n\\end{eqnarray*}\\]\n\\(\\Psi_{j,}\\) denotes row \\(\\) \\(\\Psi_{j}\\), .e., response variable \\(\\) horizon \\(j\\) (\\(Q=Id\\)).maximization problem subject side constraint \\(q_1'q_1=1\\) can written Lagrangian: \\[\nL=q_1'Sq_1-\\lambda(q_1'q_1-1),\n\\]\nfirst-order condition \\(Sq_1=\\lambda q_1\\) (side constraint \\(q_1'q_1=1\\)). equation, see solution \\(q_1\\) eigenvector \\(S\\), one associated eigenvalue \\(\\lambda\\). also see \\(\\sigma^2(\\underline{h},\\overline{h},q_1)=\\lambda\\). Thus, maximize variance, need find eigenvector \\(S\\) associated maximal eigenvalue \\(\\lambda\\). defines first principal component (see Section 10.1). , \\(S\\) admits following spectral decomposition:\n\\[\nS = \\mathcal{P}D\\mathcal{P}',\n\\]\n\\(D\\) diagonal matrix whose entries (ordered) eigenvalues: \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0\\), \\(\\sigma^2(\\underline{h},\\overline{h},q_1)\\) maximized \\(q_1 = p_1\\), \\(p_1\\) first column \\(\\mathcal{P}\\).following code identifies ``main GDP shock’’ using Uhlig’s method.\nFigure 6.1: Main GDP shock.\n``main GDP shock’’ explains 74% variance GDP.following code replicates Levchenko Pandalai-Nayar (2018). use mix zero FEVD identify TFP surprises, TFP news, sentiment shocks.\nFigure 6.2: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\n\nFigure 6.3: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\n\nFigure 6.4: Replication Levchenko Pandalai-Nayar (2020). FEVD zero restrictions.\nSentiment shocks explain 12% variance GDP, 72% TFP shocks (including 62% TFP news shocks).","code":"\nlibrary(IdSS)\nlibrary(readxl)\nlibrary(vars)\nlibrary(Matrix)\n# Declare data:\nTFP   <- levpan$tfp_lev\nGDP   <- levpan$lngdpcap\nE12   <- levpan$e12m\nCONS  <- levpan$lnconcap\nHOURS <- levpan$lnhrscap\ny <- cbind(TFP,GDP,E12,CONS,HOURS)\nnames.of.variables <- c(\"TFP\",\"GDP\",\"E12\",\"Consumption\",\"Hours\")\ncolnames(y)  <- names.of.variables\nT <- dim(y)[1]\nn <- dim(y)[2]\np <- 2\nnb.periods.IRF <- 40\nbootstrap.replications <- 1000\nconfid.interv <- 0.75\nindic.plot <- 1\nH <- 20\n# ===============================\n# FEVM\n# ===============================\ny0.star <- rep(0,n*p)\nEn <- array(0,c(n,n))\nEn[2,2] <- 1\nstdv.IRFs <- list()\n# do Choleski (we need the FULL IRFs)\ncholesky.res <- svar.ordering.all(y,p,nb.periods.IRF,n)\n# Store results\nIRFs <- cholesky.res$IRFs\nest.VAR <- cholesky.res$est.VAR\nPhi     <- Acoef(est.VAR)\nB.hat   <- cholesky.res$B.hat\ncst     <- Bcoef(est.VAR)[,p*n+1]\nresids  <- residuals(est.VAR)\nOmega   <- var(resids)\n# if no bootstrap then simply use point estimate\nsimulated.IRFs  <- replicate(1,IRFs, simplify=\"array\")\nsimulated.B.hat <- replicate(1,B.hat, simplify=\"array\")\nsimulated.Phi   <- replicate(1,Phi, simplify=\"array\")\n# if bootstrap then generate and store simulated IRFs, B.hat and Phi\nif(bootstrap.replications>1){\n  bootstrap.res <- param.bootstrap(y,p,nb.periods.IRF,n,bootstrap.replications,\n                                  posit.of.shock = 0)\n  simulated.IRFs  <- bootstrap.res$simulated.IRFs\n  simulated.B.hat <- bootstrap.res$simulated.B.hat\n  simulated.Phi   <- bootstrap.res$simulated.Phi}\n# Initialize Q as identity matrix\nQ <- replicate(bootstrap.replications,diag(n), simplify=\"array\")\n# This is where we will store the IRFs\nIRFs.final <- array(NaN,c(n,n,nb.periods.IRF,bootstrap.replications))\n# This loop identifies the relevant Q for each bootstrap replication\n  for (l in 1:bootstrap.replications){\n    ##############################\n    # Identification of main GDP shock\n    ##############################\n    # Compute S\n    WWW <- array(0,c(n,n))\n    for (h in 1:H){\n      V99 <- simulated.IRFs[,,h,l]\n      JJ <- (H+1-h)*t(V99)%*%En%*%V99\n      WWW <- WWW+JJ}\n    r <- eigen(WWW) \n    # Take the eigenvector with the highest eigenvalue\n    eigvec <- matrix(r$vectors[,1],n,1)\n    # We might need to adjust the sign\n    if (simulated.IRFs[1,,20,l]%*%eigvec>0){\n      Q[,1,l] <- Q[,,l]%*%eigvec\n      }else{Q[,1,l] <- - Q[,,l]%*%eigvec}\n    Q[,2:n,l] <- Null(Q[,1,l]) # we ensure that columns 2 to n are \n    #   orthogonal to the first one.\n    # New IRFs\n    for (t in 1:nb.periods.IRF){\n      IRFs.final[,,t,l] <- simulated.IRFs[,,t,l]%*%Q[,,l]}\n  }\n# compute some key moments of the simulated IRFs:\nstdv.IRFs <- apply(IRFs.final,c(1,2,3),sd)\nCI.lower.bounds <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,(1-confid.interv)/2)})\nCI.upper.bounds <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,1-(1-confid.interv)/2)})\nCI.median <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,0.5)})\n# Plot graphs\n  par(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\n  for(i in 1:n){\n    plot(CI.median[i,1,],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n         ylim=c(min(CI.lower.bounds[i,1,]),\n                max(CI.upper.bounds[i,1,])),\n         main=paste(\"Effect of Main GDP shock on \",\n                    names.of.variables[i],sep=\"\"))\n    abline(h=0,col=\"grey\")\n      lines(CI.lower.bounds[i,1,],col=\"red\",lty=2,lwd=2)\n      lines(CI.upper.bounds[i,1,],col=\"red\",lty=2,lwd=2)}\nvariance.decomp <- variance.decomp(IRFs.final)\nvardecomp <- variance.decomp$vardecomp\nmean(vardecomp[2,2,40,,1]) ## [1] 0.739408\nnb.periods.IRF <- 40\np <- 2\nbootstrap.replications <- 1000\nconfid.interv <- 0.75\nindic.plot <- 1\nnb.shocks <- 3\nnames.of.shocks <- c(\"TFP surprise\",\"TFP news\",\"Sentiment\")\nHn <- 40 #horizon for news shock\nHs <- 2 #horizon for sentiment shock\n# ===============================\n# FEVM + zeros\n# ===============================\nEn <- array(0,c(n,n))\nEn[1,1] <- 1\nEs <- array(0,c(n,n))\nEs[3,3] <- 1\n# Initialize Q as identity matrix\nQ <- replicate(bootstrap.replications,diag(n), simplify=\"array\")\n# This loop identifies the relevant Q for each bootstrap replication\n  for (l in 1:bootstrap.replications){\n    ##############################\n    # Identification of news shocks\n    ##############################\n    # Compute S\n    WWW <- array(0,c(n-1,n-1))\n    for (h in 1:Hn){\n      V99 <- simulated.IRFs[,,h,l]%*%Q[,2:n,l] \n      # Notice that we use only columns 2 to n of Q:\n      #  the first column selects the TFP surprise shock,\n      #  which is the first shock in the Cholesky\n      #  decomposition where TFP is ordered first.\n      JJ <- (Hn+1-h)*t(V99)%*%En%*%V99\n      WWW <- WWW+JJ}\n    r <- eigen(WWW) \n    # Take the eigenvector with the highest eigenvalue\n    eigvec <- matrix(r$vectors[,1],n-1,1)\n    # We might need to adjust the sign\n    if (simulated.IRFs[1,,20,l]%*%Q[,2:n,l]%*%eigvec>0){\n      Q[,2,l] <- Q[,2:n,l]%*%eigvec\n      }else{\n        Q[,2,l] <- - Q[,2:n,l]%*%eigvec}\n    Q[,3:n,l] <- Null(Q[,1:2,l]) # we ensure that columns 3 to n\n    #   are orthogonal to the first 2\n    #####################################\n    # Identification of sentiment shocks\n    #####################################\n    # Compute S\n    WWW <- array(0,c(n-2,n-2))\n    for (h in 1:Hs){\n      V99 <- simulated.IRFs[,,h,l]%*%Q[,3:n,l]\n      # Notice that we use only columns 3 to n of Q:\n      # the first column selects the TFP surprise shock, \n      # which is the first shock in the Cholesky decomposition\n      # where TFP is ordered first, the second column generates \n      # the linear combination of \"Cholesky shocks\" that is orthogonal\n      # to TFP surprise and best explains TFP up to \n      # horizon 40 (see last step)\n      JJ <- (Hs+1-h)*t(V99)%*%Es%*%V99\n      WWW <- WWW+JJ}\n    r <- eigen(WWW) \n    # Take the eigenvector with the highest eigenvalue\n    eigvec <- matrix(r$vectors[,1],n-2,1)\n    # We might need to adjust the sign\n    if (simulated.IRFs[3,,2,l]%*%Q[,3:n,l]%*%eigvec>0){\n      Q[,3,l] <- Q[,3:n,l]%*%eigvec\n    }else{Q[,3,l] <- -Q[,3:n,l]%*%eigvec}\n    Q[,4:n,l] <- Null(Q[,1:3,l]) # we ensure that columns 4 to n are\n    # orthogonal to the first 3\n    for (t in 1:nb.periods.IRF){\n      IRFs.final[,,t,l] <- simulated.IRFs[,,t,l]%*%Q[,,l]}\n  }\n# compute some key moments of the simulated IRFs\nstdv.IRFs <- apply(IRFs.final,c(1,2,3),sd)\nCI.lower.bounds <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,(1-confid.interv)/2)})\nCI.upper.bounds <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,1-(1-confid.interv)/2)})\nCI.median <-\n  apply(IRFs.final,c(1,2,3),function(x){quantile(x,0.5)})\n# Plot graphs\nfor (j in 1:nb.shocks){\n  par(mfrow=c(2,ifelse(round(n/2)==n/2,n/2,(n+1)/2)))\n  for(i in 1:n){\n    plot(CI.median[i,j,],type=\"l\",lwd=2,xlab=\"\",ylab=\"\",\n         ylim=c(min(CI.lower.bounds[i,j,]),\n                max(CI.upper.bounds[i,j,])),\n         main=paste(\"Effect of \",names.of.shocks[j],\" on \",\n                    names.of.variables[i],sep=\"\"))\n    abline(h=0,col=\"grey\")\n      lines(CI.lower.bounds[i,j,],col=\"red\",lty=2,lwd=2)\n      lines(CI.upper.bounds[i,j,],col=\"red\",lty=2,lwd=2)}\n}\nvariance.decomp <- variance.decomp(IRFs.final)\nvardecomp <- variance.decomp$vardecomp\nmean(vardecomp[2,2,40,,1])## [1] 0.1004037\nmean(vardecomp[2,2,40,,2])## [1] 0.6356303\nmean(vardecomp[2,2,40,,3])## [1] 0.1192929"},{"path":"forecast-error-variance-maximization.html","id":"NarrativeHistDecomp","chapter":"6 Forecast error variance maximization","heading":"6.2 Restrictions based on narrative historical decomposition","text":"related approach, introduced Antolín-Díaz Rubio-Ramírez (2018), consists imposing , specific dates (based narrative information), particular shock important contributor unexpected movement variable particular period.6 can formalized two different ways (respectively called Type Type B Antolín-Díaz Rubio-Ramírez (2018)):Type : given shock important (least important) driver unexpected change variable periods. periods, absolute value contribution unexpected change variable larger (smaller) absolute value contribution structural shock.Type B: given shock overwhelming (negligible) driver unexpected change given variable period. periods, absolute value contribution unexpected change variable larger (smaller) sum absolute value contributions structural shocks.","code":""},{"path":"NonGaussian.html","id":"NonGaussian","chapter":"7 Identification based on non-normality of the shocks","heading":"7 Identification based on non-normality of the shocks","text":"","code":""},{"path":"NonGaussian.html","id":"intuition","chapter":"7 Identification based on non-normality of the shocks","heading":"7.1 Intuition","text":"section, show non-identification structural shocks (\\(\\eta_t\\)) specific Gaussian case. propose consistent estimation approaches SVAR context non-Gaussian shocks.seen precedes identify \\(B\\) based first second moments . Since Gaussian distribution perfectly determined first two moments, comes one achieve identification structural shocks Gaussian. , even observe infinite number ..d. \\(B \\eta_t\\), recover \\(B\\) \\(\\eta_t\\)’s Gaussian.Indeed, \\(\\eta_t \\sim \\mathcal{N}(0,Id)\\), distribution \\(\\varepsilon_t \\equiv B \\eta_t\\) \\(\\mathcal{N}(0,BB')\\). Hence \\(\\Omega = B B'\\) observed (population), orthogonal matrix \\(Q\\) (.e. \\(QQ'=Id\\)), also \\(BQ \\eta_t \\sim \\mathcal{N}(0,\\Omega)\\).illustrate, consider following bivariate Gaussian situations, \\(\\Theta_1=0\\)):\\(\\left[\\begin{array}{c}\\eta_{1,t}\\\\ \\eta_{2,t}\\end{array}\\right]\\sim \\mathcal{N}(0,Id)\\), \n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\) (rotation).Figure 7.1 shows distributions \\(B \\eta_t\\) \\(BQ\\eta_t\\) identical. However, impulse response functions associated one impulse matrix (\\(B\\) \\(BQ\\)) different. illustrated Figure 7.2, shows IRFs associated two identical models (defined Eq. (1.5)), difference impulse matrix (\\(B\\) \\(BQ\\)).\nFigure 7.1: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_{t} \\sim \\mathcal{N}(0,Id)\\) (therefore \\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\) independent), \\(Q\\) orthogonal matrix.\n\nFigure 7.2: figure shows impulse response functions associated impulse matrix equal \\(B\\) (black line) \\(BQ\\) (red line) different (even \\(BB'=BQ(BQ)'\\)).\nHence, Gaussian case, external restrictions (economic hypotheses) needed identify \\(B\\) (see previous sections). restrictions may necessary structural shocks Gaussian. , identification problem specific normally-distributed \\(\\eta_t\\)’s (Rigobon (2003), Normandin Phaneuf (2004), Lanne Lütkepohl (2008)).better see can case, consider bivariate vector independent structural shocks (\\(\\eta_{1,t}\\) \\(\\eta_{2,t}\\)) , now, assume one Gaussian . Specifically, assume \\(\\eta_{2,t}\\) drawn Student distribution 5 degrees freedom:\n\\(\\eta_{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_{2,t} \\sim t(5)\\),\n\\(B = \\left[\\begin{array}{cc} 1 & 2 \\\\ -1 & 1 \\end{array}\\right]\\) \n\\(Q = \\left[\\begin{array}{cc} \\cos(\\pi/3) & -\\sin(\\pi/3) \\\\ \\sin(\\pi/3) & \\cos(\\pi/3) \\end{array}\\right]\\).Figure 7.3 shows , case, \\(B \\eta_t\\) \\(BQ\\eta_t\\) distribution (spite fact , cases, \\(\\mathbb{V}ar(\\varepsilon_t)=BB'\\)). opens door identification impulse matrix (\\(BQ\\)) non-Gaussian case.\nFigure 7.3: figure compares distributions two Gaussian bivariate vectors, \\(B \\eta_t\\) \\(BQ\\eta_t\\), \\(\\eta_t{1,t} \\sim \\mathcal{N}(0,1)\\), \\(\\eta_t{2,t} \\sim t(5)\\), \\(Q\\) orthogonal matrix.\n","code":""},{"path":"NonGaussian.html","id":"independent-component-analysis-ica","chapter":"7 Identification based on non-normality of the shocks","heading":"7.2 Independent Component Analysis (ICA)","text":"exercise consists identifying non-Gaussian independent shocks linear combinations shocks well-known problem signal-processing literature, called independent component analysis (ICA). Without loss generality, can assume \\(BB' = Id\\) (.e. \\(B\\) orthogonal). (case, .e. \\(\\mathbb{V}ar(\\varepsilon_t)=\\Omega \\ne Id\\), one can pre-multiply data \\(\\Omega^{-1/2}\\).) classical ICA problem follows: Find \\(B\\) \\(\\varepsilon_t = B \\eta_t\\) ($_t= B’ _t $) given thatWe observe \\(\\varepsilon_t\\)’s,components \\(\\eta_t\\) independent,\\(BB'=Id\\) (.e., \\(B\\) orthogonal).Figure 7.4 represents bivariate distributions. black (red) lines correspond distributions \\(\\eta_t\\) (\\(B\\eta_t\\)). important note two components vector \\(B \\eta_t\\) independent (contrary \\(\\eta_t\\)).\nFigure 7.4: three plots represent bivariate distributions \\(\\eta_t\\) (black) \\(B\\eta_t\\) (red), two components \\(\\eta_t\\) independent, unit variance, \\(B\\) orthogonal. Hence, three plots, \\(\\mathbb{V}ar(B\\eta_t)=Id\\).\ncases, \\(\\mathbb{V}ar(\\varepsilon_t)=\\mathbb{V}ar(\\eta_t)=Id\\). two components \\(\\varepsilon_t\\) independent. instance: \\(\\mathbb{E}(\\varepsilon_{2,t}|\\varepsilon_{1,t}>4)<0\\) (whereas \\(\\mathbb{E}(\\eta_{2,t}|\\eta_{1,t}>4)=0\\)). objctive ICA rotate \\(\\varepsilon_t\\) retrieve independent components (\\(\\eta_t\\)).Hypothesis 7.1  Process \\(\\eta_t\\) satisfies:\\(\\eta_t\\)’s ..d. (across time) \\(\\mathbb{E}(\\eta_t) = 0\\) \\(\\mathbb{V}ar(\\eta_t) = Id.\\)components \\(\\eta_{1,t}, \\ldots, \\eta_{n,t}\\) mutually independent.\niii \n\\[\n\\varepsilon_t = B_0 \\eta_t,\n\\]\n\\(\\mathbb{V}ar(\\varepsilon_t) = Id\\) (.e. \\(B_0\\) orthogonal).Theorem 7.1  (Eriksson, Koivunen (2004)) Hypothesis 7.1 satisfied one components \\(\\eta\\) Gaussian, matrix \\(B_0\\) identifiable post multiplication \\(DP\\), \\(P\\) permutation matrix \\(D\\) diagonal matrix whose diagonal entries 1 \\(-1\\).}","code":""},{"path":"NonGaussian.html","id":"pseudo-maximum-likelihood-pml-approach","chapter":"7 Identification based on non-normality of the shocks","heading":"7.3 Pseudo-Maximum Likelihood (PML) approach","text":"Hence, non-normal structural shocks identifiable. estimate based observations \\(\\varepsilon_t\\)’s? Gouriéroux, Monfort, Renne (2017) proposed Pseudo-Maximum Likelihood (PML) approach. approach consists maximizing -called pseudo log-likelihood function, based set p.d.f. \\(g_i (\\eta_i), =1,\\ldots,n\\) (may different true p.d.f. \\(\\eta_{,t}\\)’s):\n\\[\\begin{equation}\n\\log \\mathcal{L}_T (B) = \\sum^T_{t=1} \\sum^n_{=1} \\log g_i (b'_i Y_t),\\tag{7.1}\n\\end{equation}\\]\n\\(b_i\\) \\(^{th}\\) column matrix \\(B\\) (\\(b'_i\\) \\(^{th}\\) row \\(B^{-1}\\) since \\(B^{-1}=B'\\)).restrictions \\(B'B = Id\\) can eliminated parameterizing \\(B\\) way , whatever consider parameters, \\(B\\) orthogonal.7 Gouriéroux, Monfort, Renne (2017) propose use, , Cayley’s representation: orthogonal matrix eigenvalue equal \\(-1\\) can written \n\\[\\begin{equation}\nB() = (Id+) (Id-)^{-1},\n\\end{equation}\\]\n\\(\\) skew symmetric (antisymmetric) matrix, \\('=-\\). one--one relationship \\(\\), since:\n\\[\\begin{equation}\n= (B()+Id)^{-1} (B()-Id).\n\\end{equation}\\]Hence, PML estimator matrix \\(B\\) obtained \\(\\widehat{B_T} = B(\\hat{}_T),\\) :\n\\[\\begin{equation}\n\\hat{}_T = \\arg \\max_{a_{,j}, >j} \\sum^T_{t=1} \\sum^n_{=1} \\log g_i [b_i ()' \\varepsilon_t].\\tag{7.3}\n\\end{equation}\\]assumptions \\(g_i\\) functions (excluding Gaussian distributions), Gouriéroux, Monfort, Renne (2017) derive asymptotic properties PML estimator. Specifically, PML estimator \\(\\widehat{B_T}\\) \\(B_0\\) consistent (\\(\\mathcal{P}_0\\), set matrices obtained permutation sign change columns \\(B_0\\)) asymptotically normal, speed convergence \\(1/\\sqrt{T}\\).asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{B_T} - B_0)\\) \\(^{-1} \\left[\\begin{array}{cc} \\Gamma & 0 \\\\ 0 & 0 \\end{array} \\right] (')^{-1}\\), matrices \\(\\) \\(\\Gamma\\) detailed Gouriéroux, Monfort, Renne (2017).Note potential misspecification pseudo-distributions \\(g_i\\) effect consistency specific PML estimators.Table 7.1 reports usual p.d.f. derivatives. (latter needed compute asymptotic variance-covariance matrix \\(vec \\sqrt{T} (\\widehat{B_T} - B_0)\\).)Table 7.1:  table reports usual p.d.f. derivatives.Example 7.1  (Non-Gaussian monetary-policy shocks) apply PML-ICA approach U.S. data coerving period 1959:IV 2015:quarterly frequency (\\(T=224\\)). consider three dependent variables: inflation (\\(\\pi_t\\)), economic activity (\\(z_t\\), output gap) nominal short-term interest rate (\\(r_t\\)). Changes log oil prices added exogenous variable (\\(x_t\\)).Let us denote \\(W_t\\) set information made past values \\(y_t= [\\pi_t,z_t,r_t]\\), \\(\\{y_{t-1},y_{t-2},\\dots\\}\\), exogenous variables \\(\\{x_{t},x_{t-1},\\dots\\}\\). reduced-form VAR model reads:\n\\[\ny_t  = \\underbrace{\\mu + \\sum_{=1}^{p} \\Phi_i y_{t-} + \\Theta x_t}_{(W_t;\\theta)} + u_t\n\\]\n\\(u_t\\)’s assumed serially independent, zero mean variance-covariance matrix \\(\\Sigma\\).Matrices \\(\\mu\\), \\(\\Phi_i\\), \\(\\Theta\\) \\(\\Sigma\\) consistently estimated OLS. Jarque-Bera tests support hypothesis non-normality residuals.want estimate orthogonal matrix \\(B\\) \\(u_t=SB \\eta_t\\), \\(S\\) results Cholesky decomposition \\(\\Sigma\\) andthe components \\(\\eta_t\\) independent, zero-mean unit variance.PML approach applied standardized VAR residuals given :\n\\[\n\\hat\\varepsilon_t = \\hat{S}_T^{-1}\\underbrace{[y_t - (W_t;\\hat\\theta_T)]}_{\\mbox{VAR residuals}}.\n\\]\nconstruction \\(\\hat{S}_T^{-1}\\), comes covariance matrix residuals \\(Id\\).pseudo density functions distinct asymmetric mixtures Gaussian distributions.(Note: always useful combine two optimization algorithms, Nelder-Mead BFGS.)obtain close results neglecting commodity prices. case, one can simply use function estim.SVAR.ICA IdSS package. Let us compare \\(C\\) matrix obtained two cases (without commodity prices):\\(B\\) estimated, remains label resulting structural shocks (components \\(\\eta_{t}\\)). Postulated shocks monetary-policy, supply, demand shocks. labelling can based following considerations:Contractionary monetary-policy shocks negative impact real activity inflation.Supply shock influences opposite signs economic activity inflation.Demand shock influences signs economic activity inflation.Let us compute IRFs associated three structural shocks. (sake comparison, first line plots shows IRFs monetary-policy shock obtained Cholesky-based approach short-term rate ordered last.)\nFigure 7.5: first row plots shows responses three endogenous variables monetary policy shock context Cholesky-idendtified SVAR (ordering: inflation, output gap, interest rate). next three rows plots show repsonses endogenous variables three structural shocks identified ICA. last one (Shock 3) close Cholesky-identified monetary policy shock.\nAccording Figure 7.5, Shock 1 supply shock, Shock 2 demand shock, Shock 3 monetary-policy shock. Note Shock 3 close one resulting Cholesky approach.","code":"\nlibrary(IdSS)\nFirst.date <- \"1959-04-01\"\nLast.date  <- \"2015-01-01\"\ndata <- US3var\ndata <- data[(data$Date>=First.date)&(data$Date<=Last.date),]\nY <- as.matrix(data[c(\"infl\",\"y.gdp.gap\",\"r\")])\nnames.var <- c(\"inflation\",\"real activity\",\"short-term rate\")\nT <- dim(Y)[1]\nn <- dim(Y)[2]\nnb.lags <- 6 # number of lags used in the VAR model\nX <- NULL\nfor(i in 1:nb.lags){\n  lagged.Y <- rbind(matrix(NaN,i,n),Y[1:(T-i),])\n  X <- cbind(X,lagged.Y)}\nX <- cbind(X,data$commo) # add exogenous variables\nPhi <- matrix(0,n,n*nb.lags);mu <- rep(0,n)\neffect.commo <- rep(0,n)\nU <- NULL # Eta is the matrix of OLS residuals\nfor(i in 1:n){\n  eq <- lm(Y[,i] ~ X)\n  Phi[i,] <- eq$coef[2:(dim(Phi)[2]+1)]\n  mu[i] <- eq$coef[1]\n  U <- cbind(U,eq$residuals)\n  effect.commo[i] <- eq$coef[length(eq$coef)]\n}\nOmega <- var(U) # Covariance matrix of the OLS residuals.\nB <- t(chol(Omega)) # Cholesky matrix associated with Omega (lower triang.)\nEps <- U %*% t(solve(B)) # Recover associated structural shocks\ndistri <- list(\n  type=c(\"mixt.gaussian\",\"mixt.gaussian\",\"mixt.gaussian\"),\n  df=c(NaN,NaN,NaN),\n  p=c(0.5,.5,.5),mu=c(.1,.1,.1),sigma=c(.5,.7,1.3))\nAA.0 <- c(0,0,0)\nres.optim <- optim(AA.0,func.2.minimize,\n                   Y = Eps, distri = distri,\n                   gr = d.func.2.minimize,\n                   method=\"Nelder-Mead\",\n                   control=list(trace=FALSE,maxit=1000))\nAA.0 <- res.optim$par\nres.optim <- optim(AA.0,func.2.minimize,d.func.2.minimize,\n                   Y = Eps, distri = distri,\n                   method=\"BFGS\",\n                   control=list(trace=FALSE))\nAA.est <- res.optim$par\nn <- ncol(Y)\nM <- make.M(n)\nA.est <- matrix(M %*% AA.est,n,n)\nC.PML <- (diag(n) + A.est) %*% solve(diag(n) - A.est)\neta.PML <- Eps %*% C.PML # eta.PML are the ICA-estimated structural shocks\n\n# Compute asymptotic covariance matrix of C.PML:\nV <- make.Asympt.Cov.delta(eta.PML,distri,C.PML)\nparam <- c(C.PML)\nst.dev <- sqrt(diag(V))\nt.stat <- c(C.PML)/sqrt(diag(V))\ncbind(param,st.dev,t.stat) # print results of PML estimation##             param      st.dev      t.stat\n##  [1,]  0.94417705 0.040848382  23.1141845\n##  [2,] -0.32711569 0.118802653  -2.7534376\n##  [3,]  0.03905164 0.074172945   0.5264944\n##  [4,]  0.32070293 0.119270893   2.6888616\n##  [5,]  0.93977707 0.041629110  22.5749976\n##  [6,]  0.11818924 0.060821400   1.9432179\n##  [7,] -0.07536139 0.071980455  -1.0469702\n##  [8,] -0.09906759 0.062185577  -1.5930959\n##  [9,]  0.99222290 0.007785691 127.4418551\nICA.res.no.commo <- estim.SVAR.ICA(Y,distri = distri,p=6)\nround(cbind(ICA.res.no.commo$C.PML,NaN,C.PML),3)##        [,1]  [,2]   [,3] [,4]   [,5]  [,6]   [,7]\n## [1,]  0.956 0.287 -0.059  NaN  0.944 0.321 -0.075\n## [2,] -0.292 0.950 -0.108  NaN -0.327 0.940 -0.099\n## [3,]  0.025 0.121  0.992  NaN  0.039 0.118  0.992\nIRF.Chol <- array(NaN,c(n,41,n))\nIRF.ICA  <- array(NaN,c(n,41,n))\nPHI <- list();for(i in 1:nb.lags){PHI[[i]]<-array(Phi,c(3,3,nb.lags))[,,i]}\nfor(jjjj in 1:n){\n  u.shock <- rep(0,n)\n  u.shock[jjjj] <- 1\n  IRF.Chol[,,jjjj] <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=B,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n  IRF.ICA[,,jjjj]  <- \n    t(simul.VAR(c=rep(0,3),Phi=PHI,B=B%*%C.PML,nb.sim=41,\n                y0.star=rep(0,3*nb.lags),indic.IRF = 1,u.shock = u.shock))\n}"},{"path":"NonGaussian.html","id":"relation-with-the-heteroskedasticity-identification","chapter":"7 Identification based on non-normality of the shocks","heading":"7.4 Relation with the Heteroskedasticity Identification","text":"cases, \\(\\varepsilon_t\\)’s heteroskedastic, \\(B\\) matrix can identified (Rigobon (2003), Lanne, Lütkepohl, Maciejowska (2010)).Consider case still \\(\\varepsilon_t = B \\eta_t\\) \\(\\eta_t\\)’s variance conditionally depends regime \\(s_t \\\\{1,\\dots,M\\}\\). :\n\\[\n\\mathbb{V}ar(\\eta_{k,t}|s_t) = \\lambda_{s_t,k} \\quad \\mbox{} k \\\\{1,\\dots,n\\}\n\\]Denoting \\(\\Lambda_i\\) diagonal matrix whose diagonal entries \\(\\lambda_{,k}\\)’s, comes :\n\\[\n\\mathbb{V}ar(\\eta_{t}|s_t) = \\Lambda_{s_t},\\quad \\mbox{}\\quad \\mathbb{V}ar(\\varepsilon_{t}|s_t) = B\\Lambda_{s_t}B'.\n\\]Without loss generality, can assumed \\(\\Lambda_1=Id\\).context, \\(B\\) identified, apart sign reversal columns \\(k \\ne j \\\\{1,\\dots,n\\}\\), regime \\(\\) s.t. \\(\\lambda_{,k} \\ne \\lambda_{,j}\\). (Prop.1 Lanne, Lütkepohl, Maciejowska (2010)).Bivariate regime case (\\(M=2\\)): \\(B\\) identified \\(\\lambda_{2,k}\\)’s different. , identification ensured “sufficient heterogeneity volatility changes” (Lütkepohl Netšunajev (2017)).regimes \\(s_t\\) exogenous serially independent, situation consistent “non-Gaussian” situation described .","code":""},{"path":"Projections.html","id":"Projections","chapter":"8 Local projection methods","heading":"8 Local projection methods","text":"","code":""},{"path":"Projections.html","id":"overview-of-the-approach","chapter":"8 Local projection methods","heading":"8.1 Overview of the approach","text":"Consider infinite MA representation \\(y_t\\) (Eq. (1.4)):\n\\[\ny_t = \\mu + \\sum_{h=0}^\\infty \\Psi_{h} \\eta_{t-h}.\n\\]\nseen Section 1.2, entries \\((,j)\\) sequence \\(\\Psi_h\\) matrices define IRF \\(\\eta_{j,t}\\) \\(y_{,t}\\).Assume observe \\(\\eta_{j,t}\\), consistent estimate \\(\\Psi_{,j,h}\\) simply obtained OLS regression \\(y_{,t+h}\\) \\(\\eta_{j,t}\\):8\n\\[\\begin{equation}\ny_{,t+h} = \\mu_i + \\Psi_{,j,h}\\eta_{j,t} + u_{,j,t+h}.\\tag{8.1}\n\\end{equation}\\]\nRunning kind regression (using instruments \\(\\eta_{j,t}\\)) core idea local projection (LP) approach proposed Jordà (2005).Now, proceed (usual) case \\(\\eta_{j,t}\\) observed? consider two situations. second requires instruments, first approach . first approach (Section @ref(#LPIVww)) original Jordà (2005)’s approach.","code":""},{"path":"Projections.html","id":"LPIVww","chapter":"8 Local projection methods","heading":"8.2 Situation A: Without IV","text":"Assume structural shock interest (\\(\\eta_{1,t}\\), say) can consistently obtained residual regression variable \\(x_t\\) set control variables \\(w_t\\) independent \\(\\eta_{1,t}\\):\n\\[\\begin{equation}\n\\eta_{1,t} = x_t - \\mathbb{E}(x_t|w_t),\\tag{8.2}\n\\end{equation}\\]\n\\(\\mathbb{E}(x_t|w_t)\\) affine \\(w_t\\) \\(w_t\\) affine transformation \\(\\eta_{2:n,t}\\) past shocks \\(\\eta_{t-1},\\eta_{t-2},\\dots\\).Eq. (8.2) implies , conditional \\(w_t\\), additional knowledge \\(x_t\\) useful comes forecast something depends \\(\\eta_{1,t}\\). Hence, given \\(u_{,1,t+h}\\) (see Eq. (8.1)) independent \\(\\eta_{1,t}\\) (depends \\(\\eta_{t+h},\\dots,\\eta_{t+1},\\color{blue}{\\eta_{2:n,t}},\\eta_{t-1},\\eta_{t-2},\\dots\\)), comes \n\\[\\begin{equation}\n\\mathbb{E}(u_{,1,t+h}|x_t,w_t)= \\mathbb{E}(u_{,1,t+h}|w_t).\\tag{8.3}\n\\end{equation}\\]\nconditional mean independence case.Using (8.2), one can rewrite Eq. (8.1) follows:\n\\[\\begin{eqnarray*}\ny_{,t+h} &=& \\mu_i + \\Psi_{,1,h}\\eta_{1,t} + u_{,1,t+h}\\\\\n&=&  \\mu_i + \\Psi_{,1,h}x_t  \\color{blue}{-\\Psi_{,1,h}\\mathbb{E}(x_t|w_t) + u_{,1,t+h}},\n\\end{eqnarray*}\\]Given Eq. (8.3), comes , conditional \\(x_t\\) \\(w_t\\), expectation blue term function \\(w_t\\). Assuming expectation linear, standard results conditional mean independence case imply OLS estimator regression \\(y_{,t+h}\\) \\(x_t\\), controlling \\(w_t\\), provides consistent estimate \\(\\Psi_{,1,h}\\):\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\Psi_{,1,h}x_t + \\beta'w_t + v_{,t+h}.\n\\end{equation}\\]instance consistent case \\([\\Delta GDP_t, \\pi_t,i_t]'\\) follows VAR(1) monetary-policy shock contemporaneously affect \\(\\Delta GDP_t\\) \\(\\pi_t\\). IRFs can estimated LP, taking \\(x_t = i_t\\) \\(w_t = [\\Delta GDP_t,\\pi_t,\\Delta GDP_{t-1}, \\pi_{t-1},i_{t-1}]'\\).approach closely relates SVAR Cholesky-based identification approach. Specifically, \\(w_t = [\\color{blue}{y_{1,t},\\dots,y_{k-1,t}}, y_{t-1}',\\dots,y_{t-p}']'\\), \\(k\\le n\\), \\(x_t = y_{k,t}\\), approach corresponds, \\(h=0\\), SVAR(\\(p\\)) Cholesky-based IRF (focusing responses \\(k^{th}\\) structural shock). However, two approaches differ \\(h>0\\), LP methodology assumes VAR dynamics \\(y_t\\).9","code":""},{"path":"Projections.html","id":"situation-b-iv-approach","chapter":"8 Local projection methods","heading":"8.3 Situation B: IV approach","text":"","code":""},{"path":"Projections.html","id":"instruments-proxies-for-structural-shocks","chapter":"8 Local projection methods","heading":"8.3.1 Instruments (proxies for structural shocks)","text":"Consider now valid instrument \\(z_t\\) \\(\\eta_{1,t}\\) (\\(\\mathbb{E}(z_t)=0\\)). :\n\\[\\begin{equation}\n\\left\\{\n\\begin{array}{llll}\n(IV.) & \\mathbb{E}(z_t \\eta_{1,t}) &\\ne 0 & \\mbox{(relevance condition)} \\\\\n(IV.ii) & \\mathbb{E}(z_t \\eta_{j,t}) &= 0 \\quad \\mbox{} j>1 & \\mbox{(exogeneity condition).}\n\\end{array}\\right.\\tag{8.4}\n\\end{equation}\\]\ninstrument \\(z_t\\) can used identify structural shock. Eq. (8.4) implies exist \\(\\rho \\ne 0\\) mean-zero variable \\(\\xi_t\\) :\n\\[\n\\eta_{1,t} = \\rho z_t + \\xi_t,\n\\]\n\\(\\xi_t\\) correlated neither \\(z_t\\), \\(\\eta_{j,t}\\), \\(j\\ge2\\).Proof. Define \\(\\rho = \\frac{\\mathbb{E}(\\eta_{1,t}z_t)}{\\mathbb{V}ar(z_t)}\\) \\(\\xi_t = \\eta_{1,t} - \\rho z_t\\). easily seen \\(\\xi_t\\) satisfies moment restrictions given .Ramey (2016) reviews different approaches employed construct monetary policy-shocks (two main approaches presented 8.1 8.2 ). also collected time series shocks, see website. Several shocks included Ramey dataset package IdSS.Example 8.1  (Identification Monetary-Policy Shocks Based High-Frequency Data) Instruments monetary-policy shocks can extracted high-frequency market data associated interest-rate products.quotes interest-rate-related financial products sensitive monetary-policy announcements. quotes mainly depends investors’ expectations regarding future short-term rates: \\(\\mathbb{E}_t(i_{t+s})\\). Typically, agents risk-neutral, maturity-\\(h\\) interest rate approximatively given :\n\\[\ni_{t,h} \\approx \\mathbb{E}_t\\left(\\frac{1}{h}\\int_{0}^{h} i_{t+s} ds\\right) = \\frac{1}{h}\\int_{0}^{h} \\mathbb{E}_t\\left(i_{t+s}\\right) ds.\n\\]\ngeneral, changes \\(\\mathbb{E}_t(i_{t+s})\\), \\(s>0\\), can affected types shocks may trigger reaction central bank.However, MP announcement takes place \\(t\\) \\(t+\\epsilon\\), \\(\\mathbb{E}_{t+\\epsilon}(i_{t+s})-\\mathbb{E}_t(i_{t+s})\\) attributed MP shock (see Figure 8.1, Gürkaynak, Sack, Swanson (2005)). Hence, monthly time series MP shocks can obtained summing, month, changes \\(i_{t+ \\epsilon,h} - i_{t,h}\\) associated given interest rate (T-bills, futures, swaps) given maturity \\(h\\).See among others: Kuttner (2001), Cochrane Piazzesi (2002), Gürkaynak, Sack, Swanson (2005), Piazzesi Swanson (2008), Gertler Karadi (2015). time series named\nFF4_TC, ED2_TC, ED3_TC, ED4_TC, GS1, ff1_vr, ff4_vr, ed2_vr, ff1_gkgreen, ff4_gkgreen, ed2_gkgreen data frame Ramey package IdSS time series shocks based approach (see Ramey’s website details).\nFigure 8.1: Source: Gurkaynak, Sack Swanson (2005). Transaction rates Federal funds futures June 25, 2003, day regularly scheduled FOMC meeting scheduled. 2:15 p.m., FOMC announced lowering target federal funds rate 1.25% 1%, many market participants expecting 50 bp cut. shows () financial markets seem fully adjust policy action within just minutes (ii) federal funds rate surprise necessarily direction federal funds rate action .\nExample 8.2  (Identification Monetary-Policy Shocks Based Narrative Approach) Romer Romer (2004) propose two-step approach:derive series Federal Reserve intentions federal funds rate (explicit target Fed) around FOMC meetings,control Federal Reserve forecasts.gives measure intended monetary policy actions driven information future economic developments.“intentions” measured combination narrative quantitative evidence. Sources: (among others) Minutes FOMC “Blue Books”.Controls = variables spanning information Federal Reserve future developments. Data: Federal Reserve’s internal forecasts (inflation, real output unemployment), “Greenbook’s forecasts” – usually issued 6 days FOMC meeting.shock measure residual series linear regression () (b). time series Ramey$rrshock83 Ramey$rrshock83b (Ramey data frame included package IdSS) contain shocks period 1983-2007. (Ramey$rrshock83b uses long-horizon Greenbook forecasts.)create measure news future government spending, Ramey (2011) uses newspaper articles construct time series (unexpected) fiscal shocks:10Example 8.3  (Identification news future government spending) Ramey (2011)’s measure aims measure expected discounted value government spending changes due foreign political events. argues variable matter wealth effect neoclassical framework. series constructed reading periodicals order gauge public’s expectations (Business Week 2001, newspapers afterwards). principal source sample often gave detailed predictions.According Ramey (2011), government sources used () either released timely manner (b) known underestimate costs certain actions.Figure 8.2 shows resulting time series shocks. Figure 8.3 shows IRF macro variables shock expected government spending.\nFigure 8.2: Source: Ramey (2011). Defense News: PDV Change Spending Percent GDP.\n\nFigure 8.3: Source: Ramey (2011) [Figure X paper]. Responses macro variables shock expected government spending.\ntwo main IV approaches estimate IRFs see Stock Watson (2018):SVAR-IV approach (Subsection 8.3.2),LP-IV approach, \\(y_t\\)’s DGP left unspecified (Subsection 8.3.3).LP-IV approach based set IV regressions (variable interest, one forecast horizon). SVAR-IV approach based IV regressions VAR innovations (one series VAR innovations).VAR adequately captures DGP, IV-SVAR optimal horizons. However, VAR misspecified, specification errors compounded horizon local projection method lead better results.","code":""},{"path":"Projections.html","id":"SVARIVa","chapter":"8 Local projection methods","heading":"8.3.2 Situation B.1: SVAR-IV approach","text":"Assume consistent estimates \\(\\varepsilon_t = B\\eta_t\\), estimates (\\(\\hat\\varepsilon_{t}\\)) coming estimation VAR model. , \\(\\\\{1,\\dots,n\\}\\):\n\\[\\begin{eqnarray}\n\\varepsilon_{,t} &=& b_{,1} \\eta_{1,t} + u_{,t} \\tag{8.5}\\\\\n&=& b_{,1} \\rho z_t + \\underbrace{b_{,1}\\xi_t + u_{,t}}_{\\perp z_t}. \\nonumber\n\\end{eqnarray}\\]\n(\\(u_{,t}\\) linear combination \\(\\eta_{j,t}\\)’s, \\(j\\ge2\\)).Hence, multiplicative factor (\\(\\rho\\)), (OLS) regressions \\(\\hat\\varepsilon_{,t}\\)’s \\(z_t\\) (consistent true \\(\\varepsilon_{,t}\\)’s) provide consistent estimates \\(b_{,1}\\)’s.Combined estimated VAR (\\(\\Phi_k\\) matrices), provides consistent estimates IRFs \\(\\eta_{1,t}\\) \\(y_t\\), though multiplicative factor. scale ambiguity can solved rescaling structural shock (“unit-effect normalisation”, see Stock Watson (2018)). Let us consider \\(\\tilde\\eta_{1,t}=b_{1,1}\\eta_{1,t}\\); construction, \\(\\tilde\\eta_{1,t}\\) unit contemporaneous effect \\(y_{1,t}\\). Denoting \\(\\tilde{B}_{,1}\\) contemporaneous impact \\(\\tilde\\eta_{1,t}\\), get:\n\\[\n\\tilde{B}_{1} = \\frac{1}{b_{1,1}} {B}_{1},\n\\]\n\\(B_{1}\\) denotes \\(1^{st}\\) column \\(B\\) \\(\\tilde{B}_{1}=[1,\\tilde{B}_{2,1},\\dots,\\tilde{B}_{n,1}]'\\).Eq. (8.5) gives:\n\\[\\begin{eqnarray*}\n\\varepsilon_{1,t} &=& \\tilde\\eta_{1,t} + u_{1,t}\\\\\n\\varepsilon_{,t} &=& \\tilde{B}_{,1} \\tilde\\eta_{1,t} + u_{,t}.\n\\end{eqnarray*}\\]\nsuggests \\(\\tilde{B}_{,1}\\) can estimated regressing \\(\\varepsilon_{,t}\\) \\(\\varepsilon_{1,t}\\) (\\(\\hat\\varepsilon_{,t}\\) \\(\\hat\\varepsilon_{1,t}\\) practice), using \\(z_t\\) instrument.inference? use usual TSLS standard deviations \\(\\varepsilon_{,t}\\)’s directly observed. Bootstrap procedures can resorted . Stock Watson (2018) propose, particular, Gaussian parametric bootstrap:Assume estimated \\(\\{\\widehat{\\Phi}_1,\\dots,\\widehat{\\Phi}_p,\\widehat{B}_1\\}\\) using SVAR-IV approach based size-\\(T\\) sample. Generate \\(N\\) (\\(N\\) large) size-\\(T\\) samples following VAR:\n\\[\n\\left[\n\\begin{array}{cc}\n\\widehat{\\Phi}(L) & 0 \\\\\n0 & \\widehat{\\rho}(L)\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\ny_t \\\\\nz_t\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right],\n\\]\n\\[\n\\mbox{} \\quad \\left[\n\\begin{array}{c}\n\\varepsilon_t \\\\\ne_t\n\\end{array}\n\\right]\\sim \\, ..d.\\,\\mathcal{N}\\left(\\left[\\begin{array}{c}0\\\\0\\end{array}\\right],\n\\left[\\begin{array}{cc}\n\\Omega & S'_{\\varepsilon,e}\\\\\nS_{\\varepsilon,e}& \\sigma^2_{e}\n\\end{array}\\right]\n\\right),\n\\]\n\\(\\widehat{\\rho}(L)\\) \\(\\sigma^2_{e}\\) result estimation AR process \\(z_t\\), \\(\\Omega\\) \\(S_{\\varepsilon,e}\\) sample covariances VAR/AR residuals.simulated sample (\\(\\tilde{y}_t\\) \\(\\tilde{z}_t\\), say), estimate \\(\\{\\widetilde{\\widehat{\\Phi}}_1,\\dots,\\widetilde{\\widehat{\\Phi}}_p,\\widetilde{\\widehat{B}}_1\\}\\) associated \\(\\widetilde{\\Psi}_{,1,h}\\). provides e.g. sequence \\(N\\) estimates \\(\\Psi_{,1,h}\\), quantiles conf. intervals can deduced.following lines code, use approach estimate response macroeconomic variables monetary policy shock. instrument FF4_TC Ramsey’s database; base Gertler Karadi (2015) approach, use 3-month fed funds futures.\nFigure 8.4: Gertler-Karadi monthly shocks, fed funds futures 3 months.\n\nFigure 8.5: Reponses monetary-policy shock, SVAR-IV approach.\n","code":"\nlibrary(vars);library(IdSS)\ndata(\"USmonthly\")\nFirst.date <- \"1990-05-01\";Last.date <- \"2012-6-01\"\nindic.first <- which(USmonthly$DATES==First.date)\nindic.last  <- which(USmonthly$DATES==Last.date)\nUSmonthly   <- USmonthly[indic.first:indic.last,]\nshock.name <- \"FF4_TC\" #\"FF4_TC\", \"ED2_TC\", \"ff1_vr\", \"rrshock83b\"\nindic.shock.name <- which(names(USmonthly)==shock.name)\nZ <- matrix(USmonthly[,indic.shock.name],ncol=1)\npar(plt=c(.1,.95,.1,.95))\nplot(USmonthly$DATES,Z,type=\"l\",xlab=\"\",ylab=\"\",lwd=2)\nconsidered.variables <- c(\"GS1\",\"LIP\",\"LCPI\",\"EBP\")\ny <- as.matrix(USmonthly[,considered.variables])\nn <- length(considered.variables)\ncolnames(y) <- considered.variables\npar(plt=c(.15,.95,.15,.8))\nres.svar.iv <- \n  svar.iv(y,Z,p = 4,names.of.variables=considered.variables,\n          nb.periods.IRF = 20,\n          z.AR.order=1, \n          nb.bootstrap.replications = 100, \n          confidence.interval = 0.90,\n          indic.plot=1)"},{"path":"Projections.html","id":"LPIVa","chapter":"8 Local projection methods","heading":"8.3.3 Situation B.2: LP-IV","text":"want posit VAR-type dynamics \\(y_t\\) –e.g., suspect true generating model may non-invertible VARMA model– can directly proceed IV-projection methods obtain \\(\\tilde\\Psi_{,1,h}\\equiv \\Psi_{,1,h}/b_{1,1}\\) (IRFs \\(\\tilde\\eta_{1,t}\\) \\(y_{,t}\\)).However, Assumptions (IV.) (IV.ii) (Eq. (8.4)) complemented (IV.iii):\n\\[\\begin{equation*}\n\\begin{array}{llll}\n(IV.iii) & \\mathbb{E}(z_t \\eta_{j,t+h}) &= 0 \\, \\mbox{ } h \\ne 0 & \\mbox{(lead-lag exogeneity)}\n\\end{array}\n\\end{equation*}\\](IV.), (IV.ii) (IV.iii) satisfied, \\(\\tilde\\Psi_{,1,h}\\) can estimated regressing \\(y_{,t+h}\\) \\(y_{1,t}\\), using \\(z_t\\) instrument, .e. considering TSLS estimation :\n\\[\\begin{equation}\ny_{,t+h} = \\alpha_i + \\tilde\\Psi_{,1,h}y_{1,t} + \\nu_{,t+h},\\tag{8.6}\n\\end{equation}\\]\n\\(\\nu_{,t+h}\\) correlated \\(y_{1,t}\\), \\(z_t\\).indeed:\n\\[\\begin{eqnarray*}\ny_{1,t} &=& \\alpha_1 + \\tilde\\eta_{1,t} + v_{1,t}\\\\\ny_{,t+h} &=& \\alpha_i + \\tilde\\Psi_{,1,h}\\tilde\\eta_{1,t} + v_{,t+h},\n\\end{eqnarray*}\\]\n\\(v_{,t+h}\\)’s uncorrelated \\(z_t\\) (IV.), (IV.ii) (IV.iii).Note , \\(h>0\\), \\(v_{,t+h}\\) (\\(\\nu_{,t+h}\\)) auto-correlated. Newey-West corrections therefore used compute std errors \\(\\tilde\\Psi_{,1,h}\\)’s estimates.Consider linear regression:\n\\[\n\\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon,\n\\]\n\\(\\mathbb{E}(\\boldsymbol\\varepsilon)=0\\), explicative variables \\(\\mathbf{X}\\) can correlated residuals \\(\\boldsymbol\\varepsilon\\). Moreover, \\(\\boldsymbol\\varepsilon\\)’s may feature heteroskedasticity auto-correlated. denote \\(\\mathbf{Z}\\) matrix instruments, \\(\\mathbb{E}(\\mathbf{X}'\\mathbf{Z}) \\ne 0\\) \\(\\mathbb{E}(\\boldsymbol\\varepsilon'\\mathbf{Z}) = 0\\).IV estimator \\(\\boldsymbol\\beta\\) obtained regressing \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\), \\(\\hat{\\mathbf{Y}}\\) \\(\\hat{\\mathbf{X}}\\) respective residuals regressions \\(\\mathbf{Y}\\) \\(\\mathbf{X}\\) \\(\\mathbf{Z}\\).\n\\[\\begin{eqnarray*}\n\\mathbf{b}_{iv} &=& [\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{Y}\\\\\n\\mathbf{b}_{iv} &=& \\boldsymbol\\beta + \\frac{1}{\\sqrt{T}}\\underbrace{T[\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}\\mathbf{Z}'\\mathbf{X}]^{-1}\\mathbf{X}'\\mathbf{Z}(\\mathbf{Z}'\\mathbf{Z})^{-1}}_{=Q(\\mathbf{X},\\mathbf{Z}) \\overset{p}{\\rightarrow} \\mathbf{Q}_{xz}}\\underbrace{\\sqrt{T}\\left(\\frac{1}{T}\\mathbf{Z}'\\boldsymbol\\varepsilon\\right)}_{\\overset{d}{\\rightarrow} \\mathcal{N}(0,S)},\n\\end{eqnarray*}\\]\n\\(\\mathbf{S}\\) long-run variance \\(\\mathbf{z}_t\\varepsilon_t\\).11 asymptotic covariance matrix \\(\\sqrt{T}\\mathbf{b}_{iv}\\) \\(\\mathbf{Q}_{xz} \\mathbf{S} \\mathbf{Q}_{xz}'\\). Therefore, covariance matrix \\(\\mathbf{b}_{iv}\\) can approximated \\(\\frac{1}{T}Q(\\mathbf{X},\\mathbf{Z})\\hat{\\mathbf{S}}Q(\\mathbf{X},\\mathbf{Z})'\\) \\(\\hat{\\mathbf{S}}\\) Newey-West estimator \\(\\mathbf{S}\\).12Assumption (IV.iii) usually restrictive \\(h>0\\) (\\(z_t\\) usually affected future shocks). contrast, may restrictive \\(h<0\\). can solved adding controls Regression (8.6). controls span space \\(\\{\\eta_{t-1},\\eta_{t-2},\\dots\\}\\).\\(z_t\\) suspected correlated past values \\(\\eta_{1,t}\\) \\(\\eta_{j,t}\\)’s, \\(j>1\\), one can add lags \\(z_t\\) controls (method e.g. advocated Ramey, 2016, p.108, considering instrument Gertler Karadi (2015)).general case, one can use lags \\(y_t\\) controls. Note , even (IV.iii) holds, adding controls may reduce variance regression error.","code":"\nres.LP.IV <- make.LPIV.irf(y,Z,\n                           nb.periods.IRF = 20,\n                           nb.lags.Y.4.control=4,\n                           nb.lags.Z.4.control=4,\n                           indic.plot = 1, # Plots are displayed if = 1.\n                           confidence.interval = 0.90)"},{"path":"PanelVARs.html","id":"PanelVARs","chapter":"9 Panel VARs","heading":"9 Panel VARs","text":"Panel VARs structure VAR models, \nsense variables assumed endogenous \ninterdependent, cross sectional dimension \nadded representation. \\(N\\) units indexed \n\\(\\\\{1,...,N\\}\\). index \\(\\) generic indicate\ncountries, sectors, markets… panel VAR \n\\[ y_{}=c_i+\\Phi_i(L)\ny_{t-1}+\\varepsilon_{}.\\] \\(y_t\\) stacked version \n\\(y_{}\\) \\(\\varepsilon_t\\) ..d., variance-covariance\nmatrix \\(\\Omega\\). Vector \\(c_i\\) lag polynomial \\(\\Phi_i(L)\\) may depend unit. Canova Ciccarelli (2013) provide survey panel estimation methods.Contrary standard VARs, panel VARs may help study\n* similarities/differences transmission shocks;\n* Spillovers, contagion.panel VARs subject tothe curse dimensionality. Indeed, can characterized \n* Dynamic interdependence: potentially, lags \nendogenous variables units can enter model unit \\(\\).\n* Static interdependence: \\(\\varepsilon_{}\\) \ngenerally correlated across \\(\\).\n* Cross sectional heterogeneity: intercept, slope \nvariance shocks may unit-specific.","code":""},{"path":"PanelVARs.html","id":"without-dynamic-interdependence","chapter":"9 Panel VARs","heading":"9.1 Without Dynamic interdependence","text":"panel VAR, assuming dynamic interdependence, form:\n\\[ y_{}=c_i+\\Phi_i(L)\n\\color{red}{y_{-1}}+\\varepsilon_{}.\\]comparison, consider micro panel data, univariate cae (AR(1) case): \\[y_{}=c_i+{\\color{red}\\phi} y_{-1}+\\varepsilon_{}.\\] kind context, usually cross-sectional heterogeneity \\(\\phi_i=\\phi\\) \\(\\). Typically, large cross-sectional dimension \\(N\\), small time dimension \\(T\\). one uses ``Fixed-effect’’ regression:\n\\[y_{}-\\frac{1}{T}\\sum_{s=1}^Ty_{}=\\phi(y_{-1}-\\frac{1}{T}\\sum_{s=1}^Ty_{})+\\varepsilon_{}-\\frac{1}{T}\\sum_{s=1}^T\\color{red}{\\varepsilon_{}},\\]\none faces Nickell bias: lagged dependent variable, estimator biased, bias size \\(\\sim 1/T\\). One can use GMM regressions (Arellano Bond (1991)) get unbiased estimates.Macro panel data different structure, typically moderate cross-sectional dimension \\(N\\) large time dimension \\(T\\), Nickell bias negligible (\\(\\rightarrow 0\\) \\(T\\rightarrow\\infty\\)).","code":""},{"path":"PanelVARs.html","id":"mean-group-estimator","chapter":"9 Panel VARs","heading":"9.1.1 Mean Group Estimator","text":"etimate\n\\[ y_{}=c_i+\\Phi_i(L)\ny_{-1}+\\varepsilon_{},\\]\nneed take account cross-sectional heterogeneity coefficients, .e., different \\(\\Phi_i\\)’s across \\(\\)’s.\nPooled estimators (assuming cross-sectional homogeneity, .e. identical \\(\\Phi_i\\)’s across \\(\\)s) consistent (biased) underlying dynamics actually heterogeneous. contrast, Mean Group (MG) estimator, consists estimating \\(N\\) separate regressions\ncalculating coefficient means, consistent.","code":""},{"path":"PanelVARs.html","id":"shock-identification","chapter":"9 Panel VARs","heading":"9.1.2 Shock identification","text":"Shock identification can performed standard methods (zeros, signs, FEVM, etc.). make assumption \\(\\Omega\\) block-diagonal (interdependence impact), consistent assumption cross-sectional dependence.","code":""},{"path":"PanelVARs.html","id":"with-dynamic-interdependencies","chapter":"9 Panel VARs","heading":"9.2 With Dynamic Interdependencies","text":"panel VAR accommodates dynamic interdependence form:\n\\[ y_{}=c_i+\\Phi_i(L)\ny_{t-1}+\\varepsilon_{}.\\]face serious curse dimensionality : \\(NGp+1\\) coefficients estimate \nequation.solution select eligible dynamic links (See instance Negro (2011)). Another alternative use factor model. consists capturing dynamic interdependencies set unobservable factors (See Canova Ciccarelli (2004) Canova Ciccarelli (2009)). See Section 10 details FAVAR models.","code":""},{"path":"FAVAR.html","id":"FAVAR","chapter":"10 Factor-Augmented VAR","heading":"10 Factor-Augmented VAR","text":"VAR models subject curse dimensionality: \\(n\\), large, number parameters (\\(n^2\\)) explodes.case one suspects \\(y_{,t}\\)’s mainly driven small number random sources, factor structure may imposed, principal component analysis (PCA, see Appendix 10.1) can employed estimate relevant factors (Bernanke, Boivin, Eliasz (2005)).","code":""},{"path":"FAVAR.html","id":"PCAapp","chapter":"10 Factor-Augmented VAR","heading":"10.1 Principal component analysis (PCA)","text":"Principal component analysis (PCA) classical easy--use statistical method reduce dimension large datasets containing variables linearly driven relatively small number factors. approach widely used data analysis image compression.Suppose \\(T\\) observations \\(n\\)-dimensional random vector \\(x\\), denoted \\(x_{1},x_{2},\\ldots,x_{T}\\). suppose component \\(x\\) mean zero.Denote \\(X\\) matrix given \\(\\left[\\begin{array}{cccc} x_{1} & x_{2} & \\ldots & x_{T}\\end{array}\\right]'\\). Denote \\(j^{th}\\) column \\(X\\) \\(X_{j}\\).want find linear combination \\(x_{}\\)’s (\\(x.u\\)), \\(\\left\\Vert u\\right\\Vert =1\\), “maximum variance.” , want solve:\n\\[\\begin{equation}\n\\begin{array}{clll}\n\\underset{u}{\\arg\\max} & u'X'Xu. \\\\\n\\mbox{s.t. } & \\left| u\\right| =1\n\\end{array}\\tag{10.1}\n\\end{equation}\\]Since \\(X'X\\) positive definite matrix, admits following decomposition:\n\\[\\begin{eqnarray*}\nX'X & = & PDP'\\\\\n& = & P\\left[\\begin{array}{ccc}\n\\lambda_{1}\\\\\n& \\ddots\\\\\n&  & \\lambda_{n}\n\\end{array}\\right]P',\n\\end{eqnarray*}\\]\n\\(P\\) orthogonal matrix whose columns eigenvectors \\(X'X\\).can order eigenvalues \\(\\lambda_{1}\\geq\\ldots\\geq\\lambda_{n}\\). (Since \\(X'X\\) positive definite, eigenvalues positive.)Since \\(P\\) orthogonal, \\(u'X'Xu=u'PDP'u=y'Dy\\) \\(\\left\\Vert y\\right\\Vert =1\\). Therefore, \\(y_{}^{2}\\leq 1\\) \\(\\leq n\\).consequence:\n\\[\ny'Dy=\\sum_{=1}^{n}y_{}^{2}\\lambda_{}\\leq\\lambda_{1}\\sum_{=1}^{n}y_{}^{2}=\\lambda_{1}.\n\\]easily seen maximum reached \\(y=\\left[1,0,\\cdots,0\\right]'\\). Therefore, maximum optimization program (Eq. (10.1)) obtained \\(u=P\\left[1,0,\\cdots,0\\right]'\\). , \\(u\\) eigenvector \\(X'X\\) associated larger eigenvalue (first column \\(P\\)).Let us denote \\(F\\) vector given matrix product \\(XP\\) (note last column equal \\(Xu\\)). columns \\(F\\), denoted \\(F_{j}\\), called factors. :\n\\[\nF'F=P'X'XP=D.\n\\]\nTherefore, particular, \\(F_{j}\\)’s orthogonal.Since \\(X=FP'\\), \\(X_{j}\\)’s linear combinations factors. Let us denote \\(\\hat{X}_{,j}\\) part \\(X_{}\\) explained factor \\(F_{j}\\), :\n\\[\\begin{eqnarray*}\n\\hat{X}_{,j} & = & p_{ij}F_{j}\\\\\nX_{} & = & \\sum_{j}\\hat{X}_{,j}=\\sum_{j}p_{ij}F_{j}.\n\\end{eqnarray*}\\]Consider share variance explained –\\(n\\) variables (\\(X_{1},\\ldots,X_{n}\\))– first factor \\(F_{1}\\):\n\\[\\begin{eqnarray*}\n\\frac{\\sum_{}\\hat{X}_{,1}\\hat{X}'_{,1}}{\\sum_{}X_{}X'_{}} & = & \\frac{\\sum_{}p_{i1}F_{1}F'_{1}p_{i1}}{tr(X'X)} = \\frac{\\sum_{}p_{i1}^{2}\\lambda_{1}}{tr(X'X)} = \\frac{\\lambda_{1}}{\\sum_{}\\lambda_{}}.\n\\end{eqnarray*}\\]Intuitively, first eigenvalue large, means first factor embed large share fluctutaions \\(n\\) \\(X_{}\\)’s.Let us illustrate PCA term structure yields. term strucutre yields (yield curve) know driven small number factors (e.g., Litterman Scheinkman (1991)). One can typically employ PCA recover factors. data used example taken Fred database (tickers: “DGS6MO”,“DGS1”, …). second plot shows factor loardings, indicate first factor level factor (loadings = black line), second factor slope factor (loadings = blue line), third factor curvature factor (loadings = red line).run PCA, one simply apply function prcomp matrix data:Let us know visualize results. first plot Figure 10.1 shows share total variance explained different principal components (PCs). second plot shows facotr loadings. two bottom plots show yields (black) fitted linear combinations first two PCs .\nFigure 10.1: PCA results. dataset contains 8 time series U.S. interest rates different maturities.\n","code":"\nlibrary(IdSS)\nUSyields <- USyields[complete.cases(USyields),]\nyds <- USyields[c(\"Y1\",\"Y2\",\"Y3\",\"Y5\",\"Y7\",\"Y10\",\"Y20\",\"Y30\")]\nPCA.yds <- prcomp(yds,center=TRUE,scale. = TRUE)\npar(mfrow=c(2,2))\npar(plt=c(.1,.95,.2,.8))\nbarplot(PCA.yds$sdev^2/sum(PCA.yds$sdev^2),\n        main=\"Share of variance expl. by PC's\")\naxis(1, at=1:dim(yds)[2], labels=colnames(PCA.yds$x))\nnb.PC <- 2\nplot(-PCA.yds$rotation[,1],type=\"l\",lwd=2,ylim=c(-1,1),\n     main=\"Factor loadings (1st 3 PCs)\",xaxt=\"n\",xlab=\"\")\naxis(1, at=1:dim(yds)[2], labels=colnames(yds))\nlines(PCA.yds$rotation[,2],type=\"l\",lwd=2,col=\"blue\")\nlines(PCA.yds$rotation[,3],type=\"l\",lwd=2,col=\"red\")\nY1.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y1\",1:2]\nY1.hat <- mean(USyields$Y1) + sd(USyields$Y1) * Y1.hat\nplot(USyields$date,USyields$Y1,type=\"l\",lwd=2,\n     main=\"Fit of 1-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y1.hat,col=\"blue\",lty=2,lwd=2)\nY10.hat <- PCA.yds$x[,1:nb.PC] %*% PCA.yds$rotation[\"Y10\",1:2]\nY10.hat <- mean(USyields$Y10) + sd(USyields$Y10) * Y10.hat\nplot(USyields$date,USyields$Y10,type=\"l\",lwd=2,\n     main=\"Fit of 10-year yields (2 PCs)\",\n     ylab=\"Obs (black) / Fitted by 2PCs (dashed blue)\")\nlines(USyields$date,Y10.hat,col=\"blue\",lty=2,lwd=2)"},{"path":"FAVAR.html","id":"favar-models","chapter":"10 Factor-Augmented VAR","heading":"10.2 FAVAR models","text":"Let us denote \\(F_t\\) \\(k\\)-dimensional vector latent factors accounting important shares variances \\(y_{,t}\\)’s (\\(K \\ll n\\)) \\(x_t\\) small \\(M\\)-dimensional subset \\(y_t\\) (\\(M \\ll n\\)). following factor structure posited:\n\\[\ny_t = \\Lambda^f F_t + \\Lambda^x x_t + e_t,\n\\]\n\\(e_t\\) “small” serially mutually ..d. error terms. \\(F_t\\) \\(x_t\\) supposed drive fluctuations \\(y_t\\)’s components.model complemented positing VAR dynamics \\([F_t',x_t']'\\):\n\\[\\begin{equation}\n\\left[\\begin{array}{c}F_t\\\\x_t\\end{array}\\right] = \\Phi(L)\\left[\\begin{array}{c}F_{t-1}\\\\ x_{t-1}\\end{array}\\right] + v_t.\\tag{10.2}\n\\end{equation}\\]Standard identification techniques structural shocks can employed Eq. (10.2): Cholesky approach can used instance last component \\(x_t\\) short-term interest rate assumed MP shock contemporaneous impact macro-variables (\\(x_t\\)).identification procedure, Bernanke, Boivin, Eliasz (2005) exploit fact macro-finance variables can decomposed two sets —fast-moving slow-moving variables— former reacts contemporaneously monetary-policy shocks. Now, estimate (unobserved) factors \\(F_t\\)? Bernanke, Boivin, Eliasz (2005) note first \\(K+M\\) PCA whole dataset (\\(y_t\\)), denote \\(\\hat{C}(F_t,x_t)\\) span space \\(F_t\\) \\(x_t)\\). get estimate \\(F_t\\), dependence \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) removed. done regressing, OLS, \\(\\hat{C}(F_t,x_t)\\) \\(x_t)\\) \\(\\hat{C}^*(F_t)\\), latter estimate common components \\(x_t\\). proxy \\(\\hat{C}^*(F_t)\\), Bernanke, Boivin, Eliasz (2005) take principal components set slow-moving variables, comtemporaneously correlated \\(x_t\\). Vector \\(\\hat{F}_t\\) computed \\(\\hat{C}(F_t,x_t) - b_x x_t\\), \\(b_x\\) coefficients coming previous OLS regressions.Note approach implies vectorial space spanned \\((\\hat{F}_t,x_t)\\) spanned \\(\\hat{C}(F_t,x_t)\\)., employ method dataset built McCracken Ng (2016) —FRED:MD database— includes 119 time series.\nFigure 10.2: Responses monetary-policy shock. FAVAR approach Bernanke, Boivin, Eliasz (2005). FRED-MD dataset.\n","code":"\nlibrary(BVAR)# contains the fred_md dataset\nlibrary(IdSS)\nlibrary(vars)\ndata <- fred_transform(fred_md,na.rm = FALSE, type = \"fred_md\")\nFirst.date <- \"1959-02-01\"\nLast.date <- \"2020-01-01\"\ndata <- data[(rownames(data)>First.date)&(rownames(data)<Last.date),]\nvariables.with.na <- which(is.na(apply(data,2,sum)))\ndata <- data[,-variables.with.na]\ndata.values <- scale(data, center = TRUE, scale = TRUE)\ndata_scaled <- data\ndata_scaled[1:dim(data)[1],1:dim(data)[2]] <- data.values\nK <- 3\nM <- 1\nPCA <- prcomp(data_scaled) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat <- PCA$x[,1:(K+M)]\nfast_moving <- c(\"HOUST\",\"HOUSTNE\",\"HOUSTMW\",\"HOUSTS\",\"HOUSTW\",\"HOUSTS\",\"AMDMNOx\",\n                 \"FEDFUNDS\",\"CP3Mx\",\"TB3MS\",\"TB6MS\",\"GS1\",\"GS5\",\"GS10\",\n                 \"COMPAPFFx\",\"TB3SMFFM\",\"TB6SMFFM\",\"T1YFFM\",\"T5YFFM\",\"T10YFFM\",\n                 \"AAAFFM\",\"EXSZUSx\",\"EXJPUSx\",\"EXUSUKx\",\"EXCAUSx\")\ndata.slow <- data_scaled[,-which(fast_moving %in% names(data))]\nPCA.star <- prcomp(data.slow) # implies that PCA$x %*% t(PCA$rotation) = data\nC.hat.star <- PCA.star$x[,1:K]\nD <- cbind(data$FEDFUNDS,C.hat.star)\nb.x <- solve(t(D)%*%D) %*% t(D) %*% C.hat\nF.hat <- C.hat - data$FEDFUNDS %*% matrix(b.x[1,],nrow=1)\ndata_var <- data.frame(F.hat, FEDFUNDS = data$FEDFUNDS)\np <- 10\nvar <- VAR(data_var, p)\nOmega <- var(residuals(var))\nB <- t(chol(Omega))\nD <- cbind(F.hat,data$FEDFUNDS)\nloadings <- solve(t(D)%*%D) %*% t(D) %*% as.matrix(data_scaled)\nirf <- simul.VAR(c=rep(0,(K+M)*p),Phi=Acoef(var),B,nb.sim=120,\n                 y0.star=rep(0,(K+M)*p),indic.IRF = 1,\n                 u.shock = c(rep(0,K+1),1))\nirf.all <- irf %*% loadings\npar(mfrow=c(2,2))\nvariables.2.plot <- c(\"FEDFUNDS\",\"INDPRO\",\"UNRATE\",\"CPIAUCSL\")\npar(plt=c(.2,.95,.3,.95))\nfor(i in 1:length(variables.2.plot)){\n  plot(cumsum(irf.all[,which(variables.2.plot[i]==names(data))]),lwd=2,\n       type=\"l\",xlab=\"months after shock\",ylab=variables.2.plot[i])\n}"},{"path":"append.html","id":"append","chapter":"11 Appendix","heading":"11 Appendix","text":"","code":""},{"path":"append.html","id":"definitions-and-statistical-results","chapter":"11 Appendix","heading":"11.1 Definitions and statistical results","text":"Definition 11.1  (Covariance stationarity) process \\(y_t\\) covariance stationary —weakly stationary— , \\(t\\) \\(j\\),\n\\[\n\\mathbb{E}(y_t) = \\mu \\quad \\mbox{} \\quad \\mathbb{E}\\{(y_t - \\mu)(y_{t-j} - \\mu)\\} = \\gamma_j.\n\\]Definition 11.2  (Likelihood Ratio test statistics) likelihood ratio associated restriction form \\(H_0: h({\\boldsymbol\\theta})=0\\) (\\(h({\\boldsymbol\\theta})\\) \\(r\\)-dimensional vector) given :\n\\[\nLR = \\frac{\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})}{\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})} \\quad (\\[0,1]),\n\\]\n\\(\\mathcal{L}_R\\) (respectively \\(\\mathcal{L}_U\\)) likelihood function imposes (resp. impose) restriction. likelihood ratio test statistic given \\(-2\\log(LR)\\), :\n\\[\n\\boxed{\\xi^{LR}= 2 (\\log\\mathcal{L}_U(\\boldsymbol\\theta;\\mathbf{y})-\\log\\mathcal{L}_R(\\boldsymbol\\theta;\\mathbf{y})).}\n\\]\nregularity assumptions null hypothesis, test statistic follows chi-square distribution \\(r\\) degrees freedom (see Table 11.3).Proposition 11.1  (p.d.f. multivariate Gaussian variable) \\(Y \\sim \\mathcal{N}(\\mu,\\Omega)\\) \\(Y\\) \\(n\\)-dimensional vector, density function \\(Y\\) :\n\\[\n\\frac{1}{(2 \\pi)^{n/2}|\\Omega|^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(Y-\\mu\\right)'\\Omega^{-1}\\left(Y-\\mu\\right)\\right].\n\\]","code":""},{"path":"append.html","id":"AppendixProof","chapter":"11 Appendix","heading":"11.2 Proofs","text":"Proof Proposition 1.2Proof. Using Proposition 11.1, obtain , conditionally \\(x_1\\), log-likelihood given \n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\theta) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right|\\\\\n&  & -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right].\n\\end{eqnarray*}\\]\nLet’s rewrite last term log-likelihood:\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\hat{\\Pi}'x_{t}+\\hat{\\Pi}'x_{t}-\\Pi'x_{t}\\right)\\right] & =\\\\\n\\sum_{t=1}^{T}\\left[\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)'\\Omega^{-1}\\left(\\hat{\\varepsilon}_{t}+(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\right],\n\\end{eqnarray*}\\]\n\\(j^{th}\\) element \\((n\\times1)\\) vector \\(\\hat{\\varepsilon}_{t}\\) sample residual, observation \\(t\\), OLS regression \\(y_{j,t}\\) \\(x_{t}\\). Expanding previous equation, get:\n\\[\\begin{eqnarray*}\n&&\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right]  = \\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\\\\n&&+2\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}+\\sum_{t=1}^{T}x'_{t}(\\hat{\\Pi}-\\Pi)\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}.\n\\end{eqnarray*}\\]\nLet’s apply trace operator second term (scalar):\n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t} & = & Tr\\left(\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\right)\\\\\n=  Tr\\left(\\sum_{t=1}^{T}\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'x_{t}\\hat{\\varepsilon}_{t}'\\right) & = & Tr\\left(\\Omega^{-1}(\\hat{\\Pi}-\\Pi)'\\sum_{t=1}^{T}x_{t}\\hat{\\varepsilon}_{t}'\\right).\n\\end{eqnarray*}\\]\nGiven , construction (property OLS estimates), sample residuals orthogonal explanatory variables, term zero. Introducing \\(\\tilde{x}_{t}=(\\hat{\\Pi}-\\Pi)'x_{t}\\), \n\\[\\begin{eqnarray*}\n\\sum_{t=1}^{T}\\left[\\left(y_{t}-\\Pi'x_{t}\\right)'\\Omega^{-1}\\left(y_{t}-\\Pi'x_{t}\\right)\\right] =\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}+\\sum_{t=1}^{T}\\tilde{x}'_{t}\\Omega^{-1}\\tilde{x}_{t}.\n\\end{eqnarray*}\\]\nSince \\(\\Omega\\) positive definite matrix, \\(\\Omega^{-1}\\) well. Consequently, smallest value last term can take obtained \\(\\tilde{x}_{t}=0\\), .e. \\(\\Pi=\\hat{\\Pi}.\\)MLE \\(\\Omega\\) matrix \\(\\hat{\\Omega}\\) maximizes \\(\\Omega\\overset{\\ell}{\\rightarrow}L(Y_{T};\\hat{\\Pi},\\Omega)\\). :\n\\[\\begin{eqnarray*}\n\\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega) & = & -(Tn/2)\\log(2\\pi)+(T/2)\\log\\left|\\Omega^{-1}\\right| -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\hat{\\varepsilon}_{t}'\\Omega^{-1}\\hat{\\varepsilon}_{t}\\right].\n\\end{eqnarray*}\\]Matrix \\(\\hat{\\Omega}\\) symmetric positive definite. easily checked (unrestricted) matrix maximizes latter expression symmetric positive definite matrix. Indeed:\n\\[\n\\frac{\\partial \\log\\mathcal{L}(Y_{T};\\hat{\\Pi},\\Omega)}{\\partial\\Omega}=\\frac{T}{2}\\Omega'-\\frac{1}{2}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t}\\Rightarrow\\hat{\\Omega}'=\\frac{1}{T}\\sum_{t=1}^{T}\\hat{\\varepsilon}_{t}\\hat{\\varepsilon}'_{t},\n\\]\nleads result.Proof Proposition 1.3Proof. Let us drop \\(\\) subscript. Rearranging Eq. (1.12), :\n\\[\n\\sqrt{T}(\\mathbf{b}-\\boldsymbol{\\beta}) =  (X'X/T)^{-1}\\sqrt{T}(X'\\boldsymbol\\varepsilon/T).\n\\]\nLet us consider autocovariances \\(\\mathbf{v}_t = x_t \\varepsilon_t\\), denoted \\(\\gamma^v_j\\). Using fact \\(x_t\\) linear combination past \\(\\varepsilon_t\\)s \\(\\varepsilon_t\\) white noise, get \\(\\mathbb{E}(\\varepsilon_t x_t)=0\\). Therefore\n\\[\n\\gamma^v_j = \\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}').\n\\]\n\\(j>0\\), \\(\\mathbb{E}(\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}')=\\mathbb{E}(\\mathbb{E}[\\varepsilon_t\\varepsilon_{t-j}x_tx_{t-j}'|\\varepsilon_{t-j},x_t,x_{t-j}])=\\) \\(\\mathbb{E}(\\varepsilon_{t-j}x_tx_{t-j}'\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}])=0\\). Note \\(\\mathbb{E}[\\varepsilon_t|\\varepsilon_{t-j},x_t,x_{t-j}]=0\\) \\(\\{\\varepsilon_t\\}\\) ..d. white noise sequence. \\(j=0\\), :\n\\[\n\\gamma^v_0 = \\mathbb{E}(\\varepsilon_t^2x_tx_{t}')= \\mathbb{E}(\\varepsilon_t^2) \\mathbb{E}(x_tx_{t}')=\\sigma^2\\mathbf{Q}.\n\\]\nconvergence distribution \\(\\sqrt{T}(X'\\boldsymbol\\varepsilon/T)=\\sqrt{T}\\frac{1}{T}\\sum_{t=1}^Tv_t\\) results Central Limit Theorem covariance-stationary processes, using \\(\\gamma_j^v\\) computed .","code":""},{"path":"append.html","id":"statistical-tables","chapter":"11 Appendix","heading":"11.3 Statistical Tables","text":"Table 11.1: Quantiles \\(\\mathcal{N}(0,1)\\) distribution. \\(\\) \\(b\\) respectively row column number; corresponding cell gives \\(\\mathbb{P}(0<X\\le +b)\\), \\(X \\sim \\mathcal{N}(0,1)\\).Table 11.2: Quantiles Student-\\(t\\) distribution. rows correspond different degrees freedom (\\(\\nu\\), say); columns correspond different probabilities (\\(z\\), say). cell gives \\(q\\) s.t. \\(\\mathbb{P}(-q<X<q)=z\\), \\(X \\sim t(\\nu)\\).Table 11.3: Quantiles \\(\\chi^2\\) distribution. rows correspond different degrees freedom; columns correspond different probabilities.Table 11.4: Quantiles \\(\\mathcal{F}\\) distribution. columns rows correspond different degrees freedom (resp. \\(n_1\\) \\(n_2\\)). different panels correspond different probabilities (\\(\\alpha\\)) corresponding cell gives \\(z\\) s.t. \\(\\mathbb{P}(X \\le z)=\\alpha\\), \\(X \\sim \\mathcal{F}(n_1,n_2)\\).","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
